app,view,panel,classification,text,"app_version",disabled
"GMC-001","Get execution metrics on Dashboards","tracking,health_assessments",Dashboards,"","index=`setup_summary_index` search_name=splunk_internal_web_access_idx_summary_tracker earliest=-7d@d
| `get_shcluster_label(Splunk_Instance)` 
| search shcluster_label=""*"" 
| `get_dashboards_info(shcluster_label,App,Dashboard_Name)` 
| `get_identity_info(User)`
| search emp_name=""*"" AND App=""*""",0
"GMC-002","Jobs running for > Nm and investigate if Min_Result_Count & Max_Result_Count are both zero!","health_assessments","Scheduled_Jobs","","index=`setup_summary_index` search_name=splunk_internal_scheduler_jobs_idx_summary_tracker Run_Time>900 earliest=-7d@d
| fields _time Splunk_Instance App Run_Time Result_Count Savedsearch_Name User 
| `get_shcluster_label(Splunk_Instance)` 
| fields - Splunk_Instance 
| stats Max(Run_Time) as Max_Run_Time Min(Run_Time) As Min_Run_Time Min(Result_Count) As Min_Result_Count Max(Result_Count) As Max_Result_Count count As Num_Executions by shcluster_label App User Savedsearch_Name 
| eval Run_Tume_Human=Max_Run_Time 
| `gmc_convert_runtime(Run_Tume_Human)` 
| `get_saved_searches_info(shcluster_label,App,Savedsearch_Name)` 
| eval dispatch_earliest_time=if(dispatch_earliest_time=1,""All-Time"", dispatch_earliest_time ) 
| lookup splunk_rest_data_macros_sh_kv_store_lookup shcluster_label app As App title As Macro_Reference OUTPUT definition 
| table shcluster_label App User Savedsearch_Name Num_Executions Min_Result_Count Max_Result_Count Min_Run_Time Max_Run_Time Run_Tume_Human savedsearch_type description author sharing cron_schedule realtime_schedule schedule_priority schedule_window updated allow_skew correlationsearch_enabled correlationsearch_label dispatch_auto_cancel dispatch_earliest_time dispatch_latest_time savedsearch_search Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference",0
"GMC-003","Jobs drifting or changing state",changes,"Scheduled_Jobs","","index=`setup_summary_index` search_name=splunk_rest_saved_searches_sh_summary_data shcluster_label=""*"" updated>0 earliest=-7d@d
| stats list(is_scheduled) As is_scheduled list(disabled) As disabled dc(is_scheduled) As dc_is_scheduled list(updated) as updated by shcluster_label app author savedsearch_name 
| eval updated_human=updated 
| convert ctime(updated_human) 
| where dc_is_scheduled >1",0
"GMC-004","Report on highest number of events by sourcetype enriched with sourcetype metadata","health_assessments",Sourcetypes,,"| metadata type=sourcetypes index=* 
| `get_sourcetype_info(sourcetype)` 
| table sourcetype totalCount firstTime lastTime recentTime author app sharing category rename SHOULD_LINEMERGE LINE_BREAKER TIME_PREFIX TIME_FORMAT MAX_TIMESTAMP_LOOKAHEAD TRUNCATE TZ KV_MODE EVENT_BREAKER_ENABLE EVENT_BREAKER DATETIME_CONFIG INDEXED_EXTRACTIONS LEARN_SOURCETYPE TRANSFORMS updated 
| sort 0 - totalCount",0
"GMC-005","Daily Total Index Size by Cluster By Index Over Time Trellis View report","indexes,health_assessments","Indexer_Cluster","","index=`setup_summary_index` search_name=splunk_internal_index_license_usage_idx_summary_tracker Index_Name=""*"" idxcluster_label=""*"" earliest=-7d@d
| fields _time search_name idxcluster_label Lic_Pool Lic_Pool_Size type Index_Name Lic_Sourcetype Lic_Source Lic_Host License_Usage 
| makemv delim=""|"" Lic_Sourcetype 
| makemv delim=""|"" Lic_Source 
| makemv delim=""|"" Lic_Host 
| `gmc_byte2mb(License_Usage)` 
| `gmc_byte2gb(License_Usage)` 
| `gmc_byte2tb(License_Usage)` 
| where License_Usage_MB>=0 
| bin _time span=1d 
| stats 
    sum(License_Usage) AS License_Usage
    by _time idxcluster_label Index_Name 
| timechart
    sum(License_Usage) AS License_Usage
    by Index_Name fixedrange=f span=1d 
| foreach ""*"" 
    [ eval <<FIELD>>=round('<<FIELD>>'/1024/1024/1024, 3)]",0
"GMC-006","Jobs Drifited from golden configuration",changes,"Scheduled_Jobs","","index=`setup_summary_index` search_name=""splunk_rest_saved_searches_sh_summary_data"" earliest=-30d@d latest=-7d@d 
| fields shcluster_label app savedsearch_name savedsearch_type description author disabled is_scheduled sharing cron_schedule realtime_schedule schedule_priority schedule_window updated allow_skew acl_perms_read acl_perms_write correlationsearch_enabled dispatch_earliest_time dispatch_latest_time savedsearch_search 
| rename 
    updated as old_updated
    disabled AS old_disabled
    is_scheduled as old_is_scheduled 
    cron_schedule as old_cron_schedule 
    dispatch_earliest_time as old_dispatch_earliest_time
    dispatch_latest_time as old_dispatch_latest_time 
    schedule_priority as old_schedule_priority 
    schedule_window as old_schedule_window
    allow_skew as old_allow_skew
    realtime_schedule as old_realtime_schedule 
    savedsearch_type as old_savedsearch_type 
    savedsearch_search as old_savedsearch_search 
| stats latest(*) as * by shcluster_label app savedsearch_name 
| join shcluster_label app savedsearch_name 
    [ search index=`setup_summary_index` search_name=""splunk_rest_saved_searches_sh_summary_data"" earliest=-7d@d 
    | fields shcluster_label app savedsearch_name savedsearch_type description author disabled is_scheduled sharing cron_schedule realtime_schedule schedule_priority schedule_window updated allow_skew acl_perms_read acl_perms_write correlationsearch_enabled dispatch_earliest_time dispatch_latest_time savedsearch_search 
    | rename 
        updated AS new_updated
        disabled as new_disabled 
        is_scheduled as new_is_scheduled
        cron_schedule as new_cron_schedule 
        dispatch_earliest_time as new_dispatch_earliest_time 
        dispatch_latest_time as new_dispatch_latest_time 
        schedule_priority as new_schedule_priority
        schedule_window as new_schedule_window 
        allow_skew as new_allow_skew
        realtime_schedule as new_realtime_schedule 
        search as new_savedsearch_search
        savedsearch_type as new_savedsearch_type 
        savedsearch_search as new_savedsearch_search 
    | stats latest(*) as * by shcluster_label app savedsearch_name ] 
| where old_disabled!=new_disabled OR old_is_scheduled!=new_is_scheduled OR old_cron_schedule!=new_cron_schedule 
| convert ctime(*_updated) 
| table shcluster_label app savedsearch_name old_updated new_updated old_disabled new_disabled old_is_scheduled new_is_scheduled old_cron_schedule new_cron_schedule",0
"GMC-007","Find users with empty LOB information",identities,Dashboards,,"| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE
    count
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    BY host Search_Activity.Audit_Search.user1 
| `gmc_drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename user1 As user 
| `get_shcluster_info(host)` 
| `get_identity_info(shcluster_label,user)` 
| `get_identity_info(user)` 
| table shcluster_label user emp_name emp_lob emp_dep 
| where isnull(emp_lob) 
| stats count by user 
| search NOT 
    [| inputlookup splunk_identities_exceptions_sh_csv_lookup 
    | fields identity 
    | rename identity As user] 
| rename user as identity 
    `gmc_comment(""| outputlookup append=true splunk_identities_exceptions_sh_csv_lookup"")`",0
"GMC-008","Generate Random Daily License Usage Data for ML Forecasting",license,"Indexer_Cluster","","| gentimes start=1/1/2019 end=07/18/2020 increment=1d 
| fields starthuman endhuman 
| eval _time=strptime(starthuman,""%a %B %d %H:%M:%S.%N %Y"") 
| eval low = 1073741824 , high = 2147483648 , idxcluster_label=""org_idx_cluster1"", Lic_Stack_Size=53687091200 
| eval Lic_Pool=mvappend(""IDX1_Pool"",""IDX2_Pool"",""IDX3_Pool"") 
| eval License_Usage = round(((random() % high)/(high)) * (high - low) + low) 
| stats 
    sum(License_Usage) AS License_Usage 
    latest(Lic_Stack_Size) AS Lic_Stack_Size
    BY _time idxcluster_label Lic_Pool 
| eval search_name=""splunk_internal_license_rollover_summary_idx_summary_tracker"" , type=""RolloverSummary"" 
| table _time search_name idxcluster_label Lic_Pool type Lic_Stack_Size License_Usage 
| collect index=`setup_summary_index` testmode=true",0
"GMC-009","Users Logged into the Platform Today","access,health_assessments",Dashboards,"","index=`setup_summary_index` search_name= ""splunk_audit_user_login_idx_summary_tracker "" 
| makemv delim=""|"" Splunk_Instance 
| makemv delim=""|"" Login_Source 
| makemv delim=""|"" OS_Name 
| makemv delim=""|"" OS_Version 
| makemv delim=""|"" Browser_Name 
| makemv delim=""|"" Browser_Version 
| search Splunk_Instance=`setup_gmc_search` 
| fields _time Splunk_Instance User Login_Source Latest_Access OS_Name OS_Version Browser_Name Browser_Version 
| `get_shcluster_label(Splunk_Instance)` 
| `get_identity_info(shcluster_label,User)` 
| stats 
    latest(Latest_Access) as Latest_Access 
    latest(Login_Source) as Login_Source 
    latest(OS_Name) as OS_Name 
    latest(OS_Version) as OS_Version 
    latest(emp_name) as Name 
    latest(Browser_Name) as Browser_Name 
    latest(Browser_Version) as Browser_Version
    latest(emp_city) as emp_city
    latest(emp_region1) as emp_region1
    latest(emp_country) as emp_country
    By shcluster_label User 
| sort 0 - Latest_Access 
| `strftime_format(Latest_Access)` 
| `get_iplocation_info2(Login_Source)` 
| table shcluster_label Latest_Access Name User emp_city emp_region1 emp_country",0
"GMC-010","Users not logging into the platform in more than (30) Days","health_assessments",Dashboards,,"| from lookup:splunk_rest_identities_kv_store_lookup 
| `get_instance_roles(Splunk_Instance)` 
| `get_splunk_roles_info(shcluster_label,splunk_role_map)` 
| stats 
    Values(email) As email
    Values(emp_dep) As emp_dep
    Values(emp_lob1) As emp_lob1
    Values(emp_name) As emp_name
    Values(emp_status) As emp_status
    Values(emp_title) As emp_title
    Values(emp_type) As emp_type
    Values(endDate) As endDate
    Values(emp_manager) As emp_manager
    By shcluster_label identity 
| lookup splunk_index_audit_user_login_tracker_sh_kv_store_lookup User AS identity OUTPUTNEW Latest_Access Splunk_Instance Login_Source 
| eval time_diff=now()-Latest_Access 
| eval Days_Latest_Access=30*24*60*60 
| where time_diff > Days_Latest_Access AND emp_type!= ""Service "" AND identity!= ""admin "" 
| `gmc_convert_runtime(time_diff)` 
| `strftime_format(Latest_Access)` 
| rename time_diff As  ""Time Since Latest Login "" 
| table emp_name identity Latest_Access shcluster_label  ""Time Since Latest Login "" emp_title emp_dep emp_lob1 emp_type emp_status 
| `rename_identity_fields` 
| `rename_common_fields`",0
"GMC-011","Dashboard Accessed in the last (30) Days","access,health_assessments",Dashboards,"","index=`setup_summary_index` search_name=splunk_internal_web_access_idx_summary_tracker earliest=-30d App!=""global_monitoring_console"" NOT Dashboard_Name IN (home,setup)
| fields _time User Splunk_Instance App OS_Name avg_spent Dashboard_Name Latest_Access 
| `get_shcluster_label(Splunk_Instance)` 
| `get_identity_info(shcluster_label, User)` 
| stats 
    Dc(User) As Num_Users 
    Values(emp_lob) As emp_lob1 
    Values(emp_dep) As emp_dep
    Latest(_time) As _time
    By shcluster_label App Dashboard_Name 
| table  shcluster_label App Dashboard_Name _time emp_name emp_title emp_dep emp_lob1 emp_type emp_status Num_Users 
| `strftime_format(_time)` 
| `rename_identity_fields` 
| `rename_common_fields`",0
"GMC-012","New Dashboards Created or Updated in the last (7) Days","dashboards,changes,health_assessments",Dashboards,,"| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| eval time_diff=now()-updated 
| eval Days_Latest_Access=7*24*60*60 
| where time_diff < Days_Latest_Access 
| `strftime_format(updated)` 
| `gmc_convert_runtime(time_diff)` 
| `get_identity_info(shcluster_label, author)` 
| `get_identity_info(author)` 
| table updated shcluster_label label title app sharing author emp_name emp_title emp_dep emp_lob1 emp_type emp_status time_diff acl_perms_read acl_perms_write description 
| rename time_diff As ""Time Since Latest Login"" 
| `rename_dashboards_fields` 
| `rename_common_fields` 
| `rename_identity_fields`",0
"GMC-013","Splunk Knowledge Objects that has been created or updated in the last (30) Days","changes,health_assessments",Dashboards,"","index=`setup_summary_index` search_name IN (splunk_rest_admin_eventtypes_sh_summary_data, splunk_rest_admin_lookup_table_files_sh_summary_data, splunk_rest_admin_transforms_lookup_sh_summary_data, splunk_rest_configs_conf_props_sh_summary_data, splunk_rest_data_macros_sh_summary_data, splunk_rest_data_models_sh_summary_data, splunk_rest_data_props_calcfields_sh_summary_data, splunk_rest_data_props_extractions_sh_summary_data, splunk_rest_data_props_fieldaliases_sh_summary_data, splunk_rest_data_props_lookups_sh_summary_data, splunk_rest_data_transforms_extractions_sh_summary_data, splunk_rest_data_ui_views_sh_summary_data, splunk_rest_saved_searches_sh_summary_data) NOT author IN (system, admin, splunk-system-user) updated=* earliest=-30d 
| eval time_diff = now() - updated 
| eval Days_Latest_Access = 30*24*60*60 
| where time_diff < Days_Latest_Access 
| `get_ko_type` 
| eval title = case (
    search_name=""splunk_rest_admin_lookup_table_files_sh_summary_data"", Filename,
    search_name=""splunk_rest_admin_transforms_lookup_sh_summary_data"", Lookup,
    search_name=""splunk_rest_saved_searches_sh_summary_data"", savedsearch_name,
    search_name=""splunk_rest_data_props_calcfields_sh_summary_data"", name,
    match(search_name, ""splunk_rest_data_props_fieldaliases_sh_summary_data|splunk_rest_data_props_extractions_sh_summary_data|splunk_rest_data_props_lookups_sh_summary_data""), attribute,
    true(), title) 
| stats 
    Latest(updated) As updated 
    Latest(time_diff) As time_diff
    earliest(sharing) AS start_share
    latest(sharing) AS end_share
    dc(sharing) AS share_states
    count by shcluster_label Knowledge_Object_Type app author title 
| eval sharing=end_share, sharing_change=if(share_states>1, ""From "". start_share ."" to "". end_share, ""No change""), sharing_change=if(start_share=end_share, ""No change"", sharing_change) 
| `get_identity_info(shcluster_label,author)` 
| `get_identity_info(author)` 
| `strftime_format(updated)` 
| `gmc_convert_runtime(time_diff)` 
| table shcluster_label Knowledge_Object_Type title app sharing sharing_change author emp_name emp_title emp_dep emp_lob1 emp_type emp_status updated time_diff 
| `rename_identity_fields` 
| `rename_common_fields` 
| rename Knowledge_Object_Type As ""Knowledge Object Type"", title As ""Knowledge Object Name"" app As App updated As ""Knowledge Object Update Time"" time_diff As ""Updated in the last"" author As User, sharing_change AS ""Sharing Change""",0
"GMC-014","Jobs not written using | tstats",es,"Scheduled_Jobs",,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where disabled=""0"" AND is_scheduled=""1"" AND shcluster_label=""es"" 
| search savedsearch_search!=""*tstats*"" 
| `get_identity_info(author)` 
| table shcluster_label app sharing savedsearch_name savedsearch_type author cron_schedule realtime_schedule schedule_priority schedule_window allow_skew savedsearch_search",0
"GMC-015","Jobs that outputs to a summary index","health_assessments","Scheduled_Jobs","we need to decrease the dispatch earliest time according to the schedule. For example: if the job runs 3 times per day, set the earliest time to -9h@h and the reason we added an additional hour is so we don't miss any event although this will cause duplicates.  If the job is business critical increase earliest to a maximum of -24h@h","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| makemv delim="","" actions 
| search shcluster_label=""*"" disabled=0 is_scheduled=1 AND (savedsearch_search=""* | *collect *"" OR actions=""*summary*"") 
| `strftime_format(updated)` 
| `strftime_format(next_scheduled_time)` 
| `get_identity_info(author)` 
| table shcluster_label author emp_name app sharing savedsearch_name updated savedsearch_type dispatch_earliest_time dispatch_latest_time cron_schedule next_scheduled_time actions summary_index_name savedsearch_search",0
"GMC-016","Jobs failed with stats including failure reasons and user info","failures,health_assessments","Scheduled_Jobs",,"| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE
    count As Num_Failures
    Max(Search_Activity.Audit_Search.event_count) AS event_count 
    Max(Search_Activity.Audit_Search.dispatch_time1) AS dispatch_time 
    Max(Search_Activity.Audit_Search.exec_time) AS exec_time 
    Max(Search_Activity.Audit_Search.scan_count1) AS scan_count 
    Max(Search_Activity.Audit_Search.search_et) AS search_et 
    Max(Search_Activity.Audit_Search.search_startup_time) AS search_startup_time 
    Max(Search_Activity.Audit_Search.searched_buckets) AS searched_buckets 
    Max(Search_Activity.Audit_Search.total_run_time1) AS total_run_time 
    Latest(Search_Activity.Audit_Search.search_lt) AS search_lt 
    Latest(Search_Activity.Audit_Search.search_id1) AS search_id 
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    index=_audit
    AND Search_Activity.Audit_Search.search_type1 = ""scheduled""
    AND Search_Activity.Audit_Search.info1 = ""failed""
    BY host Search_Activity.Audit_Search.savedsearch_name1 Search_Activity.Audit_Search.user1 
| `gmc_drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename user1 As user, savedsearch_name1 As savedsearch_name 
| `get_shcluster_label(host)` 
| `get_normalized_search_id(search_id)` 
| fields shcluster_label search_id_normalized user savedsearch_name event_count dispatch_time exec_time scan_count search_et search_lt search_startup_time searched_buckets total_run_time Num_Failures 
| `get_search_jobs_info(shcluster_label,search_id_normalized)` 
| `ustime_format(search_et)` 
| `ustime_format(search_lt)` 
| `ustime_format(exec_time)` 
| `ustime_format(dispatch_time)` 
| `get_identity_info(user)` 
| `get_saved_searches_info(shcluster_label,savedsearch_name)` 
| eval total_run_time_human=total_run_time 
| `gmc_convert_runtime(total_run_time_human)` 
| stats sum(Num_Failures) As Num_Failures Max(*) as * by shcluster_label savedsearch_search 
| table shcluster_label user emp_name email savedsearch_name Num_Failures total_run_time total_run_time_human searched_buckets scan_count event_count dispatch_time exec_time search_et search_lt search_startup_time error_messages savedsearch_search",0
"GMC-017","Jobs running for > 1h as the maximum run time over the last 7 days","longrunning,health_assessments","Scheduled_Jobs","","index=`setup_summary_index` search_name=splunk_internal_scheduler_jobs_idx_summary_tracker earliest=-7d@d 
| table _time Splunk_Instance App User Savedsearch_Name Scheduled_Time Dispatch_Time Priority Window_Time Run_Time Result_Count 
| `get_shcluster_label(Splunk_Instance)` 
| stats 
    Min(Run_Time) As MinRun_Time
    Max(Run_Time) As MaxRun_Time
    Avg(Run_Time) As AvgRun_Time
    Max(Result_Count) as MaxResult_Count
    dc(_time) as Number_of_Executions
    Latest(_time) As _time
    By shcluster_label App User Savedsearch_Name 
| `get_identity_info(User)` 
| eval MinRun_Time=round(MinRun_Time,2), MaxRun_Time=round(MaxRun_Time,2), AvgRun_Time=round(AvgRun_Time,2) , MaxRun_Time_Human=MaxRun_Time 
| eval Total_Run_Time = Number_of_Executions * MaxRun_Time , Total_Run_Time_Human=Total_Run_Time 
| `get_saved_searches_info(shcluster_label,App,Savedsearch_Name)` 
| `gmc_convert_runtime(MaxRun_Time_Human)` 
| `gmc_convert_runtime(Total_Run_Time_Human)` 
| `ustime_format(updated)` 
| sort 0 - MaxRun_Time 
| where MaxRun_Time > 1800 
| table _time shcluster_label App User emp_name email Savedsearch_Name updated cron_schedule dispatch_earliest_time dispatch_latest_time realtime_schedule schedule_priority schedule_window allow_skew MaxRun_Time_Human MaxRun_Time AvgRun_Time MinRun_Time MaxResult_Count Number_of_Executions Total_Run_Time Total_Run_Time_Human savedsearch_search",0
"GMC-018","Report on lookup file size in MB by cluster","searchbundle,health_assessments",Lookups,,"index=_audit isdir=0 size lookups action=update OR action=created OR action=modified OR action=add NOT action=search path=""*/lookups/*"" NOT path IN (""*/lookups/README*"") 
| where isnotnull(size) 
| `get_cluster_label(host)` 
| stats Latest(size) As size by cluster_label file_name path 
| rex field=path ""/opt/splunk/etc/(apps|slave-apps)/(?<app1>.*?)/"" 
| rex field=path ""/opt/splunk/etc/users/(?<app2>.*?)/"" 
| rex field=path ""/opt/splunk/etc/shcluster/apps/(?<app3>.*?)/"" 
| eval app=coalesce(app1,app2,app3) 
| `gmc_byte2human(size,3)` 
| table cluster_label file_name path app size size_MB",0
"GMC-019","Report on Search Head Memory %, CPU % And Load Average","cpu,mem,health_assessments",Infrastructure,,"index=_introspection sourcetype=splunk_resource_usage component=Hostwide 
| rename data.* As *
| `get_shcluster_label(host)`
| where shcluster_label=""xyz""
    | eval cpu_pct=cpu_system_pct+cpu_user_pct, mem_used_gb=round(mem_used/1024,1), mem_perc=round(mem_used/mem*100,2)
| timechart Max(cpu_pct) As cpu_pct Max(normalized_load_avg_1min) As normalized_load_avg_1min Max(mem_perc) As mem_perc  Max(mem_used_gb) As mem_used_gb span=1m",0
"GMC-020","Report on Search Head Memory %, CPU % and Memory Used by Cluster by App","perprocess,health_assessments",Infrastructure,,"index=_introspection sourcetype=splunk_resource_usage component=PerProcess (search_group=dmc_group_search_head OR host=sh-i-*)
| rename data.* As * 
| rename search_props.* As * 
| `get_shcluster_label(host)` 
| where shcluster_label=""sh123"" AND app=""App123""
| timechart Max(normalized_pct_cpu) As normalized_pct_cpu Max(pct_memory) As pct_memory Max(mem_used) As mem_used span=5m",0
"GMC-021","Retrieves Knowledge Objects Changes from the GMC Summary Index",changes,"Knowledge_Objects","Supported arguments: Reports_Alerts, Data_Models, Event_Types, Field_Aliases, Calculated_Fields, Field_Extractions, Field_Transformations, Lookup_Table_Files, Lookup_Definitions, Automatic_Lookups, Dashboards, Macros, Sourcetypes

(splunk_internal_splunkd_ui_access_ko_changes_idx_summary_tracker)","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup
| fields shcluster_label,Splunk_Instance,app,sharing,title
| `get_ko_changes(shcluster_label,Dashboards,title,5)`",0
"GMC-022","Job Changes from the GMC Summary Index",changes,"Scheduled_Jobs","splunk_rest_saved_searches_sh_summary_tracker","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| fields shcluster_label Splunk_Instance app sharing author savedsearch_name 
| `get_savedsearch_changes(shcluster_label,app,savedsearch_name,1)`",0
"GMC-023","How to create a tracking table",tracking,"Scheduled_Jobs",,"index = ... sourcetype = ... field = ... earliest=-24h@h latest=+0s 
| stats
    Min(_time) As firstTime
    Max(_time) As lastTime
    By <splitby> 
| inputlookup Append=True tracking-table 
| stats
    Min(firstTime) As firstTime
    Max(lastTime) As lastTime
    By <splitby> 
| where Strptime('lastTime', %s"") >= Relative_Time(Now(), ""-30d"") 
| outputlookup Override_If_Empty=False tracking-table 
| stats count",0
"GMC-024","Jobs created using Dashboard Schedule PDF Delivery","health_assessments",Dashboards,,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| `get_identity_info(shcluster_label,author)` 
| `get_identity_info(author)` 
| `strftime_format(updated)` 
| makemv delim="","" actions 
| search disabled=""0"" is_scheduled=""1"" savedsearch_name=""_ScheduledView__*"" 
| table shcluster_label app sharing savedsearch_name description updated author managedBy emp_name cron_schedule actions email_to",0
"GMC-025","Total Number of Buckets in a Cluster using _introspection against all Indexers","buckets,cloud,health_assessments","Indexer_Cluster",,"index=_introspection sourcetype=splunk_disk_objects component IN (Indexes) (host=idx-i-* OR search_group=dmc_group_indexer) earliest=-14d@d latest=-0d@d 
| rename data.* as * 
| bin _time span=1d 
| stats Max(total_bucket_count) As total_bucket_count By _time host name 
| stats sum(total_bucket_count) as total_bucket_count by _time host 
| timechart sum(eval(round(total_bucket_count/1000/1000,2))) as total_bucket_count",0
"GMC-026","Total Number of Buckets in a Cluster using REST against all Indexers","buckets,rest,health_assessments","Indexer_Cluster",,"| rest /services/cluster/master/peers splunk_server=local timeout=0 
| stats sum(bucket_count) AS bucket_count_all 
| eval bucket_count = round(bucket_count_all / 1000 / 1000,2).""M""",0
"GMC-027A","Lookup or Summary Indexing Generating Jobs outputting to lookups and may not end with stats count","lookups,health_assessments,summary_indexing","Scheduled_Jobs",,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where disabled=""0"" AND is_scheduled=""1"" AND shcluster_label!=""cm"" 
| search savedsearch_search IN (""*|*outputlookup *"", ""*|*collect *"") savedsearch_search!=""*|*stats count"" 
| `get_identity_info(author)` 
| table shcluster_label app savedsearch_name savedsearch_search",0
"GMC-028","Splunk Search Head Cluster (SHC) Scheduler Skips Analysis","skips,health_assessments",Scheduler,"This search excludes data model summarization and Jobs that are configured to use the continuous scheduling method i.e. realtime_schedule=0 or was disabled

What is skipped vs deferred ?
- Behavior depends on scheduling mode (real-time or continuous scheduling)
- Realtime scheduling mode searches will report a status of: completed/skipped
- Continuous scheduling mode searches will report a status of: completed/deferred,Use this example to automatically email results to users","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE 
    Latest(Search_Activity.Internal_Scheduler.info2) AS info
    Values(host) as host
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Internal_Scheduler
    index=_internal
    AND Search_Activity.Internal_Scheduler.info2 IN (skipped, delegated_remote_completion) 
    AND Search_Activity.Internal_Scheduler.search_type2 != ""*acceleration""
        earliest=-4h@h latest=-0h@h
    BY _time span=1s Search_Activity.Internal_Scheduler.search_id2 
| `gmc_drop_dm_object_name(Search_Activity.Internal_Scheduler)` 
| rename search_id2 As search_id , info AS status 
| lookup splunk_rest_assets_kv_store_lookup Splunk_Instance As host OUTPUTNEW Splunk_Roles 
| where Splunk_Roles=""shc_member"" OR Splunk_Roles=""shc_captain"" 
| timechart span=1h
    count(eval(status==""delegated_remote_completion"")) AS SHC_COMP_EXEC
    count(eval(status==""skipped"")) AS Skipped_EXEC
    count(eval(status==""delegated_remote_completion"" OR status==""skipped"")) AS SHC_TOT_EXEC 
| eval Skip_Ratio = round ( ( Skipped_EXEC / SHC_TOT_EXEC ) * 100, 0) 
| fields - SHC_TOT_EXEC",0
"GMC-029","Jobs with non-ascii characters in the Job Search String","non-ascii-chars,health_assessments","Scheduled_Jobs",,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where shcluster_label=""stack-shc"" 
| eval savedsearch_search_rex=savedsearch_search 
| rex field=savedsearch_search_rex mode=sed ""s/[^[:ascii:]]/NON-ASCII/g"" 
| search savedsearch_search_rex=""*NON-ASCII*"" 
| table shcluster_label app savedsearch_search savedsearch_search_rex",0
"GMC-030","Jobs with non-ascii characters in the Job Name","non-ascii-chars,health_assessments","Scheduled_Jobs",,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where shcluster_label=""stack-shc"" 
| eval savedsearch_name_rex=savedsearch_name 
| rex field=savedsearch_name_rex mode=sed ""s/[^[:ascii:]]/NON-ASCII/g"" 
| search savedsearch_name_rex=""*NON-ASCII*"" 
| table shcluster_label app savedsearch_name savedsearch_name_rex",0
"GMC-031","Search Head Cluster Jobs that Skipped vs Succeeded analysis using the GMC data model","shc,skips,health_assessments",Scheduler,,"| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE 
    `tstats_gmc_internal`
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Internal_Scheduler
    index=_internal
    AND Search_Activity.Internal_Scheduler.info2 IN (success, skipped, continued, delegated_remote_completion) 
    AND Search_Activity.Internal_Scheduler.search_type2 != ""*acceleration""
    earliest=-1d@d latest=-0d@d
    BY _time Search_Activity.Internal_Scheduler.search_id2 
| `gmc_drop_dm_object_name(Search_Activity.Internal_Scheduler)` 
| rename search_id2 As search_id , info AS status 
| fields _time host app concurrency_category concurrency_context concurrency_limit dispatch_time priority reason result_count total_run_time savedsearch_id savedsearch_name scheduled_time search_id search_type skipped_count status user window_time alert_actions event_message 
| timechart span=1h 
    count(eval(status==""delegated_remote_completion"")) AS SHC_COMP_EXEC
    count(eval(status==""skipped"")) AS Skipped_EXEC
    count(eval(status==""delegated_remote_completion"" OR status==""skipped"")) AS SHC_TOT_EXEC 
| eval Skip_Ratio = round ( ( Skipped_EXEC / SHC_TOT_EXEC ) * 100, 0) 
| fields - SHC_TOT_EXEC",0
"GMC-032","Sendresults Example Code",sendresults,Commands,,"| search NOT email IN (""email123@doamin.com"")
| rename email As email_to
| `sendresults(""testing123@test.com"",""testing123@test.com"",""Take Action: Long Running Search Impacting Splunk Environment"",""TAKE ACTION: Please review the search(es) listed below. We recommend the search be stopped, then updated with any steps to reduce the length of time to complete. If no longer needed, the search should be deleted.  Please engage the Log Analytics team if you have any questions (add a link to how we are to be engaged)"")`",0
"GMC-033","Jobs changed in the last N days using the GMC summary Index",changes,"Scheduled_Jobs","","index=`setup_summary_index` search_name=splunk_rest_saved_searches_sh_summary_data shcluster_label=* is_scheduled=1 disabled=0 earliest=-30d@d 
| eval days_last_updated=round((now() - updated) / 86400 , 0) 
| stats count by shcluster_label app author savedsearch_name days_last_updated updated 
| `strftime_format(updated)` 
| where days_last_updated < 15",0
"GMC-034","GMC Btool Configuration Analysis Summary Indexing","btool,health_assessments","Configuration_Files","","index=`setup_summary_index` sourcetype=""splunk:config:btool:web"" 
| rex ""etc/((apps|master-apps|slave-apps)/)?[^/]+/(default|local)/(?<file>\w+\.conf)\s+\[(?<stanza>.+?)\]"" 
| multikv noheader=t 
| rex ""(?<SPLUNK_HOME>.*?)/etc/(?<app_folder>apps|master-apps|system|slave-apps)/((?<app>.*)/)?(?<directory>default|local)/(?<file>\w+\.conf)""",0
"GMC-035","GMC Job Execution Details including GMC DM in the last 24 hours","gmc,health_assessments",GMC,,"| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE 
    latest(Search_Activity.Internal_Scheduler.dispatch_time2) AS dispatch_time
    latest(Search_Activity.Internal_Scheduler.scheduled_time) AS scheduled_time
    Max(Search_Activity.Internal_Scheduler.total_run_time2) AS max_run_time
    Latest(Search_Activity.Internal_Scheduler.search_type2) As search_type
    Values(Search_Activity.Internal_Scheduler.info2) AS info
    latest(Search_Activity.Internal_Scheduler.user2) AS user
    latest(Search_Activity.Internal_Scheduler.app2) As app
    values(host) as host
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Internal_Scheduler
    index=_internal
    AND Search_Activity.Internal_Scheduler.app2 = ""*global_monitoring_console""
    AND Search_Activity.Internal_Scheduler.search_type2 IN (""datamodel_acceleration"", ""scheduled"")
    earliest=-0d@d
    BY Search_Activity.Internal_Scheduler.savedsearch_name2 
| `gmc_drop_dm_object_name(Search_Activity.Internal_Scheduler)` 
| rename savedsearch_name2 As savedsearch_name 
| eval savedsearch_name=if(search_type=""datamodel_acceleration"", ""GMC Data Model"", savedsearch_name) 
| convert ctime(dispatch_time) ctime(scheduled_time) 
| `get_shcluster_label(host)` 
| `get_saved_searches_info(shcluster_label,app,savedsearch_name)` 
| `cron_descriptor(cron_schedule)` 
| table shcluster_label savedsearch_name description dispatch_as dispatch_earliest_time dispatch_latest_time cron_schedule cron_schedule_described allow_skew scheduled_time dispatch_time user info max_run_time realtime_schedule",0
"GMC-036","GMC Macros List with details",macros,GMC,"","| inputlookup splunk_rest_data_macros_sh_kv_store_lookup where app=""*global_monitoring_console"" 
| eval Macro_Class = case ( 
    match(title, ""_reference""), ""Gather Reference Info"", 
    match(title, ""_usage""), ""Gather Usage Info"", 
    match(title, ""^get_""), ""Gather Info"", 
    match(title, ""^rename_""), ""Fields Renames"", 
    match(title, ""^from_|^tstats_""), ""Get Data Model Data"", 
    match(title, ""^normalize_""), ""Normalize Fields"", 
    match(title, ""time""), ""Time Manipulation"", 
    match(title, ""^setup_""), ""Macros for GMC Setup Screen"", 
    match(title, ""^gmc_""), ""Various GMC Macros"") 
| fillnull value=""Other GMC Macros"" Macro_Class 
| `strftime_format(updated)` 
| table shcluster_label title Macro_Class description args updated 
| sort 0 title 
| `rename_macros_fields`",0
"GMC-037","GMC Dashboard List with details",dashboards,GMC,,"| inputlookup splunk_rest_data_ui_views_sh_kv_store_lookup where app=""*global_monitoring_console"" 
| eval Splunk_Tier = case ( match(label, ""IDX""), ""Indexer"", match(label, ""SH""), ""Search Head"", match(label, ""UF""), ""Forwarder"", true(), ""Other"") 
| table shcluster_label Splunk_Tier label description updated 
| `strftime_format(updated)` 
| `rename_dashboards_fields`",0
"GMC-038","GMC App Lookups with details",lookups,GMC,,"| inputlookup splunk_rest_admin_transforms_lookup_sh_kv_store_lookup where app=""*global_monitoring_console"" 
| `strftime_format(updated)` 
| eval Index=""GMC"" 
| lookup splunk_gmc_kb_csv_lookup Index Field_Name As Lookup OUTPUTNEW Field_Description 
| rename Field_Description As Description 
| table shcluster_label Lookup Type Description Filename Collection updated 
| `rename_lookup_fields`",0
"GMC-039","ES Cron Schedule Statistics using cron_schedule_map.csv",cron,"Scheduled_Jobs","This search uses an ES tabled shipped under SA-Utils/lookups/cron_schedule_map.csv","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where disabled=0 AND is_scheduled=1 AND cron_schedule!="""" AND isnotnull(cron_schedule) 
| search (NOT dispatch_earliest_time=rt* NOT dispatch_latest_time=rt* (savedsearch_name=""*Action History"" OR savedsearch_name=""*Gen"" OR savedsearch_name=""*Rule"")) 
| stats 
    values(savedsearch_name) as savedsearches 
    count by cron_schedule 
| lookup cron_schedule_map.csv cron_schedule OUTPUT c0,c5,c10,c15,c20,c25,c30,c35,c40,c45,c50,c55 
| eval c0=(c0 * count), c5=(c5 * count), c10=(c10 * count), c15=(c15 * count), c20=(c20 * count), c25=(c25 * count), c30=(c30 * count), c35=(c35 * count), c40=(c40 * count), c45=(c45 * count), c50=(c50 * count), c55=(c55 * count) 
| addcoltotals labelfield=savedsearches label=Total 
| fields + cron_schedule, savedsearches, count, c0, c5, c10, c15, c20, c25, c30, c35, c40, c45, c50, c55",0
"GMC-040","Splunk Cloud Virtual Core (SVC) Usage Analysis","svc,cloud,health_assessments",Usage,"Calculates SVC Usage by LOB","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=True
    count As Num_Searches
    Sum(Search_Activity.Audit_Search.total_run_time1) AS total_run_time
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    index=_audit
    earliest=-4d@d latest=-0d@d
    AND Search_Activity.Audit_Search.total_run_time1 > 0
    BY _time span=1d host Search_Activity.Audit_Search.search_id1 Search_Activity.Audit_Search.search_type1 Search_Activity.Audit_Search.info1 Search_Activity.Audit_Search.user1 
| `gmc_drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename info1 AS info, search_id1 As search_id , user1 As user , search_type1 As search_type 
| fields search_id host user search_type info total_run_time Num_Searches 
| stats 
    Sum(total_run_time) As user_total_run_time
    By _time user 
| eventstats sum(user_total_run_time) As grand_total_run_time 
| eval user_total_run_time_perc = user_total_run_time / grand_total_run_time 
| bin _time span=1d 
| join _time 
    [ search index=summary source=splunk-virtual-core-usage earliest=-30d@d latest=now 
    | bin _time span=1d 
    | stats 
        Max(total_util) As Utilization 
        Max(License) As License
        By _time] 
| eval svc_usage = user_total_run_time_perc * Utilization 
| `get_lm_a_account(user)` 
| lookup splunk_rest_identities_kv_store_lookup identity as user OUTPUTNEW emp_lob2 emp_dep 
| lookup splunk_rest_identities_kv_store_lookup identity as lm_a_account OUTPUTNEW emp_lob2 emp_dep 
| lookup splunk_identities_exceptions_sh_csv_lookup identity as user OUTPUTNEW emp_lob2 emp_dep 
| fillnull value=""NO-Market-Data"" emp_lob2 
| fillnull value=""NO-Department-Data"" emp_dep 
| search emp_lob2 IN (""*"") emp_dep IN (""*"") user IN (""*"") 
| stats 
    Sum(svc_usage) As svc_usage
    Dc(emp_dep) As Num_Departments
    Dc(user) As Num_Users
    Max(License) As License
    Max(Utilization) As Utilization
    Max(_time) As Day
    By emp_lob2 
| eval svc_perc = round(svc_usage/License *100, 2) 
| `ustime_format(Day)` 
| sort 0 - svc_usage 
| table emp_lob2 Num_Users Num_Departments svc_usage svc_perc Utilization License Day 
| rename emp_lob2 As ""Market"", emp_dep As Departments , svc_usage As ""SVC Usage"", svc_perc As ""SVC Usage %"", Num_Users As ""# of Users"", Num_Departments As ""# of Departments""",0
"GMC-041","Jobs that were disabled > 90 Days ago! (Please archive and Delete)","disabled,archive,health_assessments","Scheduled_Jobs","","index=`setup_summary_index` search_name=splunk_rest_saved_searches_sh_summary_data shcluster_label=""*"" disabled=1 
| eval days_last_updated=round((now() - updated) / 86400 , 0) 
| stats Latest(updated) As updated by shcluster_label app author savedsearch_name days_last_updated 
| `strftime_format(updated)` 
| where days_last_updated > 90
| stats count",0
"SPL-101","Index bucket info",buckets,Indexes,,"earliest=-30m index=_introspection component=indexes source=""/opt/splunk/var/log/introspection/disk_objects.log"" 
| dedup data.name host 
| stats sum(data.total_size) as indexSize min(data.bucket_dirs.home.event_min_time) as event_min_time max(data.bucket_dirs.home.event_max_time) as event_max_time sum(data.bucket_dirs.home.hot_bucket_count) as hot_bucket_count sum(data.bucket_dirs.home.warm_bucket_count) as warm_bucket_count sum(data.total_event_count) as total_event_count sum(data.total_raw_size) as total_raw_size sum(data.datamodel_summary_size) as DM_size by data.name 
| eventstats sum(indexSize) as totalDataSize count as indexCount 
| eval event_max_time=strftime(event_max_time,""%m/%d/%y %H:%M:%S"") 
| eval event_min_time=strftime(event_min_time,""%m/%d/%y %H:%M:%S"") 
| eval percOfTotal=('indexSize'/'totalDataSize')*100 
| fillnull value=0 
| sort - indexSize 
| eval ""Compression Rate""=(indexSize/total_raw_size)*100 
| rename data.name as ""Index Name"" event_max_time AS ""Latest Event"" event_min_time AS ""Earliest Event""",0
"GMC-074","Report Search Duration of any type","duration,health_assessments",SPL,"Report on search duration of any type","index=_audit TERM(action=search) ( TERM(info=completed) OR ( TERM(info=granted) apiStartTime ""search='search"")) NOT ""search_id='rsa_*"" 
| eval u=case( searchmatch(""user=splunk-system-user OR user=nobody OR search_id=*scheduler_*""), ""Scheduler"", searchmatch((""search_id='1*"")), ""AdHocUser"", 1=1, ""AdHocSaved"") 
| eval search_id=md5(search_id), 
    search_et=if(search_et=""N/A"", 0,
    search_et), search_lt=if(search_lt=""N/A"", exec_time, search_lt), 
    et_diff=case(exec_time>search_et, (exec_time-search_et)/60, 1=1, (search_lt-search_et)/60), searchStrLen=len(search) 
| stats partitions=10 
    sum(searchStrLen) AS searchStrLen
    count
    first(et_diff) AS et_diff
    first(u) as u
    values(search) AS search 
    BY search_id 
| search searchStrLen>0 et_diff=* count>1 
| eval et_range = case(et_diff<=0, ""WTF"", et_diff<2, ""0_1m"", et_diff<6, ""1_5m"", et_diff<11, ""2_10m"", et_diff<16, ""3_15m"", et_diff<=65, ""4_60m"", et_diff<=4*60+10, ""5_4h"", et_diff<=24*60+10, ""6_24h"", et_diff<=7*24*60+10, ""7_7d"", et_diff<=30*24*60+10, ""8_30d"", et_diff<=90*24*60+10, ""9_90d"", 1=1, ""10_>90d"") 
| chart count by et_range, u 
| eval Total=AdHocUser + AdHocSaved + Scheduler 
| eventstats sum(AdHocUser) AS uTotal sum(AdHocSaved) AS aTotal, sum(Scheduler) AS sTotal, sum(Total) AS tTotal 
| eval AdHocUserPerc=round((AdHocUser*100)/uTotal,3), AdHocSavedPerc=round((AdHocSaved*100)/aTotal,3), SchedulerPerc=round((Scheduler*100)/sTotal, 3), TotalPerc=round((Total*100)/tTotal, 3) 
| addcoltotals 
| eval et_range=if(isnull(et_range), ""8_Total"", et_range) 
| fields - aTotal sTotal tTotal, uTotal 
| rex mode=sed field=et_range ""s/\d+_(.*)/\1/g"" 
| accum TotalPerc AS TotalPercCumulative 
| eval TotalPercCumulative=if(TotalPercCumulative<101, round(TotalPercCumulative, 1), """")",0
"SPL-103","How Long is this Going to Take?",,SPL,,"#How Long is this Going to Take?#
source=history
| stats stdev(dur) as stdev, avg(dur) as avg
| eval soonest=avg-(3*stdev)
| eval latest=avg+(3*stdev)",0
"SPL-105","Calculate the relevancy of the search and sort the results in descending order",relevance,Miscellaneous,,"... disk error | relevancy | sort -relevancy",0
"SPL-106","Error Search Broken Down by Sourcetype",error,Miscellaneous,,"... error OR failed OR severe OR ( sourcetype=* ( 404 OR 500 OR 503 ) )| timechart count by host",0
"SPL-107","Return the average for each hour, of any unique field that ends with the string lay (for example, delay, xdelay, relay, etc)","avg,os,stats",Miscellaneous,"Return the average for each hour, of any unique field that ends with the string ""lay"" (for example, delay, xdelay, relay, etc)","... |  stats avg(*lay) BY date_hour",0
"SPL-108","Save the running total of count in a field called total_count",accum,Miscellaneous,"Save the running total of ""count"" in a field called ""total_count""","... | accum count AS total_count",0
"SPL-109","Calculate the sums of the numeric fields of each result",addtotals,SPL,"Put the sums in the field sum","... | addtotals fieldname=sum",0
"SPL-110","Return the average (mean) size for each distinct host",mean,Miscellaneous,"Return the average (mean) ""size"" for each distinct ""host""","... | chart avg(size) by host",0
"SPL-111","Return the ratio of the average (mean) size to the maximum delay for each distinct host and user pair","avg,delay,eval,os",SPL,"","... | chart eval(avg(size)/max(delay)) by host user",0
"SPL-112","Return the the maximum delay by size","bins,bucket",SPL,"where ""size"" is broken down into a maximum of 10 equal sized buckets","... | chart max(delay) by size bins=10",0
"SPL-114","Compute the difference between count and its previous value and store the result in countdiff","countdiff,delta",Miscellaneous,"","... | delta count AS countdiff",0
"SPL-115","Extracts out values like 7/01, putting them into the monthday attribute","erex,extract,rex",Miscellaneous,"Extracts out values like ""7/01"", putting them into the ""monthday"" attribute","... | erex monthday examples=""7/01""",0
"SPL-116","Comparing equal time ranges in one report",compare,SPL,"1 day is 86,400 seconds","... | eval _time = _time + 86400",0
"SPL-117","Extract field/value pairs pipe delimited","delim,extract",SPL,"Values of fields that are delimited by ""=:""","... | extract pairdelim=""|;"", kvdelim=""=:"", auto=f",0
"SPL-119","Return the first 20 results","head,top",Miscellaneous,,"... | head 20",0
"SPL-120","Highlight the terms login and logout",highlight,Miscellaneous,"Highlight the terms ""login"" and ""logout""","... | highlight login,logout",0
"SPL-123","Output the _raw field of your current search into _xml",output,Miscellaneous,"Output the ""_raw"" field of your current search into ""_xml""","... | outputtext",0
"SPL-124","Change any host value that ends with localhost to localhost","replace,web",SPL,"","... | replace *localhost with localhost in host",0
"SPL-125","Reverse the order of a result set",reverse,Miscellaneous,,"... | reverse",0
"SPL-126","REX Command Example",rex,Commands,"REX in the Search Bar - <<The field comes out as aaa in the interesting fields section>>. Your regex (between the """") needs to be correct for your sample to extract a value.","... | rex field=_raw ""\,\d\d\d\s(?<aaa>[^\s]+)""",0
"SPL-128","Anonymize the current search results",scrub,SPL,"","... | scrub",0
"SPL-129","Send search results to the specified email",email,Miscellaneous,,"... | sendemail to=""youremail@youraddress.com""",0
"SPL-130","Sort results by ip value in ascending order and then by url value in descending order",sort,Miscellaneous,"Sort results by ""ip"" value in ascending order and then by ""url"" value in descending order","... | sort ip, -url",0
"SPL-133","Create a timechart of average cpu_seconds by host","os,outlier","Resource_Usage","Create a timechart of average ""cpu_seconds"" by ""host"", and remove data (outlying values) that may distort the timechart's axis","... | timechart avg(cpu_seconds) by host | outlier action=tf",0
"SPL-134","Build a time series chart of web events by host and fill all empty fields with NULL","",SPL,"","... | timechart count by host | fillnull value=NULL",0
"SPL-135","Calculate the average value of CPU each minute for each host","os,span,timechart","Resource_Usage","Calculate the average value of ""CPU"" each minute for each ""host""","... | timechart span=1m avg(CPU) by host",0
"SPL-136","Graph the average thruput of hosts over time","avg,os",Miscellaneous,"Graph the average ""thruput"" of hosts over time","... | timechart span=5m avg(thruput) by host",0
"SPL-137","For the current search, keep only unique result",uniq,SPL,,"... | uniq",0
"SPL-138","Extract field/value pairs from XML formatted data","extract,xml",SPL,"xmlkv automatically extracts values between XML tags","... | xmlkv",0
"SPL-140","Day over Day",compare,SPL,,"index=""*"" error earliest=-1d@d latest=@d
| eval Series=""Yesterday""
| eval _time = _time + 86400
| append [ search tag=failure earliest=@d latest=now
| eval Series = ""Today"" ]
| timechart fixedrange=f span=30m count by Series",0
"SPL-141","Viewing data together",compare,SPL,,"index=""*"" error earliest=-30d latest=@d
| timechart span=1d count as dailyCount
| stats avg(dailyCount) as AveragePerDay
| appendcols [search error earliest=@d latest=now
| stats count as TodaysCount]",0
"SPL-143","Worldmap with unique visitors last 24 hours","map,ui",Web,"Example uses clientip and an access sourcetype.","index=""*"" sourcetype=""*access*""
| iplocation clientip
| stats dc(clientip) by Country
| geom geo_countries featureIdField=""Country"" earliest=-24h",0
"SPL-144","For each event, add a count field that represent the number of event seen so far ",streamstats,Web,"Including that event. i.e., 1 for the first event, 2 for the second, 3, 4 ... and so on","index=""*"" sourcetype=""*access*""
| streamstats count",0
"SPL-145","Create a timechart of the count of from web sources by host",timechart,"Resource_Usage","","index=""*"" sourcetype=""*access*""
| timechart count by host",0
"SPL-146","Total Unique Client IPs Over Time","unique,web",Web,,"index=""*"" sourcetype=""*access*""
| timechart distinct_count(FIELD) span=1h",0
"SPL-147","Search the access logs, and return the number of hits from the top 100 values of referer_domain","referrer,web",Web,"Search the access logs, and return the number of hits from the top 100 values of ""referer_domain""","index=""*"" sourcetype=""*access*""
| top limit=100 referer_domain
| stats sum(count)",0
"SPL-148","Bounce Rate (Enters and exits on the same page) (Pie Chart)","bounce,web",SPL,"","index=""*"" sourcetype=""*access*""
| transaction clientip maxpause=1h keepevicted=t mvlist=t
| eval user_type=case (eventcount=1,""Bounced"", eventcount<=5, ""2-5 pages"", eventcount<=10, ""6-10 pages"")
| top  limit=5000 user_type",0
"SPL-149","Detect ShellShock Attempts in Apache Logs",security,Web,,"index=""*"" sourcetype=""*access*"" http_method=POST request=""*{ :;};*"" OR request=""*/bin/*""",0
"SPL-150","Condition as Percentage",web,SPL,"Cart Conversion","index=""*"" sourcetype=""*access*"" method=GET
| stats count AS Page_Views, count(eval(action=""purchase"")) AS Actual_Purchases
| eval ""Conversion %"" =((Actual_Purchases*100)/Page_Views)
| rename Page_Views as Views Actual_Purchases as Purchases",0
"SPL-151","Password Non Compliance in Windows","windows,wineventlog",Security,"Return results for failed attempts to change password","index=""*"" sourcetype=""WinEventLog:Security"" EventCode=4723 â€¯Keywords=""Audit Failure"" #(WinEventLog:Security) This/these sourcetype/s must  exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"")
| rex ""Target\sAccount:\s+Security\sID:.*\\\(?<account>\S+)""
| stats count by Date, account, host | sort - Date",0
"SPL-152","Top Referer",referer,Web,,"index=""*"" sourcetype=*access*
| top referer
| fields referer percent",0
"SPL-153","Top User Agent","agent,user",Web,,"index=""*"" sourcetype=*access*
| top useragent",0
"SPL-154","Qualys Top 10 Vulnerabilities by Severity",qualys,Security,,"index=""*"" sourcetype=qualys_vm_detection HOSTVULN SEVERITY=3 OR 4 OR 5 TYPE=""CONFIRMED"" earliest=-30d@d
| dedup HOST_ID, QID | search STATUS!=""FIXED""
| join QID [search sourcetype=qualys_knowledgebase PATCHABLE=1 ]
| eval Published=strftime(strptime(PUBLISHED_DATETIME, ""%Y-%m-%d""), ""%m/%d/%Y"")
| join HOST_ID [search sourcetype=qualys_vm_detection HOSTSUMMARY OS=""Windows*"" NOT ""Windows Server*""
| where cidrmatch(""10.128.0.0/9"", IP)  ]
|  stats count(HOST_ID) as #_Hosts by QID, Published, TITLE, SEVERITY
| sort  -SEVERITY, 10 -#_Hosts",0
"SPL-155","Average Processing Time (field = processing_time - could be whatever field you want)",duration,SPL,"","index=""INDEX"" sourcetype=""SOURCETYPE""
| eval processingTime = tonumber(trim(processing_time, ""ms""))
| eval formattedTime = strftime(_time, ""%Y%m%d"")
| eval processingTimeBase = if (formattedTime = ""20121012"", processingTime, null)
| stats avg(processingTimeBase) as ""Average Processing Time""
| rangemap field=""Average Processing Time"" low=0-240 elevated=241-320 severe=321-400 default=severe",0
"SPL-156","List of Sourcetypes Sent by Forwarder","forwarder,uf",Sourcetypes,,"index=""_internal""
| where host!=splunk_server
| stats values(series) as Sourcetypes by host",0
"SPL-157","Traffic Volume by Forwarder","forwarder,nodata,traffic",Internal,,"index=""_internal"" source=""*metrics.lo*"" group=tcpin_connections NOT eventType=*
| eval sourceHost=if(isnull(hostname), sourceHost,hostname)
| search sourceHost=***
| timechart per_second(kb) by sourceHost WHERE max in top5 useother=f",0
"SPL-158","List Ports Forwarders are Using","ports,pwn,uf",Internal,,"index=""_internal"" source=""*metrics.lo*"" group=tcpin_connections NOT eventType=*Â 
| dedup sourceHost
| stats count by destPort",0
"SPL-159","Introspection â€“ Memory used by SID (Search ID)","memory,sid",Internal,,"index=""_introspection"" Â ""data.process""=splunkd
| timechart max(data.mem_used) by data.search_props.sid usenull=f useother=f",0
"SPL-160","Qualys Hosts not Scanned in 30 days+",qualys,Security,,"index=""qualys"" HOSTVULN earliest=-30d@d STATUS=""RE-OPENED""
| dedup HOST_ID, QID sortby +_time
| join HOST_ID [search index=qualys HOSTSUMMARY OS=""Windows*"" NOT ""Windows Server*""
| where cidrmatch(""10.128.0.0/9"", IP) ]
| timechart span=1d count(QID) by SEVERITY",0
"SPL-162","json mv extraction","fields,json,rename,rex",Miscellaneous,,"index=*
 | rename events{}.code AS a_c, events{}.message AS a_m, events{}.timestamp AS a_ts, events{}.priority as a_p # combine mv fields together using mvzip (to get tuples as comma-delim'd strings)
| eval b_combined = mvzip(mvzip(mvzip(a_c, a_m), a_ts), a_p) # get rid of the a_* fields, simply b/c we don't need them clogging up the ui
| fields - a_* # expand out the combined fields
| mvexpand b_combined # extract nicely named fields from the results (using the comma from mvzip as the delimiter)
| rex field=b_combined ""(?<e_c>[^,]*),(?<e_m>[^,]*),(?<e_ts>[^,]*),(?<e_p>[^,]*)"" # get rid of the combined field b/c we don't need it
| fields - b_* # urldecode the field that you care about
| eval e_m = urldecode(e_m)",0
"SPL-163","Sourcetype Searches",,Sourcetypes,,"index=*
 | stats count by sourcetype",0
"SPL-164","Find the top 5 ip addresses that are attempting to attack us","security,threat",Security,"Requires juniper:idp data","index=*
sourcetype = ""juniper:idp"" attack*
| top limit=5 src_ip",0
"SPL-165","Extract SQL Insert Params","eval,rex,sql,stream,timechart",SPL,"Extracts fields from a SQL Insert statement so that the values inserted into the database can be manipulated via splunk searches. In this case, it is used in conjunction with splunk stream & mysql, but should work with any source / database technology.","index=*
sourcetype=stream:mysql* query=""insert into*""
| rex ""insert into \S* \((?<aaa>[^)]+)\) values \((?<bbb>[^)]+)\)""
| rex mode=sed field=bbb ""s/\\\\\""//g""
| makemv aaa delim="",""
| makemv bbb delim="",""
| eval a_kvfield = mvzip(aaa, bbb)
| extract jam_kv_extract
| timechart span=1s per_second(m_value) by m_name",0
"SPL-166","Find Rare Processes","rare,security,windows",Security,"Find rarely seen windows processes. Might indicate custom malware.","index=*
sourcetype=winregistry
| rare process_image",0
"SPL-167","Malware Detection",malware,Security,,"index=*
| convert mktime(_time) as epoch
| sort 0 uri_host,client_ip,epoch
| delta epoch as epoch_delta
| search epoch_delta>0 epoch_delta<30
| chart count over epoch_delta by uri_host",0
"SPL-168","Convert field timestamp format to Epoch","convert,epoch",SPL,"Convert field timestamp format to Epoch. This does not include the meta data field _time.
Scenario: You have a non timestamp field that you need to convert to epoch time to perform statistics on within splunk. Hereâ€™s how you do it:","index=*
| eval Epoch_Time=strptime(Field_Date, ""%Y-%m-%d %H:%M:%S"")",0
"SPL-169","Convert Seconds to Hours Minutes Seconds","convert,time",SPL,"Take any field in splunk that outputs a value in seconds and change it to report in HH:MM:SS format","index=*
| eval HHMMSS=tostring(Field_In_Seconds, ""duration"")
| table HHMMSS",0
"SPL-170","Are you SURE your time range is correct?",internal,SPL,"You wouldn't be the first!. Search over all time to double check. Check for lag.","index=*
| eval time=_time
| eval itime=_indextime
| eval lag=(itime - time)/60
| stats avg(lag), min(lag), max(lag) by index host sourcetype",0
"SPL-171","Calculate the Difference in time Between two Fields",compare,SPL,"","index=*
| eval timeDiff = strptime(timeField1, ""%Y-%m-%d %H:%M:%S.%3N"") - strptime(timeField2, ""%Y-%m-%d%H:%M:%S.%3N"")
| table timeDiff",0
"SPL-172","Detect unauthorized admin activity via foreign country","geoip,map",Security,,"index=*
| geoip clientip as clientip
| table _time clientip client_country
| where client_country NOT (""Germany"" OR ""Austria"" OR ""Switzerland"")",0
"SPL-173","Splunk Server's Time",utils,Internal,,"index=*
| head 1
| eval tnow = now()
| fieldformat tnow=strftime(tnow, ""%c %Z"")
| table tnow",0
"SPL-174",Streamstats,streamstats,Miscellaneous,,"index=*
| head 5
| sort _time
| streamstats sum(bytes) as ASimpleSumOfBytes by clientip",0
"SPL-175","Rename the _ip field as IPAddress",rename,Miscellaneous,"Rename the ""_ip"" field as ""IPAddress""","index=*
| rename _ip as IPAddress",0
"SPL-178","Detect machines/applications who are potentially infected","security,threat",Security,"Detect machines/applications who are potentially infected and have active running malware on it. Even use it to detect fraud for shopping site orders coming from bad IP's. requirements:
machine data with external IP's + IP Reputation App","index=*
| stats count by src_ip dst_ip dst_port protocol
| lookup threatscore clientip as dst_ip
| sort â€“threatscore
| where threatscore>0",0
"SPL-179","Machines with Multiple Services",security,Security,"You can also filter it down with a additional | where ""Different Ports"" > 5.  Replace the wildcard * and or field names to be applicable to your data set(s.)","index=*
| stats count by src_ip dst_ip dst_port protocol
| stats dc(dst_port) as ""Different Ports"" by dst_ip",0
"SPL-180","Detect Account Sharing","security,windows",Security,"Detect Users who login from multiple IP's / User account Sharing
Requires: Login logs with Username + Source IP field extractions","index=*
| stats dc(src_ip) as ip_count by user",0
"SPL-181","Figure out how to show an SLA on a current graph",overlay,Miscellaneous,,"index=*
| timechart avg(responsetime) as responsetime
| eval SLA = 5",0
"SPL-182","Transaction Command Example",transaction,Commands,"Replace the wildcard * and or field names to be applicable to your data set(s.)","index=*
| transaction clientip
| where duration >1
| table clientip  host duration _time",0
"SPL-183","Group search results that have the same host and cookie",transaction,Commands,"occur within 30 seconds of each other, and do not have a pause greater than 5 seconds between each event into a transaction.  Replace the wildcard * and or field names to be applicable to your data set(s.)","index=*
| transaction host cookie maxspan=30s maxpause=5s",0
"SPL-184","Failed Attempt to initiate Remote Desktop session",windows,Security,"Must have Splunk app for windows installed. Works with Server 2003 and older.Â ","index=*  source=WinEventLog:Security sourcetype=WinEventLog:security Logon_Type=10 (EventCode=529 OR EventCode=530 OR EventCode=531 OR EventCode=532 OR EventCode=533 OR EventCode=534 OR EventCode=535 OR EventCode=536 OR EventCode=537 OR EventCode=539)
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, User_Name, Reason, host",0
"SPL-185","Convert Bytes to Megabytes",convert,Web,,"index=*  sourcetype=""*access*""
| eval megabytes=((bytes/1024)/1024)
| timechart sum(megabytes)",0
"SPL-186","Sparkline by Client IP",sparkline,Web,,"index=*  sourcetype=""*access*""
| stats sparkline count by clientip | sort -count",0
"SPL-187","File deletion attempts in Windows 2008","windows,wineventlog",Security,"File deletion attempts in Windows, which returns results based on any suer account who attempts to delete a file. This will return both successful and unsuccessful attempts. Requires Splunk App for Windows: 2008 and newer.","index=*  sourcetype=""WinEventLog:Security"" EventCode=4660 (Security_ID!=""NT AUTHORITY*"") (Security_ID!=""S-*"")
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, Account_Name, Process_Name, Keywords, host",0
"SPL-188","File deletion attempts in Windows 2003","windows,wineventlog",Security,"File deletion attempts in Windows, which returns results based on any suer account who attempts to delete a file. This will return both successful and unsuccessful attempts. Requires Splunk App for Windows: 2003 and older.Â ","index=*  sourcetype=""WinEventLog:Security"" EventCode=564 #(WinEventLog:Security) This/these sourcetype/s must  exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, Image_File_Name, Type, host
| sort - Date",0
"SPL-189","Password Non Compliance in Windows","windows,wineventlog",Security,"Return results for failed attempts to change password","index=*  sourcetype=""WinEventLog:Security"" EventCode=627 Type=""Failure Audit""
| eval Date=strftime(_time, ""%Y/%m/%d"") | stats count by Date, Target_Account_Name, host
| sort - Date",0
"SPL-190","Console Lock Duration","windows,wineventlog",Security,"","index=*  sourcetype=WinEventLog:Security (EventCode=4800 OR EventCode=4801) #(WinEventLog:Security) This/these sourcetype/s must  exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"")
| transaction host Account_Name startswith=EventCode=4800 endswith=EventCode=4801
| eval duration = duration/60
| eval duration=round(duration,2)
| table host, Account_Name, duration, Date
| rename duration as ""Console Lock Duration in Minutes""
| sort - date",0
"SPL-191","Removal of USB storage device",windows,Security,"This will detect if any storage device was removed from a Windows machine.","index=*  sourcetype=WinRegistry key_path=""HKLM\\system\\controlset*\\enum\\usbstor\\*"" process_image=""c:\\Windows\\System32\\svchost.exe"" registry_type=DeleteKey
| eval Date=strftime(_time, ""%Y/%m/%d %H:%M:%S"")
| rex ""key_path.*usbstor\S(?<DeviceType>.*)&ven\S(?<Vendor>.*)&prod\S(?<Product>\S*)&rev\S""
| stats count by Date, host, Vendor, Product, DeviceTypeâ€¯â€¯
| fields â€¯- countâ€¯
| sort â€¯- Date",0
"SPL-192","Simple Outlier Search","outliers,security,stdev",SPL,"Find outliers - hosts that have an error count which is greater than two standard deviations away from the mean.","index=* ""Failed Password""
| stats count by user
| eventstats avg(count) as avg_count stdev(count) as stdev_count
| where count>(avg_count + 2*stdev_count)
| sort  -count",0
"SPL-193","Find crazy errors","find,haystack,needle,pwn",SPL,,"index=* err* OR fail* OR crit* OR fatal* OR except*
| rex field=punct ""(?<smallpunct>.{5})""
| eval smallpunct= ""*"" + smallpunct
| stats first(_raw) as example count by smallpunct
| sort -count
| fields smallpunct,count,example",0
"SPL-194","Errors in the last 24 hours",errors,Internal,,"index=* error OR failed OR severe OR ( sourcetype=*access* ( 404 OR 500 OR 503 ) )",0
"SPL-195","Single Value: COMPARE",compare,SPL,,"index=* error earliest=-30d latest=@d
| timechart span=1h count
| eval Hour = strftime(_time,""%H"")
| stats avg(count) as AverageCount by Hour",0
"SPL-196","Seven Days Over One Day",compare,SPL,,"index=* error earliest=-7d@d latest=@d
| eval Series=""Yesterday""
| eval _time = _time + 86400
| append [ search error earliest=@d latest=now
| eval Series = ""Today"" ]
| timechart fixedrange=f span=30m count by Series",0
"SPL-197","Low Disk Space Alert for Windows Servers","disk,windows",Infrastructure,,"index=* eventtype=hostmon_windows Type=Disk host=""*"" FileSystem=""*"" DriveType=""*""
| dedup host, Name
| eval FreeSpacePct=round(FreeSpaceKB/TotalSpaceKB*100)
| eval TotalSpaceGB=round(TotalSpaceKB/1024/1024)
| eval FreeSpaceGB=round(FreeSpaceKB/1024/1024)
| search FreeSpacePct<10 TotalSpaceGB=""*""
| dedup host, Name, DriveType, TotalSpaceGB, FreeSpaceGB, FreeSpacePct
| table host, Name, DriveType, TotalSpaceGB, FreeSpaceGB, FreeSpacePct
| sort FreeSpacePct",0
"SPL-198","List Hosts added to Splunk by some _time","index,splunkd,uf",Internal,,"index=* host=*
| stats dc(host) as host by date_month",0
"SPL-199","Modification to File Permissions in Windows 2008 and newer","windows,wineventlog",Security,"Returns results for modifications of an individual file level permissions.","index=* source=""WinEventLog:Security"" sourcetype=""WinEventLog:Security"" EventCode=4670 (Security_ID!=""NT AUTHORITY*"") (Security_ID!=""S-*"")
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, Account_Name, Process_Name, Keywords, host
| sort - Date",0
"SPL-200","Logon Types within a windows environment with logon count","windows,wineventlog",Security,,"index=* source=""WinEventLog:security"" #(WinEventLog:security) This/these source/s must exist or edit to make search valid.
| eval LogonType=case(Logon_Type=""2"", ""Local Console Access"", Logon_Type=""3"", ""Accessing Network Folders or Files"", Logon_Type=""4"", ""Scheduled Task, Batch File, or Script"", Logon_Type=""5"", ""Service Account"", Logon_Type=""7"", ""Local Console Unlock"", Logon_Type=""8"", ""Network User Logon"", Logon_Type=""9"", ""Program launched with RunAs using /netonly switch"", Logon_Type=""10"", ""Remote Desktop via Terminal Services"", Logon_Type=""11"", ""Mobile Access or Network Domain Connection Resumed"")
| top limit=15 LogonType | eval percent = round(percent,2) . "" %""",0
"SPL-201","List of legitimate account names in Windows to identify the deviations from norm:Â ",windows,Security,,"index=* source=""WinEventLog:security"" (Logon_Type=2 OR Logon_Type=7 OR Logon_Type=10) (EventCode=528 OR EventCode=540 OR EventCode=4624)
| rex ""New\sLogon:\s+.*\s+Account\sName:\s+(?<UserName>\S+)"" | eval Account=coalesce(User_Name,UserName)
| stats count by Account | sort - count",0
"SPL-202","Failed Windows Remote Desktop Connection Attempt",windows,Security,"","index=* source=WinEventLog:Security sourcetype=WinEventLog:security Logon_Type=10 EventCode=4625
| eval Date=strftime(_time, ""%Y/%m/%d"")
| rex ""Failed:\s+.*\s+Account\sName:\s+(?<TargetAccount>\S+)\s""
| stats count by Date, TargetAccount, Failure_Reason, host
| sort - Date",0
"SPL-203","Clearing of Windows Audit Logs","audit,windows",Security,,"index=* source=WinEventLog:security (EventCode=1102 OR EventCode=517)
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Client_User_Name, host, index, Date
| sort - Date
| rename Client_User_Name as ""Account Name""",0
"SPL-205","Search common event codes for suspicious behavior","windows,wineventlog",Security,"Warning, this can take a while to run.Â ","index=* source=WinEventLog:security User!=SYSTEM User!=""LOCAL SERVICE"" User!=""NETWORK SERVICE""
| eval Trigger=case(EventCode=516, ""Audit Logs Modified"",EventCode=517, ""Audit Logs Modified"",EventCode=612, ""Audit Logs Modified"",EventCode=623, ""Audit Logs Modified"",EventCode=806, ""Audit Logs Modified"",EventCode=807, ""Audit Logs Modified"",EventCode=1101, ""Audit Logs Modified"",EventCode=1102, ""Audit Logs Modified"",EventCode=4612, ""Audit Logs Modified"",EventCode=4621, ""Audit Logs Modified"",EventCode=4694, ""Audit Logs Modified"",EventCode=4695, ""Audit Logs Modified"",EventCode=4715, ""Audit Logs Modified"",EventCode=4719, ""Audit Logs Modified"",EventCode=4817, ""Audit Logs Modified"",EventCode=4885, ""Audit Logs Modified"",EventCode=4902, ""Audit Logs Modified"",EventCode=4906, ""Audit Logs Modified"",EventCode=4907, ""Audit Logs Modified"",EventCode=4912, ""Audit Logs Modified"", EventCode=642, ""Account Modification"",EventCode=646, ""Account Modification"",EventCode=685, ""Account Modification"",EventCode=4738, ""Account Modification"",EventCode=4742, ""Account Modification"",EventCode=4781, ""Account Modification"", EventCode=1102, ""Audit Logs Cleared/Deleted"",EventCode=517, ""Audit Logs Cleared/Deleted"", EventCode=628, ""Passwords Changed"",EventCode=627, ""Passwords Changed"",EventCode=4723, ""Passwords Changed"",EventCode=4724, ""Passwords Changed"", EventCode=528, ""Successful Logons"",EventCode=540, ""Successful Logons"",EventCode=4624, ""Successful Logons"", EventCode=4625, ""Failed Logons"",EventCode=529, ""Failed Logons"",EventCode=530, ""Failed Logons"",EventCode=531, ""Failed Logons"",EventCode=532, ""Failed Logons"",EventCode=533, ""Failed Logons"",EventCode=534, ""Failed Logons"",EventCode=535, ""Failed Logons"",EventCode=536, ""Failed Logons"",EventCode=537, ""Failed Logons"",EventCode=539, ""Failed Logons"", EventCode=576, ""Escalation of Privileges"",EventCode=4672, ""Escalation of Privileges"",EventCode=577, ""Escalation of Privileges"",EventCode=4673, ""Escalation of Privileges"",EventCode=578, ""Escalation of Privileges"",EventCode=4674, ""Escalation of Privileges"")
| stats count by Trigger
| sort - count",0
"SPL-206","Search Common EventCodes (EventIDâ€™s) for Suspicious Behavior","suspicious,windows",Security,,"index=* source=WinEventLog:security User!=SYSTEM User!=""LOCAL SERVICE"" User!=""NETWORK SERVICE"" | eval Trigger=case(EventCode=516, ""Audit Logs Modified"",EventCode=517, ""Audit Logs Modified"",EventCode=612, ""Audit Logs Modified"",EventCode=623, ""Audit Logs Modified"",EventCode=806, ""Audit Logs Modified"",EventCode=807, ""Audit Logs Modified"",EventCode=1101, ""Audit Logs Modified"",EventCode=1102, ""Audit Logs Modified"",EventCode=4612, ""Audit Logs Modified"",EventCode=4621, ""Audit Logs Modified"",EventCode=4694, ""Audit Logs Modified"",EventCode=4695, ""Audit Logs Modified"",EventCode=4715, ""Audit Logs Modified"",EventCode=4719, ""Audit Logs Modified"",EventCode=4817, ""Audit Logs Modified"",EventCode=4885, ""Audit Logs Modified"",EventCode=4902, ""Audit Logs Modified"",EventCode=4906, ""Audit Logs Modified"",EventCode=4907, ""Audit Logs Modified"",EventCode=4912, ""Audit Logs Modified"", EventCode=642, ""Account Modification"",EventCode=646, ""Account Modification"",EventCode=685, ""Account Modification"",EventCode=4738, ""Account Modification"",EventCode=4742, ""Account Modification"",EventCode=4781, ""Account Modification"", EventCode=1102, ""Audit Logs Cleared/Deleted"",EventCode=517, ""Audit Logs Cleared/Deleted"", EventCode=628, ""Passwords Changed"",EventCode=627, ""Passwords Changed"",EventCode=4723, ""Passwords Changed"",EventCode=4724, ""Passwords Changed"", EventCode=528, ""Successful Logons"",EventCode=540, ""Successful Logons"",EventCode=4624, ""Successful Logons"", EventCode=4625, ""Failed Logons"",EventCode=529, ""Failed Logons"",EventCode=530, ""Failed Logons"",EventCode=531, ""Failed Logons"",EventCode=532, ""Failed Logons"",EventCode=533, ""Failed Logons"",EventCode=534, ""Failed Logons"",EventCode=535, ""Failed Logons"",EventCode=536, ""Failed Logons"",EventCode=537, ""Failed Logons"",EventCode=539, ""Failed Logons"", EventCode=576, ""Escalation of Privileges"",EventCode=4672, ""Escalation of Privileges"",EventCode=577, ""Escalation of Privileges"",EventCode=4673, ""Escalation of Privileges"",EventCode=578, ""Escalation of Privileges"",EventCode=4674, ""Escalation of Privileges"") | stats count by Trigger | sort - count",0
"SPL-207","Count of Attackers on Juniper Devices","attack,juniper",Security,"The following is a Splunk search query that indicates potential attacks by source IP","index=* sourcetype = ""juniper:idp"" attack*
| stats count by src_ip",0
"SPL-208","Combining Fields","field,join",SPL,"Add the field: ""comboIP"". Values of ""comboIP"" = """"sourceIP"" + ""/"" + ""destIP""""","index=* sourcetype = *access*
| strcat sourceIP ""/"" destIP comboIP",0
"SPL-209","Top Statuses Sparklines","sparkline,status",Web,,"index=* sourcetype=""*access*""
|  stats count(status) as count sparkline(count) by status_description
|  rename status_description as ""http status code""
|  sort - count",0
"SPL-210","Apache access_logs status code reporting",status,Other,"","index=* sourcetype=""*access*""
| chart count(eval(like(status,""2%""))) AS Success, count(eval(like(status,""4%"") OR like(status,""5%""))) AS Error by status",0
"SPL-211","Reports on most used devices / platforms","devices,usage",Web,,"index=* sourcetype=""*access*""
| dedup useragent
| eval device=useragent
| replace *Windows* with Windows, *Macintosh* with Apple, *Android* with Android, *iPhone* with iPhone, *iPad* with iPad in device
| top limit=5 useother=t device",0
"SPL-212","Average Queue Size","queue,web",Queues,"","index=* sourcetype=""*access*""
| eval bytes = bytes / 170
| timechart avg(bytes)",0
"SPL-213","Show Avg Transaction duration",transaction,Miscellaneous,,"index=* sourcetype=""*access*""
| eval bytes = bytes / 20
| eval sla=105
| timechart avg(sla) as SLA avg(bytes) as ""Avg Tx Duration"" span=1m",0
"SPL-214","User Demographics by Region: Geo Map","geo,map,ui,web",Web,,"index=* sourcetype=""*access*""
| iplocation clientip
| geostats count by Country",0
"SPL-215","User Demographics by Region: Chloropleth","chloropleth,geo,ui",Web,,"index=* sourcetype=""*access*""
| iplocation clientip
| stats count by Country
| geom geo_countries featureIdField=""Country""",0
"SPL-217","Find web site status over time",status,Web,,"index=* sourcetype=""*access*""
| timechart count by status_type limit=10 usenull=f",0
"SPL-218","Prediction of traffic based on the Kalman Algorithm",predict,Web,,"index=* sourcetype=""*access*""
| timechart count(bytes) as traffic
| streamstats sum(traffic) as total_traffic
| predict total_traffic as expected_traffic algorithm=LLT future_timespan=14",0
"SPL-219","Prediction of traffic based on the Kalman Algorithm w/explicit fields assigned to display",predict,Web,,"index=* sourcetype=""*access*""
| timechart count(bytes) as traffic
| streamstats sum(traffic) as total_traffic
| predict total_traffic as expected_traffic algorithm=LLT future_timespan=14
| fields total_traffic, expected_traffic",0
"SPL-220","Users Percentage of Total Processing Time (Know User Name - $series$)",transaction,Web,,"index=* sourcetype=""*access*""
| transaction username clientip maxspan=20m maxpause=5m
| eval durationCurrentUser = if (username = ""$series$"", duration, null)
| eval eventCountCurrentUser = if (username = ""$series$"", eventcount, null)
| eval durationOtherUsers = if (username != ""$series$"", duration, null)
| eval eventCountOtherUsers = if (username != ""$series$"", eventcount, null)
| stats avg(eventCountCurrentUser), avg(eventCountOtherUsers), avg(durationCurrentUser), avg(durationOtherUsers)",0
"SPL-221","List IPs that had Successful and Failed SSH Attempts","linux,login",Security,,"index=* sourcetype=""*secure"" process=sshd ""password for""
| rex field=_raw ""(?Accepted|Failed) password for (?\w+) from (?[0-9A-Fa-f:\.]+)""
| eval success=if(result==""Failed"",0,1)
| stats count as total,sum(success) as success by ipaddr
| where total!=success AND success!=0",0
"SPL-222","XML with SPATH Searches for events which contain a field called message",,Other,"XML with SPATH
Searches for events which contain a field called ""message"" that composite field is expanded via a call to spath. Then a value from the resulting expansion is used to find events that contain a date meeting certain criteria.","index=* sourcetype=""SOURCETYPE""
| spath input=FIELD
| where strptime('FIELD.updated_at', ""%Y-%m-%d %H:%M:%S %z"") > strptime(""2013-08-07 00:00:00"", ""%Y-%m-%d %H:%M:%S"")",0
"SPL-223","Escalation of Privs in Windows by User:Â ","windows,wineventlog",Security,,"index=* sourcetype=""WinEventLog:Security"" (EventCode=576 OR EventCode=4672 OR EventCode=577 OR EventCode=4673 OR EventCode=578 OR EventCode=4674)
| stats count by user #(user) This/these field/s must exist or edit to make search valid.",0
"SPL-224","Password Changes in Windows environment by user account","windows,wineventlog",Security,,"index=* sourcetype=""WinEventLog:Security"" (EventCode=628 OR EventCode=627 OR EventCode=4723 OR EventCode=4724)
| chart count by user #(user) This/these field/s must exist or edit to make search valid.",0
"SPL-225","Windows failed logons with Average Overlay to identify deviations and anomalies",windows,Security,,"index=* sourcetype=""WinEventLog:Security"" (Logon_Type=2 OR Logon_Type=7 OR Logon_Type=10) (EventCode=4625 OR EventCode=529 OR EventCode=530 OR EventCode=531 OR EventCode=532 OR EventCode=533 OR EventCode=534 OR EventCode=535 OR EventCode=536 OR EventCode=537 OR EventCode=539)
| timechart count(EventCode) as count
| eventstats avg(count) as Average
| eval average=round(average,0)
| rename count as ""Failed Logons""",0
"SPL-226","Windows Failed Logons with Average Overlay","overlay,windows",Security,"This Splunk search will show any failed login attempt and graphically overlay an average value.","index=* sourcetype=""WinEventLog:Security"" (Logon_Type=2 OR Logon_Type=7 OR Logon_Type=10) (EventCode=4625 OR EventCode=529 OR EventCode=530 OR EventCode=531 OR EventCode=532 OR EventCode=533 OR EventCode=534 OR EventCode=535 OR EventCode=536 OR EventCode=537 OR EventCode=539)
| timechart count(EventCode) as count
| eventstats avg(count) as Average | eval average=round(average,0)
| rename count as ""Failed Logons""",0
"SPL-227","Successful Windows Logins with Average Overlay to see deviations and anomalies",windows,Security,,"index=* sourcetype=""WinEventLog:Security"" (Logon_Type=2 OR Logon_Type=7 OR Logon_Type=10) (EventCode=528 OR EventCode=540 OR EventCode=4624)
| timechart count(EventCode) as count
| eventstats avg(count) as Average
| eval average=round(average,0)
| rename count as ""Successful Logons""",0
"SPL-228","Windows 2008 and newer","access,file,windows",Security,,"index=* sourcetype=""WinEventLog:Security"" EventCode=4656 Object_Type=File (Security_ID!=""NT AUTHORITY*"") (Security_ID!=""S-*"")
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, Account_Name, Process_Name, Keywords, host",0
"SPL-229","Security Access Granted to an account. 2007/2008 or better","windows,wineventlog",Security,,"index=* sourcetype=""WinEventLog:Security"" EventCode=4717 #(WinEventLog:Security) This/these sourcetype/s must  exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by src_user, user, Access_Right, Date, Keywords #(src_user, user, Access_Right, Date, Keywords) This/these field/s must exist or edit to make search valid.
| rename src_user as ""Source Account""
| rename user as ""Target Account""
| rename Access_Right as ""New Rights Granted""",0
"SPL-230","Windows File Access Attempts",access,Other,"Windows File Access Attempts
The following splunk queries will display any file access attempts (successful or failed) by user account. Ensure the Splunk App for Windows is installed grab it here: https://apps.splunk.com/app/742/ Windows 2003 and older:","index=* sourcetype=""WinEventLog:Security"" EventCode=560 Object_Type=File
| eval Date=strftime(_time, ""%Y/%m/%d"")
| eval UserName=coalesce(Primary_User_Name, Client_User_Name)
| search UserName!=""*$"" AND UserName!=""NETWORK SERVICE""
| stats count by Date, Image_File_Name, UserName, Type, host
| sort - Date",0
"SPL-231","Security Access Granted to an account. 2003 or older","windows,wineventlog",Security,,"index=* sourcetype=""WinEventLog:Security"" EventCode=621 Account_Modified!=""%{*""
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by User_Name, Account_Modified, Access_Granted, Date, action |rename User_Name as ""Source Account""
| rename Account_Modified as ""Target Account""
| rename Access_Granted as ""New Rights Granted""",0
"SPL-232","Failed Attempt to Login to a Disabled Account","disabled,security,windows",Security,,"index=* sourcetype=""WinEventLog:security"" EventCode=4625 (Sub_Status=""0xc0000072"" OR Sub_Status=""0xC0000072"") Security_ID!=""NULL SID"" Account_Name!=""*$""
| eval Date=strftime(_time, ""%Y/%m/%d"")
| rex ""Which\sLogon\sFailed:\s+\S+\s\S+\s+\S+\s+Account\sName:\s+(?<facct>\S+)""
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, facct, host, Keywords
| rename facct as ""Target Account"" host as Host Keywords as Status count as Count",0
"SPL-233","Real Time IIS Web Site Connections",iis,Web,"Assuming JSESSIONID is auto-extracted, run the following REAL-TIME search (choose your window, I typically go with 5 minutes)","index=* sourcetype=""iis""
| stats dc(JSESSIONID)",0
"SPL-234","Return only anomalous event",anomalies,Miscellaneous,"Replace the wildcard * and or field names to be applicable to your data set(s.)","index=* sourcetype=*
| anomalies",0
"SPL-235","More than a day between events",duration,SPL,"Add a field to each event which is the time between this event and the previous one.
The only field required here is _time.","index=* sourcetype=*
| sort _time
| streamstats current=f global=f window=1 last(_time) as last_ts
| eval time_since_last = _time - last_ts
| fieldformat time_since_last = tostring(time_since_last, ""duration"")
| where time_since_last > 60*60*24",0
"SPL-236","Predict Command Example 1",predict,Commands,"Replace the wildcard * and or field names to be applicable to your data set(s.)","index=* sourcetype=*
| timechart span=""1m"" count AS foo
| predict foo as fubar algorithm=LL upper90=high lower97=low future_timespan=10 holdback=20",0
"SPL-237","Predict Command Example 2",predict,Commands,"Replace the wildcard * and or field names to be applicable to your data set(s.)","index=* sourcetype=*
| timechart span=1d count(file) as count
| predict count",0
"SPL-239","Avg Value for 24 hours over 30 days compared with todays value- optimized searches",compare,SPL,"","index=* sourcetype=* error earliest=-30d latest=@h
| timechart span=1h count
| eval StartTime=relative_time(now(),""-24h@h"")
| eval Series=if(_time>=StartTime,""TodaysCount"",""AverageCount"")
| eval Hour = strftime(_time,""%H"")
| chart avg(count) by Hour Series",0
"SPL-240","Events processed by Web Servers / load balance?",trouble,Web,,"index=* sourcetype=*access*
earliest=-7d@d  latest=now
| timechart count by host",0
"SPL-241","Gets vs Posts","gets,posts",Web,,"index=* sourcetype=*access*
| chart count(eval(method=""GET"")) AS GET,count(eval(method=""POST"")) AS POST",0
"SPL-242","Search Web / Total KB / # of Unique IPs",stats,Web,"Replace the wildcard * and or field names to be applicable to your data set(s.)","index=* sourcetype=*access*
| eval kb=bytes/1024
|  stats sum(kb) dc(clientip)
|  rename sum(kb) AS ""Total KB"" dc(FIELD) AS ""Unique Customers""",0
"SPL-244","Return the least common values of the url field","rare,url",Web,"Return the least common values of the ""url"" field.","index=* sourcetype=*access*
| rare url",0
"SPL-245","Return values of URL that contain the string 404 or 303 but not both","diff,errors",SPL,"Return values of ""URL"" that contain the string ""404"" or ""303"" but not both.","index=* sourcetype=*access*
| set diff [search 404 | fields url] [search 303 | fields url]",0
"SPL-246","A transaction search using the stats command instead","duration,stats,web",SPL,"Its less resources and more flexibility","index=* sourcetype=*access*
| stats min(_time) AS earliest max(_time) AS latest by FIELD
| eval duration=latest-earliest | stats min(duration) max(duration) avg(duration)",0
"SPL-247","Top Users",sparkline,Web,,"index=* sourcetype=*access*
| stats sparkline count by username
| sort -count
| head 10",0
"SPL-251","Return the 20 most common values of the url field",url,Web,"Return the 20 most common values of the ""url"" field.","index=* sourcetype=*access*
| top limit=20 url",0
"SPL-252","Session Hijacking report/alert","iis,transaction",Security,,"index=* sourcetype=*access*
| transaction JSESSIONID  #(JSESSIONID) This/these field/s must exist or edit to make search valid.
| where mvcount(clientip)>1  #(clientip) This/these field/s must exist or edit to make search valid.
| table clientip, JSESSIONID",0
"SPL-254","Hits by host for last 7 days","hits,host",Web,,"index=* sourcetype=*access* earliest = -7d@d latest = now
| timechart count by host",0
"SPL-255","Hits by Host without Internal access for last 7 days","access,host",Web,,"index=* sourcetype=*access* earliest = -7d@d latest = now clientip != 192*
| timechart count by host",0
"SPL-256","Traffic with Bad HTTP status for last 7 days",status,Web,,"index=* sourcetype=*access* earliest = -7d@d latest = now status>300
| timechart count BY status",0
"SPL-257","Top pages by bad HTTP status for last 7 days","bad,status",Web,,"index=* sourcetype=*access* earliest = -7d@d latest = now status>=300
| stats dc(clientip) as ""unique ips"" count as ""total count"" by uri, status",0
"SPL-258","Unique Visitors",unique,Web,,"index=* sourcetype=*access* earliest=-10d@d latest=now
| timechart dc(clientip) AS unique_visitors by host",0
"SPL-259","Who has accessed in the last 7 days",host,Web,,"index=* sourcetype=*access* earliest=-7d@d latest=now
| timechart count by host",0
"SPL-260","Traffic with Good HTTP status for last 7 days",status,Web,,"index=* sourcetype=*access* earliest=-7d@d latest=now status>100 status<300
| timechart count BY status",0
"SPL-261","Account Enabled in Windows from a previously disabled state",windows,Other,"Windows Server 2008 and newer","index=* sourcetype=WinEventLog:Security (EventCode=4722) #(WinEventLog:Security) This/these sourcetype/s must  exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"") |rex ""ID:\s+\w+\\\(?<sourceaccount>\S+)\s+""
| rex ""Account:\s+Security\sID:\s+\w+\\\(?<targetaccount>\S+)\s+""
| stats count by Date, sourceaccount, targetaccount, Keywords, host
| rename sourceaccount as ""Source Account""
| rename targetaccount as ""Target Account""
| sort - Date",0
"SPL-262","Accounts deleted within 24 hours of creation Windows 2008",wineventlog,Security,"","index=* sourcetype=WinEventLog:Security (EventCode=4726 OR EventCode=4720)
|eval Date=strftime(_time, ""%Y/%m/%d"")
|rex ""Subject:\s+\w+\s\S+\s+\S+\s+\w+\s\w+:\s+(?<SourceAccount>\S+)""
| rex ""Target\s\w+:\s+\w+\s\w+:\s+\S+\s+\w+\s\w+:\s+(?<DeletedAccount>\S+)""
| rex ""New\s\w+:\s+\w+\s\w+:\s+\S+\s+\w+\s\w+:\s+(?<NewAccount>\S+)""
| eval SuspectAccount=coalesce(DeletedAccount,NewAccount)
| transaction SuspectAccount startswith=""EventCode=4720"" endswith=""EventCode=4726"" |eval duration=round(((duration/60)/60)/24, 2)
| eval Age=case(duration<=1, ""Critical"", duration>1 AND duration<=7, ""Warning"", duration>7, ""Normal"")
| table Date, index, host, SourceAccount, SuspectAccount, duration, Age
| rename duration as ""Days Account was Active"" | sort + ""Days Account was Active""",0
"SPL-263","Time between rights granted and rights revoked: 2003 and older","windows,wineventlog",Security,,"index=* sourcetype=WinEventLog:Security (EventCode=608 OR EventCode=609)
| rex ""Message=User\sRight\sAssigned:\s+User\sRight:\s+(?\w+)""
| rex ""Message=User\sRight\sRemoved:\s+User\sRight:\s+(?\w+)""
| eval Rights=coalesce(RightGranted,RightRemoved)
| eval status=case(EventCode=608, ""New Rights Granted by:"", EventCode=609, ""Rights Removed by:"")
| transaction Rights user startswith=""Assigned"" endswith=""Removed""
| where duration > 0
| eval duration = duration/60 |eval n=round(duration,2)
| eval Date=strftime(_time, ""%Y/%m/%d"")
| table Date, host, status, User, user, Rights, n
| rename User as ""Source Account""
| rename user as ""Target Account""
| rename n as ""Minutes between Rights Granted Then Removed""
| sort - date",0
"SPL-264","Account Enabled in Windows from a previously disabled state","windows,wineventlog",Security,"","index=* sourcetype=WinEventLog:Security (EventCode=626) #(WinEventLog:Security) This/these sourcetype/s must  exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by Date, Caller_User_Name, Target_Account_Name, Type, host #(Date, Caller_User_Name, Target_Account_Name, Type, host) This/these field/s must exist or edit to make search valid.
| sort - Date",0
"SPL-265","Accounts deleted within 24 hours of creation Windows 2003","windows,wineventlog",Security,"","index=* sourcetype=WinEventLog:Security (EventCode=630 OR EventCode=624) |eval Date=strftime(_time, ""%Y/%m/%d"") | transaction Target_Account_Name startswith=""EventCode=624"" endswith=""EventCode=630"" |eval duration=round(((duration/60)/60)/24, 2) | eval Age=case(duration<=1, ""Critical"", duration>1 AND duration<=7, ""Warning"", duration>7, ""Normal"")| table Date, index, host, Caller_User_Name, Target_Account_Name, duration, Age | rename duration as ""Days Account was Active"" | sort - Date",0
"SPL-266","Accounts Deleted via EventIDâ€™s that Correspond with Post XP/2003 Operating Systems",windows,Security,"This query will search for accounts deleted via EventIDâ€™s that correspond with post XP/2003 operating systems. It will output the admin account, account deleted, details about the action, and the machine that the account deletion took place on.","index=* sourcetype=WinEventLog:Security (EventCode=630)
| eval Date=strftime(_time, ""%Y/%m/%d"")
| stats count by User, Target_Account_Name, name, host, index Date
| rename User as ""Administrator Account""
| rename Target_Account_Name as ""Account Name Deleted""
| rename name as ""Detailed Information""
| rename host as ""Computer the Account was Created on""
| rename index as ""Index of origin""| sort - Date",0
"SPL-267","Unusual outbound activity using a high number of DNS requests",dns,Security,"Occurring from a particular client compared to a baseline. Itâ€™s possible advanced threat communication (instruction, stealing data) via the DNS protocol is being used.Â ","index=* sourcetype=dns       #(dns) Make sure that this/these sourcetype/s exist or edit to make search valid.
| stats count(clientip) AS Requests by clientip #(clientip) This/these field/s must exist or edit to make search valid.
| sort - Requests",0
"SPL-268","Unusual outbound activity using a High number of same-sized DNS requests",dns,Security,"From an internal host, patterns of same-sized DNS requests which can evidence advanced threat communication (instruction/data theft) using DNS.","index=* sourcetype=dns #(dns) Make sure that this/these sourcetype/s exist or edit to make search valid.
| eval Length=len(query)
| stats count(clientip) by Length #(clientip) This/these field/s must exist or edit to make search valid.
| sort â€“ Length",0
"SPL-269","Total Unique Browsers detected in IIS logs",iis,Web,"The following Splunk search query will show a count of unique browsers (calculation to include version) that hit a given website within IIS logs:","index=* sourcetype=iis
| stats dc(cs_User_Agent)",0
"SPL-270","Repeated Unsuccessful Logon Attempts in Linux","fail,linux_secure,login",Security,,"index=* sourcetype=linux_secure
| eval Date=strftime(_time, ""%Y/%m/%d"")
| rex "".*:\d{2}\s(?<hostname>\S+)""
| rex ""gdm\S+\sauthentication\s(?<status>\w+)""
| rex ""\suser[^'](?<User>\S+\w+)""
| search status=failure
| stats count as fails by Date, User, hostname
| eval ""Alert Level""=case(fails>=50, ""Critical"", fails<50 AND fails>=20, ""Warning"", fails<20, ""Normal"")
| sort - fails
| rename fails as ""Failed Logon Attempts""
| rename User as ""Account in Question""",0
"SPL-272","Number of Hosts the Root Account was Detected on","hosts,root",SPL,"The following splunk query example will return the total number of hosts the Root account was detected on  in a given time range *NOTE* if the host field is being auto-extracted (for instance if you are using a universal forwarder) you will not need the regex command and can call upon the auto extracted fieldname [â€¦]","index=* sourcetype=linux_secure
| rex "".*:\d{2}\s(?<hostname>\S+)""
| rex ""\suser[^'](?<User>\S+\w+)""
| search User=""root"" | stats dc(hostname)",0
"SPL-273","List of hosts in a Linux Environment","linux_secure",Security,"If you are using a universal forwarder, this field will be autoextracted as host.Â ","index=* sourcetype=linux_secure
| rex "".*:\d{2}\s(?<hostname>\S+)""
| stats count by hostname",0
"SPL-274","Top 10 most active hosts in a Linux Environment","linux_secure,rex",Security,"if using a universal forwarder, rex isn't needed, this will be extracted as host","index=* sourcetype=linux_secure
| rex "".*:\d{2}\s(?<hostname>\S+)""
| top limit=10 hostname",0
"SPL-275","Count of Unique users in a linux environmentÂ ","linux_secure",Security,,"index=* sourcetype=linux_secure
| rex ""\suser[^'](?<User>\S+\w+)""
| stats dc(User)",0
"SPL-276","Top 10 most active users in Linux","linux_secure",Security,,"index=* sourcetype=linux_secure
| rex ""\suser[^'](?<User>\S+\w+)""
| top limit=10 User",0
"SPL-277","Linux Logons: Failed","fail,linux,login",Security,,"index=* sourcetype=linux_secure
| rex ""\w{3}\s\d{1,2}\s\d{2}:\d{2}:\d{2}\s\S+\s(?<session>gdm-\w+)\S:\s""
| search session=gdm-password
| rex ""\w{3}\s\d{1,2}\s\d{2}:\d{2}:\d{2}\s(?<hostname>\S+)\s.+\Sgdm-password:auth\S:\s(?<authstatus>\w+\s\w+);\s.+user=(?<username>\S+)""
| search authstatus=""authentication failure""
| timechart count(username)",0
"SPL-278","Linux Logons: Successful","linux_secure",Security,,"index=* sourcetype=linux_secure
| rex ""\w{3}\s\d{1,2}\s\d{2}:\d{2}:\d{2}\s\S+\s(?<session>gdm-\w+)\S:\s""
| search session=gdm-password
| rex ""\w{3}\s\d{1,2}\s\d{2}:\d{2}:\d{2}\s(?<hostname>\S+)\s.+\Sgdm-password:auth\S:\s(?<authstatus>\w+\s\w+);\s.+user=(?<username>\S+)""
| search authstatus=""authentication success""
| timechart count(username)",0
"SPL-279","List of user accounts in a linux environment","linux_secure",Security,,"index=* sourcetype=linux_secure #(linux_secure) This/these sourcetype/s exist or edit to make search valid.
| rex ""\suser[^'](?<User>\S+\w+)""
| stats count by User",0
"SPL-280","Escaltion of Privs via SU in Linux:Â ","linux_secure",Security,,"index=* sourcetype=linux_secure source=""/var/log/*"" ""su: ("" #(linux_secure) This/these sourcetype/s exist or edit to make search valid.
| eval Date=strftime(_time, ""%Y/%m/%d"")
| rex "".*:\d{2}\s(?<hostname>\S+)""
| rex ""su:\s\Sto\sroot\S\s(?<SU>\S+)""
| stats â€¯count by Date, hostname, SU
| rename â€¯count as ""Occurrences""
| rename â€¯SU as ""Account with Escalated Privileges"" | sort - Date",0
"SPL-281","Real Time Users","realtime,tag,users,web",Web,,"index=* tag=web
| timechart dc(clientip) AS unique_visitors by host",0
"SPL-282","Does a distinct on data by its shape (punct). punct is the skeleton of the event",punct,Miscellaneous,,"index=INDEX sourcetype=SOURCETYPE
| dedup punct",0
"SPL-283","Rare Punctuation by Host","punct,rare",SPL,,"index=INDEX sourcetype=SOURCETYPE
| err* OR fail* OR crit* OR fatal* OR except* | rex field=punct ""(?<smallpunct>.{5})""
| eval smallpunct= ""*"" + smallpunct
| stats first(_raw) as example count by smallpunct
| sort -count | fields smallpunct,count,example",0
"SPL-284","Request Duration Deviation Critical (processing_time)",duration,Web,,"index=INDEX sourcetype=SOURCETYPE
| eval processingTime = tonumber(trim(processing_time, ""ms""))
| eval formattedTime = strftime(_time, ""%Y%m%d"")
| eval processingTimeBase = if (formattedTime = ""20121012"", processingTime, null)
| eval processingTimeNow = if (formattedTime = ""20121013"", processingTime, null)
| stats avg(processingTimeBase) as avgProcessingTimeBase, avg(processingTimeNow) as avgProcessingTimeNow by request_type
| eval diff = avgProcessingTimeNow - avgProcessingTimeBase
| eval pctChange = round(diff / avgProcessingTimeBase, 3) * 100
| where pctChange > 10000",0
"SPL-285","Find the count of a number of times something happened for ACTORX VS ALLACTORS","count,percentage",Security,"Shows the ACTORCOUNT and its percentage to the whole.","index=INDEX sourcetype=SOURCETYPE
| eventstats count as ""total_count""
| eventstats count as ""variable_count"" by YOURVARIABLE
| eval percent=(variable_count/total_count)*100
| stats values(variable_count), values(percent) as percent by YOURVARIABLE",0
"SPL-286","Deriving Total Bytes",convert,Miscellaneous,,"index=INDEX sourcetype=SOURCETYPE
| stats sum(total_bytes) as TotalBytes
| eval MB=(TotalBytes)/1048576",0
"SPL-287","Compute the product of the average CPU and average MEM each minute for each host","cpu,mem,os,timechart","Resource_Usage","","index=INDEX sourcetype=SOURCETYPE
| timechart span=1m
| eval(avg(CPU) * avg(MEM)) by host",0
"SPL-288","Timewrap Command Example",timewrap,Commands,"Use this after a 'timechart' command, and compare week-over-week. Or use 'h' (hour), 'w' (week), 'm' (month), 'q' (quarter), 'y' (year).  Replace the wildcard * and or field names to be applicable to your data set(s.)","index=INDEX sourcetype=SOURCETYPE
| timewrap 1w",0
"SPL-289","Punct example:2","error,punct,pwn",Miscellaneous,,"index=INDEX sourcetype=SOURCETYPE  err* OR fail* OR crit* OR fatal* OR except*
| rex field=punct ""(?<smallpunct>.{5})""
| eval smallpunct= ""*"" + smallpunct
| stats first(_raw) as example count by smallpunct
| sort -count
| fields smallpunct,count,example",0
"SPL-290B","What you Don't Know Part2","errors,known,output",SPL,"Outputting of known events to a .csv file
Search #2  Scheduled to compare recent errors with table of known errors: I tagged all errors that I found as known and then the search became:
err* OR fail* OR crit* OR down NOT tag=known","index=INDEX sourcetype=SOURCETYPE FIELD=*
| dedup FIELD
| search NOT [inputlookup known_errors.csv
| fields +
| dedup FIELD]",0
"SPL-290A","What you Don't Know Part 1","errors,known,output",SPL,"(Outputting of known events to a .csv file)
Search #1  Scheduled to populate ""known errors"" table.","index=INDEX sourcetype=SOURCETYPE FIELD=*
| fields + FIELD
| dedup FIELD
| outputlookup known_errors.csv",0
"SPL-292","Punct example:1","error,punct",Miscellaneous,,"index=INDEX sourcetype=SOURCETYPE err* OR fail* OR crit* OR down OR fatal OR except*
| rare punct",0
"SPL-293","Unusual/short file names used by hackers to avoid detection","endpoint,system",Security,,"index=YOURINDEX
| eval file_length=len(file)       #(file) This/these field/s must exist or edit to make search valid.
| where file_length < 4",0
"SPL-294","Process List via Tasklist in Microsoft Windows or Linux ps command",process,Security,,"index=YOURINDEX
| stats dc(host) by process
| sort dc(host)",0
"SPL-295","Splunk Server Restart Duration","duration,pwn,splunkd",Internal,,"index=_audit (action=""splunkShuttingDown"" OR action=""splunkStarting"")
| eval Date=strftime(_time, ""%Y/%m/%d"") | transaction splunk_server startswith=action=""splunkShuttingDown"" endswith=action=""splunkStarting""
| eval duration=round(duration/60, 2)
| table Date splunk_server duration
| rename duration as ""Splunk Restart Duration"" splunk_server as ""Splunk Server""",0
"SPL-296","User training for overall search best practices","audit,search",Internal,"e.g. specifying indexes/sourcetypes, when wildcards are appropriate, and using proper time ranges).","index=_audit [|rest /services/server/info splunk_server=local
| fields host]  action=search user=* user!=splunk-system-user search_id=* (info=granted OR info=completed)
|rex field=apiStartTime ""'(?<start_time>[^']+)'""
|rex field=apiEndTime ""'(?<end_time>[^']+)'""
|rex ""search\=(?<search_string>.+)\,\sautojoin""
|eval range=if(start_time==""ZERO_TIME"",""All Time"", tostring(strptime(end_time, ""%a %b %d %H:%M:%S %Y"") - strptime(start_time, ""%a %b %d %H:%M:%S %Y""),""duration""))
|eval range2=if(start_time==""ZERO_TIME"",""All Time"", strptime(end_time, ""%a %b %d %H:%M:%S %Y"") - strptime(start_time, ""%a %b %d %H:%M:%S %Y""))
|stats max(event_count) AS event_count values(start_time) AS ""Search Earliest"" values(end_time) AS ""Search Latest"" count values(range) AS range values(range2) AS range2 values(search) AS Search values(user) AS User max(total_run_time) AS run_time(sec) by search_id savedsearch_name
| convert ctime(*Time) ctime(""Search Start"") ctime(""Search End"")
|where count>1 |rename search_id AS SID range AS ""Search Range""
| eval ""Run Time""=tostring('run_time(sec)',""duration"")
| eval ""Overran?""=if('run_time(sec)'>range2,""Overran its range!!!"", ""NO"")
| eval SID = trim(SID, ""'"")
| table SID savedsearch_name event_count ""Search Earliest"" ""Search Latest"" ""Search Range"" Search User ""run_time(sec)"" ""Overran?"" ""Run Time""
| sort - ""Run Time""",0
"SPL-297","Shows users by search time","audit,user",Internal,,"index=_audit action=search (id=* OR search_id=*)
| eval search_id = if(isnull(search_id), id, search_id)
| replace '*' with * in search_id
| rex ""search='(?<search>.*?)', autojoin""
| search search_id!=scheduler_*
| convert num(total_run_time)
| rex ""\,\s+user=(?<user>\S+),""
| eval user = if(user=""n/a"", null(), user)
| stats min(_time) as _time first(user) as user max(total_run_time) as total_run_time first(search) as search by search_id host
| search search=search* search!=*_internal* search!=*_audit*
| stats median(total_run_time) as ""Median search time"" perc95(total_run_time) as ""95th Percentile search time"" sum(total_run_time) as ""Total search time"" count as ""Search count"" max(_time) as ""Last use"" by user host
| fieldformat ""Last use"" = strftime('Last use', ""%F %T.%Q %:z"")
| rename host AS ""Search Head""
| sort - ""Median search time""",0
"SPL-298","Splunk Query Count by users","count,pwn,users",Internal,,"index=_audit search=* NOT (search_id='scheduler* OR search_id='Summary*) user=admin
| timechart span=1d count by user usenull=f",0
"SPL-299","Internal Splunk User Modifications",audit,Security,"This query will search the internal audit sourcetype of splunk and report on any user modification attempts, both success and fail.","index=_audit sourcetype=audittrail action=edit_user
| eval Date=strftime(_time, ""%Y/%m/%d"")
| where user!=object
| stats count by user, info, object, Date
| rename user as ""Authenticated User""
| rename info as ""Success Status""
| rename object as ""Modified Account""
| sort - count",0
"SPL-300","Splunk User Search Activity","pwn,user",Internal,,"index=_audit splunk_server=local action=search (id=* OR search_id=*)
| eval search_id = if(isnull(search_id), id, search_id)
| replace '*' with * in search_id
| rex ""search='search\s(?<search>.*?)',\sautojoin""| search search_id!=scheduler_*
| convert num(total_run_time) | eval user = if(user=""n/a"", null(), user)
| stats min(_time) as _time first(user) as user max(total_run_time) as total_run_time first(search) as search by search_id
| search search!=*_internal*  search!=*_audit*
| chart sum(total_run_time) as ""Total search time"" count as ""Search count"" max(_time) as ""Last use"" by user
| fieldformat ""Last use"" = strftime('Last use', ""%F %T.%Q"")",0
"SPL-301","Shows what user did what over time and what was the outcome","audit,user",Internal,,"index=_audit | table _time user action info",0
"SPL-302","Splunk Errors","errors,splunkd",Internal,,"index=_internal  error  NOT debug source=*splunkd.log*",0
"SPL-303","Top Five Sourcetypes",,Sourcetypes,,"index=_internal (source=*/metrics.log* OR source=*\\metrics.log*) group=per_sourcetype_thruput
| chart sum(kb) by series
| sort -sum(kb)
| head 5",0
"SPL-304","Calculate Event Data Sizes","event,pwn",Internal,,"index=_internal earliest=""-d@d"" latest=""@d""
| fieldsummary
| rex field=values max_match=100 ""value\"":\""(?<foo>[^\""]*)\"",""
| mvexpand foo
| eval foo_len=len(foo)
| rex field=field ""^(?!date|punct|host|hostip|index|linecount|source|sourcetype|timeendpos|timestartpos|splunk_server)(?<Field>.*)""
| stats max(foo_len) as Bytes values(foo) as Values by Field,count
| eval SpaceMB=(Bytes*count)/1024/1023
| rename count as Count
| table Field Bytes Count SpaceMB
| sort -SpaceMB",0
"SPL-305","Comparing Stats Time Over Time","bin,compare,stats",Miscellaneous,,"index=_internal earliest=-48h latest=-24h
| bin _time span=10m
| stats count by _time
| eval window=""yesterday""
| append
[search index=_internal earliest=-24h
| bin _time span=10m
| stats count by _time
| eval window=""today""
| eval _time=(_time-(60*60*24))]
| timechart span=10m sum(count) by window",0
"SPL-306","Number of users who can run real-time searches","audit,search,user",Internal,,"index=_internal host=acdc-sec-splk-shcp2.cdc.gov source=*metrics.log group=search_concurrency ""system total"" NOT user=*
| timechart max(active_realtime_searches) AS ""Max Real-Time Searches""",0
"SPL-307","Counted against license",license,Internal,,"index=_internal source=* license_usage.log type=Usage
| stats sum(b) as bytes by date_wday, date_mday, date_month, idx
| eval GB = round(bytes/1024/1024/1024,5)
| table date_wday, date_mday, date_month, GB, idx",0
"SPL-308","Splunk License Usage Over the Last 30 Days",license,Internal,,"index=_internal source=*license_usage.log type=""RolloverSummary"" earliest=-30d@d
| eval _time=_time - 43200
| bin _time span=1d
| stats latest(b) AS b by slave, pool, _time
| timechart span=1d sum(b) AS ""volume"" fixedrange=false
| join type=outer _time [search index=_internal source=*license_usage.log type=""RolloverSummary"" earliest=-30d@d
| eval _time=_time - 43200
| bin _time span=1d
| stats latest(stacksz) AS ""stack size"" by _time]
| fields - _timediff
| foreach * [eval FIELD=round('FIELD'/1024/1024/1024, 3)]",0
"SPL-309","License Usage Data Cube",license,Internal,,"index=_internal source=*license_usage.log type=""Usage""
| eval h=if(len(h)=0 OR isnull(h),""(SQUASHED)"",h)
| eval s=if(len(s)=0 OR isnull(s),""(SQUASHED)"",s)
| eval idx=if(len(idx)=0 OR isnull(idx),""(UNKNOWN)"",idx)
| bin _time span=1d
| stats sum(b) as b by _time, pool, s, st, h, idx",0
"SPL-310","Top 5 License Consuming Hosts","license,pwn",Licensing,"The following Splunk search query will return the top five licensing consuming hosts:","index=_internal source=*license_usage.log type=""Usage""
| stats sum(b) AS volume by h
| eval  GB=round(volume/1024/1024/1024,5)
| table h GB
| sort 5 - GB",0
"SPL-311","License Usage by Index per Day","license,pwn",Licensing,"The following Splunk search query will output license usage for each index for each day for the week to date. It will also output an average for each index over the course of the given time period.","index=_internal source=*license_usage.log type=""Usage"" splunk_server=* earliest=-1w@d
| eval Date=strftime(_time, ""%Y/%m/%d"")
| eventstats sum(b) as volume by idx, Date
| eval MB=round(volume/1024/1024,5)
| timechart first(MB) AS volume by idx",0
"SPL-312","Data Consumption",license,Internal,,"index=_internal source=*license_usage.log type=Usage
| eval GB = b/1024/1024/1024
| timechart sum(GB) as GBytes
| streamstats sum(GBytes) as GB_Used
| predict GB_Used as GB_Expected algorithm=LLT future_timespan=14
| fields â€“ Gbytes",0
"SPL-313","Spread out scheduled searches","audit,search",Internal,,"index=_internal source=*metrics.log group=searchscheduler
| timechart partial=false span=10m sum(dispatched) AS Started sum(skipped) AS Skipped",0
"SPL-314","Are my forwarders connecting to my receiver?",inputs,Other,"Which IP addresses are connecting to
Splunk as inputs, and how many times is each IP logged in metrics.log?","index=_internal source=*metrics.log* tcpin_connections
| stats count by sourceIp",0
"SPL-316","Base for Summary Indexing","indexing,optimization,summary",Dashboards,"For Dashboard Optimization","index=_internal source=*splunkd_access.log
| bucket _time span=1h
| sistats avg(some) by _time, method, status
| addinfo
| collect index=",0
"SPL-317","Search to end all errors. Identifies frequently occurring errors in your splunk instance","errors,pwn",Errors,"LSS knocking out the top 10 on this list will make your splunk instance very happy.","index=_internal sourcetype=""splunkd"" log_level=""ERROR""
| stats sparkline count dc(host) as hosts last(_raw) as last_raw_msg values(sourcetype) as sourcetype last(_time) as last_msg_time first(_time) as first_msg_time values(index) as index by punct
| eval delta=round((first_msg_time-last_msg_time),2)
| eval msg_per_sec=round((count/delta),2)
| convert ctime(last_msg_time) ctime(first_msg_time)
| table last_raw_msg count hosts sparkline msg_per_sec sourcetype index first_msg_time last_msg_time delta
| sort -count",0
"SPL-318","Detailed list of Errors Per Host","error,errors,host,pwn,rest,splunkd",Errors,"The following Splunk search will return a detailed list (by message) of errors associated with hosts running a universal forwarder:","index=_internal sourcetype=""splunkd"" log_level=""ERROR""
| stats sparkline count dc(host) as uniqhosts last(message) as message last(_time) as last first(_time) as first by punct
| convert ctime(last) ctime(first)
| table message count uniqhosts sparkline first last
| sort -count
| rename message as ""Error Output"" count as Count uniqhosts as ""Number of Hosts"" first as ""First Occurance"" last as ""Most Recent Occurance""",0
"SPL-319","Ensure no scheduled searches are running on indexers","audit,searches",Internal,,"index=_internal sourcetype=scheduler result_count host!=acdc-sec-splk-shcp*
| extract pairdelim="","", kvdelim=""="", auto=f
| stats avg(result_count) min(result_count) max(result_count), sparkline(avg(result_count)) AS result_sparkline avg(run_time) min(run_time) max(run_time) sum(run_time) values(host) AS hosts count AS execution_count by savedsearch_name, app
| rename savedsearch_name AS title
| makemv delim="","" values(host)
| eval host_count=mvcount(hosts)
| makemv delim="","" hosts
| where app!=""TA-fire_brigade"" AND host_count > 1
| sort - ""avg(run_time)""",0
"SPL-320","Average Splunk Web requests by hour","activity,congestion,pwn,user",SPL,"This query is pretty awesome! It helped enlighten us to exactly when our splunk infrastructure is being hit with users.","index=_internal sourcetype=splunk_web_access
 [ rest / splunk_server=local
 | fields splunk_server
 | rename splunk_server as host ]
 | bin _time span=1d
 | stats count by date_hour _time
 | appendpipe [ fields _time
 | dedup _time
 | eval date_hour=mvrange(0,24,1)
 | eval count=0
 | mvexpand date_hour ]
 | stats sum(count) as count by date_hour _time
 | stats avg(count) as avg by date_hour
 | sort date_hour",0
"SPL-321","Large number of knowledge objects being pushed to the shcluster",audit,"Knowledge_Objects","Large number of knowledge objects being pushed to the shcluster.","index=_internal sourcetype=splunkd component=DistributedBundleReplicationManager bundle_file_size=*
| rex field=bundle_file_size ""(?<bundle_size>\d+)""
| eval bundle_size_mb=bundle_size/1024
| stats count max(bundle_size_mb) by host",0
"SPL-322","License Usage Forecast Model",license,Licensing,"The following query can be used for Splunk license capacity planning. It is only a prediction based on your current ingestion rate, and attempts to predict what your usage will be over the next 120 days. It is recommended not to use all time in your search, as the predict command takes quite a bit of [â€¦]","index=_internal todaysbytesindexed startdaysago=60
| eval GB_Indexed = todaysBytesIndexed/1024/1024/1024
| timechart span=1d avg(""GB_Indexed"")
| predict avg(""GB_Indexed"") future_timespan=120 as ""Forecast Index Rate""
| rename avg(""GB_Indexed"") as ""Current Index Rate""",0
"SPL-104","The idea is to tie ESA emails together by message id",email,GMC,,"index=email
| transaction internal_message_id dcid icid maxevents=100 maxspan=30s endswith=""Message finished""",0
"SPL-325","Qualys Active OS Vuln Count",qualys,Security,,"index=qualys HOSTVULN SEVERITY=3 OR 4 OR 5 TYPE=""CONFIRMED"" earliest=-30d@d
| dedup HOST_ID, QID | search STATUS!=""FIXED""
| join QID [search index=qualys QID_INFO PATCHABLE=1]
| join HOST_ID [search index=qualys HOSTSUMMARY: OS=""Windows*"" NOT ""Windows Server*""
| where cidrmatch(""10.128.0.0/9"", IP) ]
| stats count(QID) as #_Vulns by OS | sort -#_Vulns | addcoltotals #_Vulns",0
"SPL-326","Measure the delay between the time stamp of the events and the indexing time",delay,Internal,"The time that the indexer receives and processes the events, use the following method: Look at the delay in seconds per source for a particular host.","source=mysource host=myhost
| eval delay_sec=_indextime-_time
| timechart min(delay_sec) avg(delay_sec) max(delay_sec) by source",0
"SPL-327","Index Sizes","pwn,rest",Indexes,,"| rest /services/data/indexes
| eval currentDBSizeMB=tostring(currentDBSizeMB, ""commas"")
| eval totalEventCount=tostring(totalEventCount, ""commas"")
| eval frozenTimePeriodInHours=(frozenTimePeriodInSecs/60/60)
| table title splunk_server currentDBSizeMB frozenTimePeriodInHours maxTime minTime totalEventCount",0
"SPL-328","View audit trail information stored in the local audit index",audit,Internal,,"| audit",0
"SPL-329","Check indexers to see if web access is available","access,web",Internal,,"| btool web
| where app!=""learned"" AND ([| inputlookup assets.csv where search_group=""dmc_customgroup_Indexer""
| stats count by host
| fields + host
| rename host AS sos_server
| format])
|  stats dc(app) AS app_count count AS stanza_count sum(linecount) AS setting_count by sos_server
| eventstats mode(stanza_count) AS mode_st mode(setting_count) AS mode_lc mode(app_count) AS mode_app
| eval Status=if(mode_st!=stanza_count OR mode_lc!=setting_count OR mode_app!=app_count,""elevated!!!deviation found"", ""low!!!ok"")
| eval ""Application Deviation""=app_count-mode_app
| eval ""Setting Deviation""=setting_count-mode_lc
| eval ""Stanza Deviation""=stanza_count-mode_st
| fields - mode*
| rename sos_server AS Host app_count AS ""Unique Applications"" stanza_count AS ""Stanza Count"" setting_count AS ""Setting Count""
| sort Status",0
"SPL-330","Get a high level summary of your indexes (must be run on the indexer)",index,Internal,,"| dbinspect",0
"SPL-331","Combine dbinspect and REST api data for buckets","addinfo,append,dbinspect,meta,rest",Indexes,"The dbinspect API doesn't return consistent information about the size of buckets.","| dbinspect index=*
| foreach *
    [eval dbinspect_FIELD = 'FIELD']
| table dbinspect_*
| append    [
	| rest splunk_server_group=dmc_group_cluster_master ""/services/cluster/master/buckets""
	| foreach *
	    [eval rest_api_FIELD = 'FIELD']
| table rest_api_*    ]
| eval bucketId=if(isNull(rest_api_title),dbinspect_bucketId,rest_api_title)
| stats values(*) as * by bucketId
| foreach rest_api_peers.*.*    [eval rest_api_<<MATCHSEG2>>=""""]
| foreach rest_api_peers.*.*    [eval rest_api_<<MATCHSEG2>>=if (""<<MATCHSEG1>>""=dbinspect_bucketId,'FIELD','<<MATCHSEG2>>')]
| fields - rest_api_peers.*",0
"SPL-332","Returns information about index buckets","audit,buckets,dbinspect",Miscellaneous,"Returns information about the the Splunk index that you specify","| dbinspect index=_internal span=1d",0
"SPL-333","Create a field whose name is the value of another field",field,Internal,,"| eval {aName}=aValue",0
"SPL-334","Count number of events of Splunk processes","events,splunkd",Internal,,"| eventcount index=_internal",0
"SPL-335","Search to show Raw Data Size, Size of Data on the Disk, and Compression Ratio overall","disk,indexes,pwn",Internal,,"| eventcount summarize=false index=* | dedup index | fields index
| map maxsearches=100 search=""|dbinspect index=\""$index$\""| fields rawSize, sizeOnDiskMB, eventCount | table rawSize, sizeOnDiskMB, eventCount""
| stats sum(rawSize) AS rawTotal, sum(sizeOnDiskMB) AS diskTotalinMB, sum(eventCount) AS Total_Events
| eval rawTotalinMB=(rawTotal / 1024 / 1024)
| eval compression=(diskTotalinMB / rawTotalinMB * 100)
| eval Compression_Ratio=tostring(round((100 - compression),2)) + ""%""
| eval Raw_Total_MB=tostring(round(rawTotalinMB,2),""commas"")
| eval OnDisk_Total_MB=tostring(round(diskTotalinMB,4),""commas"")
| eval Total_Events=tostring(Total_Events,""commas"")
| table Total_Events, Raw_Total_MB, OnDisk_Total_MB, Compression_Ratio",0
"SPL-336","Indexes an event counts within an app",events,Internal,,"| eventcount summarize=false index=* | dedup index | fields index count",0
"SPL-338","How to move a field through time for prediction purposes","learning,machine,predictive,streamstats",SPL,"Align a future value with the features in the past based on some time delta (Time to Decision, Time to Action) for machine learning or predictive analytics in general.","| inputlookup SOMECSV
| reverse
| streamstats window=1 current=f first(RemoteAccess) as RemoteAccessFromFuture
| reverse
| ...",0
"SPL-339","Create a Normal Curve","bin,eval,makeresults,stats",Analytics,,"| makeresults count=50000
| eval r = random() / (pow(2,31)-1)
| eval r2 = random() / (pow(2,31)-1)
| eval normal = sqrt(-2 * ln(r)) * cos(2 * pi() * r2)
| bin normal span=0.1
| stats count by normal
| makecontinuous normal",0
"SPL-340","Metadata Searches",meta,Internal,,"| metadata index=* type=sources",0
"SPL-341","Find out what hosts (or sources or sourcetypes) have sent data to Splunk","hosts,splunkd",Internal,,"| metadata type=hosts",0
"SPL-342","Convert ctime(*Time) to make the epoch time readable",epoch,SPL,"","| metadata type=hosts
 | convert ctime(*Time)",0
"SPL-343","Find Future Time Descrepencies by Host",predict,SPL,,"| metadata type=hosts index=*
| where lastTime > now()
| eval secondsInFuture=lastTime-now()
| eval hrInFuture=round(secondsInFuture/3600,1)
| sort -secondsInFuture
| table host secondsInFuture hrInFuture",0
"SPL-344","What hosts (not forwarder/TCP inputs) have logged an event to Splunk in the last 10 minutes","hosts,nodata",Internal,"Including rangemap","| metadata type=hosts index=INDEX
| eval diff=now()-recentTime
| where diff < 600
| convert ctime(*Time)
| stats count
| rangemap field=count low=800-2000 elevated=100-799 high=50-99 server=0-49",0
"SPL-345","Hosts within an index Index meta data","host,hosts,index,internal,meta",Internal,,"| metadata type=hosts index=_internal",0
"SPL-346","List of Host Names without Fully Qualified Domain Names","dns,pwn,security",OS,,"| metadata type=hosts | rex field=host ""(?<shortHost>[^.]+)$"" | rex field=host ""(?<shortHost>[^.]+)\.""
| dedup shortHost
| table shortHost",0
"SPL-347","Sourcetypes within an index","index,sourcetype",Internal,,"| metadata type=sourcetypes index=""INDEX""",0
"SPL-348","Data feed status delay by over 30 minutes","audit,forwarder",Internal,,"| metadata type=sourcetypes index=*
| search
    [| inputlookup sta_all_sourcetypes.csv
    | fields sourcetype ]
| sort - totalCount
| eval Delay=now()-recentTime
| rangemap default=severe field=Delay low=0-1800
| convert ctime(recentTime) AS ""Last Indexed""
| table range, sourcetype, ""Last Indexed"", Delay, totalCount
| eval Delay=tostring(Delay,""duration"")
| eval totalCount=tostring(totalCount, ""commas"")
| rename totalCount AS Events, range AS Status sourcetype AS Sourcetype
| sort + ""Last Indexed""
| where Status=""severe"" AND NOT match(Sourcetype, ""-(\d+|too_small)$"")",0
"SPL-349","Return the values of sourcetypes for events in the non internal indexes",sourcetype,SPL,"","| metadata type=sourcetypes index=* NOT _internal",0
"SPL-350","Return the values of sourcetypes for events in the _internal index",sourcetype,Internal,"Return the values of ""sourcetypes"" for events in the ""_internal"" index.","| metadata type=sourcetypes index=_internal",0
"SPL-351","REST Call for Memory & CPU usage on Splunk Servers",rest,"Resource_Usage",,"| rest  /services/server/status/resource-usage/hostwide
| eval cpu_count = if(isnull(cpu_count), ""N/A"", cpu_count)
| eval cpu_usage = cpu_system_pct + cpu_user_pct | eval mem_used_pct = round(mem_used / mem * 100 , 2)
| eval mem_used = round(mem_used, 0) | eval mem = round(mem, 0) |eval mem=tostring(mem, ""commas"")
| eval mem_used=tostring(mem_used, ""commas"")
| fields splunk_server, cpu_count, cpu_usage, mem, mem_used, mem_used_pct
| sort - cpu_usage, -mem_used
| rename splunk_server AS Instance, cpu_count AS ""CPU Cores"", cpu_usage AS ""CPU Usage (%)"", mem AS ""Physical Memory Capacity (MB)"", mem_used AS ""Physical Memory Usage (MB)"", mem_used_pct AS ""Physical Memory Usage (%)""",0
"SPL-352","List of fired alerts in Splunk","alerts,pwn,rest",Alerting,"","| rest /services/alerts/fired_alerts splunk_server=local
| table eai:acl.owner eai:acl.app id title triggered_alert_count",0
"SPL-353","Show all currently logged in users","pwn,rest,users",Internal,,"| rest /services/authentication/current-context
| search NOT username=â€splunk-system-userâ€
| table username roles updated",0
"SPL-354","Permissions for splunk usersAnother view for which splunk user can do what in your splunk environment","permissions,pwn,splunkd,rest",RBAC,"Permissions for splunk users
Another view for which splunk user can do what in your splunk environment","| rest /services/authentication/users
| mvexpand roles
| table realname, title, roles, email
| join roles [ rest /services/authorization/roles
| rename title as roles
| search srchIndexesAllowed=*
| table roles srchIndexesAllowed]",0
"SPL-355","Look for a specific index ","indexes,rest",Internal,,"| rest /services/data/indexes
| stats sum(totalEventCount) as Events dc(splunk_server) as NumServers by title",0
"SPL-356","List of Props.conf Extractions","props,pwn,rest",Internal,,"| rest /services/data/props/extractions
| table title type value attribute",0
"SPL-357","List of extractions in transforms.conf","rest,transforms",Internal,,"| rest /services/data/transforms/extractions
| table title eai:appName REGEX FORMAT updated",0
"SPL-358","REST Call for a list of Lookup Files","lookup,lookups,rest",Internal,,"| rest /services/data/transforms/lookups
| table eai:acl.app eai:appName filename title fields_list updated id",0
"SPL-359","Detect Clock Skew","utils,rest",Security,"Detect Clock Skew. If delta is anything other than about 00:00:01 (which is easy to account for when processing a lot of indexers), you may have clock skew.","| rest /services/server/info
| eval updated_t=round(strptime(updated, ""%Y-%m-%dT%H:%M:%S%z""))
| eval delta_t=now()-updated_t
| eval delta=tostring(abs(delta_t), ""duration"")
| table serverName, updated, updated_t, delta, delta_t",0
"SPL-360","REST Call for Splunk Server Role Status","pwn,role,server,status,rest",Infrastructure,"","| rest /services/server/introspection | table title splunk_server status updated",0
"SPL-361","Splunk Objects With Permissions Granted to Non-existent Roles","objects,permissions,pwn,rest",RBAC,"","| rest /servicesNS/-/-/admin/directory count=0 splunk_server=local
 | fields eai:acl.app, eai:acl.owner, eai:acl.perms.*, eai:acl.sharing, eai:location, title
 | eval perms=mvappend('eai:acl.perms.read','eai:acl.perms.write')
 | fields - eai:acl.perms.*
 | mvexpand perms
 | where perms!=""*"" AND NOT
 [
 | rest /servicesNS/-/-/authorization/roles count=0 splunk_server=local
 | fields title
 | rename title as perms
 ]",0
"SPL-362","Clears browser cache","dev,rest","Indexer_Cluster","You can use these other commands as well:
1. | rest /servicesNS/-/-/configs/conf-props/_reload
2. | extract reload=t","| rest /servicesNS/-/-/admin/monitor/_reload(splunkURL)/en-us/debug/refresh


",0
"SPL-364","Percentage of Daily License Usage","license,rest",Licensing,"This Splunk search query will indicate the percentage of license used for the current day. This is already shown in the licensing tab under settings, however this query is extracted if you would want to use it within a dashboard or any other reason.","| rest splunk_server=* /services/licenser/pools
| rename title AS Pool
| search [rest splunk_server=* /services/licenser/groups
| search is_active=1
| eval stack_id=stack_ids
| fields stack_id]
| eval quota=if(isnull(effective_quota),quota,effective_quota)
| eval ""% used""=round(used_bytes/quota*100,2)
| fields ""% used""",0
"SPL-365","Hard Disk Usage and Information on Splunk Server","disk,rest,splunkd","Resource_Usage","The following Splunk Query will utilize a â€œ| RESTâ€ call to gather information related to disk usage on your Splunk server(s). The following has been modified from the â€œDistributed Management Consoleâ€ to be more generic for a copy, paste, and search example.","| rest splunk_server=* /services/server/status/partitions-space
| eval free = if(isnotnull(available), available, free)
| eval usage = round((capacity - free) / 1024, 2) | eval capacity = round(capacity / 1024, 2)
| eval compare_usage = usage."" / "".capacity | eval pct_usage = round(usage / capacity * 100, 2)
| stats first(fs_type) as fs_type first(compare_usage) as compare_usage first(pct_usage) as pct_usage by mount_point, splunk_server
| rename mount_point as ""Mount Point"", fs_type as ""File System Type"", compare_usage as ""Disk Usage (GB)"", capacity as ""Capacity (GB)"", pct_usage as ""Disk Usage (%)"" splunk_server as ""Splunk Server""
| sort - ""Splunk Server""",0
"SPL-366","Memory Usage and Information on Splunk Server","mem,rest","Resource_Usage",,"| rest splunk_server=* /services/server/status/resource-usage/hostwide
| stats first(normalized_load_avg_1min) as load_average first(cpu_system_pct) as system, first(cpu_user_pct) as user first(mem) AS mem first(mem_used) AS mem_used by splunk_server
| fields splunk_server mem mem_used
| eval pctmemused=round((mem_used/mem)*100).""%""
| table splunk_server pctmemused
| rename splunk_server as ""Splunk Server"" pctmemused as ""Percent of Memory Used""",0
"SPL-368","Address unnecessarily frequent scheduled searches and overall unnecessary searches","audit,searches,rest",Search,"","| rest splunk_server=* /servicesNS/-/-/admin/savedsearch/ earliest_time=-0s@s latest_time=+2d@d search=""is_scheduled=1"" search=""disabled=0""
| table splunk_server eai:acl.app eai:acl.owner cron_schedule title scheduled_times
| mvexpand scheduled_times
| rename scheduled_times as _time eai:acl.app as app eai:acl.owner as user title as search
| stats count(search) as count by search cron_schedule app
| sort 20 -count",0
"SPL-369","Search to show what apps are ready to be updated","app,rest",Infrastructure,,"| rest splunk_server=local /services/apps/local
| search update.version=* | table title version update.version",0
"SPL-370","Splunk License Gauge","license,rest",Licensing,,"| rest splunk_server=local /services/licenser/pools
| rename title AS Pool
| search [rest splunk_server=local /services/licenser/groups
| search is_active=1
| eval stack_id=stack_ids
| fields stack_id]
| join type=outer stack_id [rest splunk_server=local /services/licenser/stacks | eval stack_id=title
| eval stack_quota=quota | fields stack_id stack_quota]
| stats sum(used_bytes) as used max(stack_quota) as total
| eval usedGB=round(used/1024/1024/1024,3)
| eval totalGB=round(total/1024/1024/1024,3)
| eval gauge_base=0
| eval gauge_danger=totalGB*0.8
| eval gauge_top=totalGB+0.001
| gauge usedGB gauge_base gauge_danger totalGB gauge_top",0
"SPL-371","Get information on all indexes","_internal,rest","Indexer_Cluster",,"| rest splunk_server=local /servicesNS/-/-/configs/conf-indexes
| rename eai:appName as app eai:acl.sharing as sharing
| table app sharing author title homePath coldPath thawedPath frozenTimePeriodInSecs maxDataSize maxHotBuckets maxTotalDataSizeMB
| foreach * [eval FIELD = if(len(FIELD)>0, 'FIELD', ""-"")]
| join app type=left [| rest splunk_server=local /servicesNS/-/-/apps/local
| rename title as app label as app_label
| table app app_label]
| eval app_label = if(isnotnull(app_label), app_label, app)",0
"SPL-372","Dashboards that have not been viewed in over 21 days or are orphaned","audit,dashboard,rest",Dashboards,,"| rest splunk_server=local /servicesNS/-/-/data/ui/views
| stats count values(eai:acl.sharing) values(eai:acl.owner) AS owner by title, eai:acl.app
| rename eai:acl.app AS app
| eval last_view_time_days_ago=-1
| eval last_view_time_days_ago_pretty=""no views in last 30 days""
| join type=outer title app
    [ search index=""_internal"" sourcetype=splunk_web_access GET app earliest=-30d@d
    | rex ""GET /[^/]+/app/(?<app>[^/ ?]+)/(?<title>[^\/\?\s]+)""
    | search app=* AND title=* AND user=* AND user!=""-""
    | stats latest(_time) AS last_view_time latest(user) AS last_user by app, title
    | eval last_view_time_days_ago=round(abs(now()-last_view_time),0)
    | eval last_view_time_days_ago_pretty=tostring(last_view_time_days_ago, ""duration"")
    | convert ctime(last_view_time)]
| where last_view_time_days_ago > (60*60*24*21) OR last_view_time_days_ago=-1",0
"SPL-373","Review alerts that are not returning results to ensure no results is the expected/desired outcome","alerts,audit,rest","Scheduled_Jobs","Review alerts that are not returning results to ensure ""no results"" is the expected/desired outcome.","| rest splunk_server=local /servicesNS/-/-/saved/searches
| where is_scheduled=1 AND disabled=0
| stats values(splunk_server) by title, eai:acl.owner, eai:acl.app, cron_schedule
| join type=outer title
    [ search index=_internal sourcetype=scheduler result_count earliest=-30d@d
    | extract pairdelim="","", kvdelim=""="", auto=f
    | stats avg(result_count) min(result_count) max(result_count), sparkline(avg(result_count)) AS sparkline avg(run_time) by savedsearch_name
    | rename savedsearch_name AS title]
| makemv delim="","" setsv=true sparkline
| where 'max(result_count)' = 0
| sort - avg(run_time)
| search title!=""Missing Data*"" (eai:acl.app=search OR eai:acl.app=CDC-*)",0
"SPL-374","Multiple indexes getting full without reaching retention policy","audit,index,rest","Indexer_Cluster",,"| rest splunk_server_group=dmc_group_indexer /services/data/indexes search=""totalEventCount!=0""
| eval coldPath.maxDataSizeMB=if('coldPath.maxDataSizeMB' = 0, null(), 'coldPath.maxDataSizeMB')
| eval homePath.maxDataSizeMB=if('homePath.maxDataSizeMB' = 0, null(), 'homePath.maxDataSizeMB')
| eval roof=min((coalesce('homePath.maxDataSizeMB', 4294967295) + coalesce('coldPath.maxDataSizeMB', 4294967295)), maxTotalDataSizeMB)
| eval span=tostring(currentDBSizeMB) + "" / "" + tostring(roof) + "" MB""
| eval PercentFull=tostring(round(currentDBSizeMB * 100 / roof)) + ""%""
| eval ""Total Events""=tostring(totalEventCount,""commas"")
| stats first(span) AS ""Capacity vs Limit"" by splunk_server title minTime maxTime ""Total Events"" PercentFull
| rename splunk_server AS Indexer title AS Index minTime AS ""Oldest Event"" maxTime AS ""Newest Event""
| table Indexer Index ""Capacity vs Limit"" ""Oldest Event"" ""Newest Event"" ""Total Events"" PercentFull
| sort Index, Indexer, -PercentFull
| where match(PercentFull, ""^(10|9|8).%"")",0
"SPL-375","Forwarder Diagnostics â€“ Last time Data Was Received by Index and Sourcetype","","Universal_Forwarders","","| tstats latest(_time) as Latest by host sourcetype index
| eval current=now()
| eval Minimum_Age=round(((current-Latest)/60)/60,2)
| rangemap field=Minimum_Age default=Critical Normal=0-0.5 Elevated=0.5-2 Warning=2-4
| eval stIDX=tostring(index) + "" -- "" + tostring(sourcetype)
| stats values(stIDX) as ""Index -- Sourcetype"" list(Latest) as ""Latest Event"" list(Minimum_Age) as Minimum_Age list(range) as Threshold by host
| convert ctime(""Latest Event"")
| eventstats avg(Minimum_Age) as average by host
| eval average=round(average,2)
| sort - average
| rename Minimum_Age as ""Hours Since Last Communication"" average as ""Average Time in Hours Since Last Communication""",0
"SPL-376","WEB CIM data model predict web request durations","cim,predict",Miscellaneous,,"| tstats summariesonly=t perc95(web_request.wDuration) AS estPC95, perc90(web_request.wDuration) AS estPC90, perc50(web_request.wDuration) AS estPC50 from datamodel=""web"" where (nodename=""web_request"") groupby ""_time"" span=1m
| fields _time, estPC*",0
"SPL-377","Typeahead values for an index",typeahead,Internal,,"| typeahead prefix=source count=10 index=_internal",0
"GMC-042","Jobs with Duplicate search strings","gmc,duplicates,health_assessments","Scheduled_Jobs",,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup
| where is_scheduled=1 AND disabled=0 AND ! savedsearch_name LIKE ""%_ScheduledView_%""
| search NOT app IN (splunk_instance_monitoring)
| fields shcluster_label app author savedsearch_name savedsearch_search
| stats values(*) As * count by savedsearch_search
| where count > 1
| sort 0 - count
| table shcluster_label app author savedsearch_name count savedsearch_search",0
"GMC-043","Job Naming Convention Based on GMC Identity data","gmc,naming_convention","Scheduled_Jobs","naming convention and used Logic to determine Ranking of 1-10, where 1 has the highest priority.  1-10 gives us full flexibility and better than low/medium/high because users normally will choose either high or super high","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where is_scheduled=1 AND disabled=0 AND shcluster_label = ""*"" 
| search NOT app IN (splunk_instrumentation, splunk_monitoring_console, splunk_archiver, simple_xml_examples, *global_monitoring_console, Centrify, Code42ForSplunk, SA-nix, Splunk_TA_*, cisco-app-ACI, splunk_app_*, website_monitoring, analysis_of_splunkbase_apps, chargeback) 
| `get_identity_info(author)` 
| stats Last(emp_lob2) as emp_lob2 Last(emp_dep) as Department by shcluster_label app sharing author savedsearch_type savedsearch_name 
| eval savedsearch_name_org=savedsearch_name 
| eval savedsearch_name=upper(trim(savedsearch_name)) 
| rex field=emp_lob2 mode=sed ""s/\s+/-/g"" 
| rex field=savedsearch_name mode=sed ""s/\s+/-/g"" 
| rex field=savedsearch_name mode=sed ""s/[^[:ascii:]]//g"" 
| rex field=savedsearch_name mode=sed ""s/[^a-zA-Z_\d-]/-/g"" 
| rex field=emp_lob2 mode=sed ""s/[^a-zA-Z_\d-]/-/g"" 
| rex field=savedsearch_name mode=sed ""s/-{2,}/-/g"" 
| rex field=emp_lob2 mode=sed ""s/-{2,}/-/g"" 
| eval savedsearch_name=trim(savedsearch_name,""-"") , emp_lob2=trim(emp_lob2,""-"") 
| eval Ranking = case(
    Match(savedsearch_name,""DEVELOPMENT""), ""9"",
    Match(savedsearch_name,""REPORT""), ""9"",
    Match(savedsearch_name,""TEST""), ""9"",
    Match(savedsearch_name,""WARNING|WARN""), ""6"",
    Match(savedsearch_name,""SECURITY"") OR Match(emp_lob2,""Cybersecurity""), ""1"",
    Match(savedsearch_name,""ALERT""), ""2"",
    Match(savedsearch_name,""ERROR""), ""3"",
    Match(savedsearch_name,""_SCHEDULEVIEW_""), ""6"",
    Match(savedsearch_name,""CLAIM""), ""4"",
    Match(savedsearch_name,""PROD""), ""6"",
    Match(savedsearch_name,""FAIL""), ""5"",
    Match(savedsearch_name,""COVID19""), ""2"",
    Match(savedsearch_name,""PAYMENT|BILLING""), ""3"",
    Match(savedsearch_name,""POLICY""), ""4"",
    Match(savedsearch_name,""CRITICAL""), ""2"",
    Match(savedsearch_name,""LOGON|ACCOUNT|AGENCY""), ""3"",
    Match(savedsearch_name,""AKAMAI""), ""3"",
    Match(savedsearch_name,""TIMEOUT|TIMED-OUT""), ""3"",
    Match(savedsearch_name,""MEMORY""), ""3"",
    Match(savedsearch_name,""STUCK|EXCEPTION""), ""3"",
    Match(savedsearch_name,""404""), ""3"",
    Match(savedsearch_name,""FIREWALL""), ""3"",
    Match(savedsearch_name,""FINANCE|STOPPED|CLEARED|VOIDED""), ""3"",
    Match(savedsearch_name,""MAJOR|DOWN|ON-FIRE|NOT-RUNNING|UNAVAILABLE""), ""3"",
    Match(savedsearch_name,""MINOR""), ""5"",
    true(), 10) 
| eval New_savedsearch_name = Ranking . "":"" . emp_lob2 . "":"" . savedsearch_name 
| stats count by Ranking New_savedsearch_name savedsearch_name_org",0
"GMC-044","Report on Splunk Server Environment","assets,ulimits,memory,cpu,health_assessments",Infrastructure,"Generates a report on all configured Splunk Instances with complete detail information about the hardware specs","| from lookup:splunk_rest_assets_kv_store_lookup 
| eval isForwarding = if (match(Splunk_Roles, ""indexer""), ""N/A"", isForwarding) 
| eval Memory = round (physicalMemoryMB / 1024, 2) 
| table _time *_label Splunk_Instance serverName Splunk_Roles host* version build guid cpu_arch numberOfCores numberOfVirtualCores Memory os_* startup_time isForwarding ulimits_* ulimits_stack_size transparent_* appServerPorts dfsDisabled dfsMasterPort enableSplunkWebSSL httpport kvStoreDisabled kvStorePort mgmtHostPort minFreeSpace sessionTimeout startwebserver trustedIP SPLUNK_* rtsearch_enabled mode master_uri master_guid kvStoreStatus *",0
"GMC-045","Report on Splunk Search Head Cluster Node configuration details","gmc,shc,clustering,health_assessments",Infrastructure,,"| from lookup:splunk_rest_shcluster_config_shc_kv_store_lookup 
| table shcluster_label Splunk_Instance mode conf_deploy_fetch_url adhoc_searchhead replication_factor replication_port replication_use_ssl async_replicate_on_proxy register_replication_address captain_is_adhoc_searchhead decommission_search_jobs_wait_secs dynamic_captain heartbeat_period manual_detention max_peer_rep_load percent_peers_to_restart ping_flag preferred_captain quiet_period rolling_restart cxn_timeout heartbeat_timeout rcv_timeout send_timeout rep_cxn_timeout rep_max_rcv_timeout rep_max_send_timeout rep_rcv_timeout rep_send_timeout restart_timeout *",0
"GMC-046","Report on Splunk Search Head Concurrency Settings","gmc,shc,concurrency,health_assessments",Infrastructure,,"| from lookup:splunk_rest_admin_search_concurrency_settings_handler_sh_kv_store_lookup 
| table _time shcluster_label Splunk_Instance max_hist_scheduled_searches base_max_searches max_searches_per_cpu max_searches_perc max_hist_searches max_auto_summary_searches auto_summary_perc max_rt_scheduled_searches max_rt_searches max_rt_search_multiplier",0
"GMC-047","Report on Assets Mount Points & Capacity","capacity,disk,health_assessments",Infrastructure,,"index=_introspection sourcetype=splunk_disk_objects component=Partitions search_group=*
| rename data.* as * , host As Splunk_Instance 
| fields _time Splunk_Instance mount_point fs_type available capacity free 
| eval free = if(isnotnull(available), available, free) 
| `get_idxcluster_label(Splunk_Instance)` 
| fillnull value=""Undefined"" idxcluster_label 
| stats latest(*) as * by idxcluster_label Splunk_Instance mount_point 
| `gmc_mb2human(capacity,2)` 
| `gmc_mb2human(free,2)` 
| rename free As free_MB capacity As capacity_MB 
| table idxcluster_label Splunk_Instance mount_point fs_type capacity_GB free_GB",0
"GMC-048","Report on all data being forwarded by UFs with sourcetypes configuration details","sourcetypes,dataonboarding,uf,health_assessments",Sourcetypes,,"| tstats count dc(source) As Num_Sources where index=* AND earliest=-4h@h by index sourcetype host 
| rename host As uf 
| join uf 
    [ search index=_internal sourcetype=splunkd (search_group=dmc_group_indexer OR host=idx-i-*) fwdType=uf earliest=-15m@m 
    | rename hostname As uf 
    | fields uf ] 
| stats count As Num_Events Last(Num_Sources) As Num_Sources dc(uf) As Num_UFs by index sourcetype 
| `get_sourcetype_info(sourcetype)` 
| fillnull value=""auto"" KV_MODE 
| fillnull value=""128"" MAX_TIMESTAMP_LOOKAHEAD 
| fillnull value=""0"" EVENT_BREAKER_ENABLE 
| fillnull value=""1"" ANNOTATE_PUNCT LEARN_MODEL SHOULD_LINEMERGE LEARN_SOURCETYPE BREAK_ONLY_BEFORE_DATE ADD_EXTRA_TIME_FIELDS AUTO_KV_JSON PREFIX_SOURCETYPE 
| fillnull value=""10000"" TRUNCATE 
| fillnull value=""^"" TIME_PREFIX 
| fillnull value=""([\r\n]+)"" LINE_BREAKER 
| fillnull value=""\r\n"" EVENT_BREAKER 
| fillnull value=""/etc/datetime.xml"" DATETIME_CONFIG 
| fillnull value=""not-set"" INDEXED_EXTRACTIONS 
| fillnull value=""UTF-8"" CHARSET 
| fillnull value=""endpoint_md5"" CHECK_METHOD 
| fillnull value=""1000"" DEPTH_LIMIT 
| fillnull value=""100"" LINE_BREAKER_LOOKBEHIND 
| fillnull value=""100000"" MATCH_LIMIT 
| fillnull value=""2000"" MAX_DAYS_AGO 
| fillnull value=""2"" MAX_DAYS_HENCE 
| fillnull value=""3600"" MAX_DIFF_SECS_AGO 
| fillnull value=""604800"" MAX_DIFF_SECS_HENCE 
| table index sourcetype Num_UFs Num_Sources Num_Events app author category updated sharing LINE_BREAKER SHOULD_LINEMERGE TRUNCATE TIME_PREFIX TIME_FORMAT MAX_TIMESTAMP_LOOKAHEAD TZ EVENT_BREAKER_ENABLE EVENT_BREAKER KV_MODE LEARN_MODEL LEARN_SOURCETYPE MUST_BREAK_AFTER BREAK_ONLY_BEFORE",0
"GMC-049","Cloud Search Head Cluster Members Job Execution details with Skip Ratio","skips,health_assessments","Scheduled_Jobs","This search only looks at the Stack SHC Members excluding the captain and does the calculation","index=customer_internal sourcetype=scheduler stack=<Stack Name> host=sh*.splunkcloud.com shcluster_member=1 status IN (success,skipped) earliest=-4h@h latest=-0h@h search_type!=*_acceleration 
| timechart span=5m
    count(eval(status==""success"")) AS COMP_EXEC
    count(eval(status==""skipped"")) AS Skipped_EXEC
    count(eval(status==""success"" OR status==""skipped"")) AS TOT_EXEC 
| eval Skip_Ratio = round ( ( Skipped_EXEC / TOT_EXEC ) * 100, 0) 
| fields - TOT_EXEC",0
"GMC-050","Report on user successful logins or user logged in and active",user,SPL,,"host=< Search Heads to search> ( (index=_audit sourcetype=audittrail) AND (user=* ""action=login attempt"" info=succeeded) ) OR (user=* NOT user IN (""n/a"",splunk-system-user) action=search info=granted search_id!=""rsa_*"" search=*) OR (index=_internal sourcetype=splunkd_ui_access useragent=* status=200 user!=""-"" ) 
| eval search_type = if(match(search_id, ""\d{10}\.\d+(_[0-9A-F]{8}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{4}-[0-9A-F]{12})?$""), ""adhoc"", ""other"") 
| search info=""succeeded"" OR search_type=""adhoc"" OR (index=_internal sourcetype=splunkd_ui_access useragent=* status=200 user!=""-"")",0
"GMC-051","Report on Role Configurations","roles,rbac,user,health_assessments",Infrastructure,,"| inputlookup splunk_rest_authorization_roles_sh_kv_store_lookup 
| table shcluster_label title
    srchJobsQuota imported_srchJobsQuota cumulativeSrchJobsQuota 
    rtSrchJobsQuota imported_rtSrchJobsQuota cumulativeRTSrchJobsQuota 
    srchDiskQuota imported_srchDiskQuota 
    imported_roles grantable_roles splunk_role_map ldap_group_type ldap_group_name auth_type 
    defaultApp deleteIndexesAllowed 
    srchFilter imported_srchFilter
    srchTimeWin imported_srchTimeWin 
    srchIndexesAllowed imported_srchIndexesAllowed srchIndexesDefault imported_srchIndexesDefault",0
"GMC-052","Report on Dashboards being used","dashboards,health_assessments",Dashboards,,"host=<ITSI Search Heads> index=_internal sourcetype=splunk_web_access method=GET status=200 user!=""-"" 
| fields _time host search_group app view user spent 
| stats earliest(_time) As Earliest_Access latest(_time) As Latest_Access Values(user) As User BY app view 
| rename view As Dashboard_Name 
| convert ctime(*_Access) 
| table app Dashboard_Name User Earliest_Access Latest_Access",0
"GMC-059","Report om Indexer Cluster Configurations","indexers,cluster","Indexer_Cluster","","| inputlookup splunk_rest_cluster_config_idx_kv_store_lookup 
| stats Last(*) As * Last(_time) As _time by idxcluster_label 
| table _time idxcluster_label forwarderdata_rcv_port guid mode access_logging_for_heartbeats allowed_hbmiss_count cxn_timeout decommission_node_force_timeout decommission_search_jobs_wait_secs disabled forwarderdata_use_ssl frozen_notifications_per_batch heartbeat_period heartbeat_timeout manual_detention max_auto_service_interval max_fixup_time_ms max_peer_build_load max_peer_rep_load max_peer_sum_rep_load max_peers_to_download_bundle max_primary_backups_per_service notify_scan_min_period notify_scan_period percent_peers_to_restart ping_flag quiet_period rcv_timeout register_forwarder_address register_replication_address register_search_address remote_storage_upload_timeout rep_cxn_timeout rep_max_rcv_timeout rep_max_send_timeout rep_rcv_timeout rep_send_timeout replication_factor replication_port replication_use_ssl report_remote_storage_bucket_upload_to_targets reporting_delay_period restart_timeout search_factor search_files_retry_timeout send_timeout service_interval site warm_bucket_replication_pre_upload",0
"GMC-053","Jobs with Duplicate Names","duplicates,health_assessments","Scheduled_Jobs",,"| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup
| where is_scheduled=1 AND disabled=0 AND ! savedsearch_name LIKE ""%_ScheduledView_%""
| search NOT app IN (splunk_instance_monitoring,splunk_instrumentation)
| fields shcluster_label app author savedsearch_name savedsearch_search
| stats values(*) As * count by savedsearch_name
| where count > 1
| sort 0 - count
| table shcluster_label app author savedsearch_name count savedsearch_search",0
"GMC-054","Report on indexer queue fill-up","indexer,queues,health_assessments,rest","Indexer_Cluster","","| rest splunk_server_group=dmc_group_indexer splunk_server_group=""*"" /services/server/introspection/queues 
| search title=parsingQueue* OR title=aggQueue* OR title=typingQueue* OR title=indexQueue* 
| eval fill_perc=round(current_size_bytes / max_size_bytes * 100,2) 
| fields splunk_server, title, fill_perc 
| rex field=title ""(?<queue_name>^\w+)(?:\.(?<pipeline_number>\d+))?"" 
| eval fill_perc = if(isnotnull(pipeline_number), ""pset"".pipeline_number."": "".fill_perc, fill_perc) 
| chart values(fill_perc) over splunk_server by queue_name 
| eval pset_count = mvcount(parsingQueue) 
| join type=outer splunk_server 
    [| rest splunk_server_group=dmc_group_indexer splunk_server_group=""*"" /services/server/introspection/indexer 
    | eval average_KBps = round(average_KBps, 0) 
    | eval status = if((reason == ""."") OR (reason == """") OR isnull(reason), status, status."": "".reason) 
    | fields splunk_server, average_KBps, status] 
| fields splunk_server pset_count average_KBps status parsingQueue aggQueue typingQueue indexQueue 
| sort -average_KBps 
| rename splunk_server as Instance, pset_count as ""Pipeline Set Count"", average_KBps as ""Indexing Rate (KB/s)"", status as ""Status"", parsingQueue as ""Parsing Queue Fill Ratio (%)"", aggQueue as ""Aggregation Queue Fill Ratio (%)"", ""typingQueue"" as ""Typing Queue Fill Ratio (%)"", indexQueue as ""Indexing Queue Fill Ratio (%)""",0
"GMC-055","Persistent High Index Queue","indexer,queues,health_assessments","Indexer_Cluster",,"(index=_internal source=*metrics.log sourcetype=splunkd group=queue (name=aggqueue OR name=indexqueue OR name=parsingqueue OR name=typingqueue)) 
| eval ingest_pipe=if(isnotnull(ingest_pipe),ingest_pipe,""none"") 
| search ingest_pipe=* 
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size), curr=if(isnotnull(current_size_kb),current_size_kb,current_size), fill_perc=round(((curr / max) * 100),2) 
| bin _time span=3m 
| stats MAX(fill_perc) as maxQueue by _time host name 
| where maxQueue>90 
| stats count as highQueueCount, values(maxQueue) as maxQueue by host name 
| where highQueueCount>15 
| rename name as data_path, host as dest, highQueueCount as count 
| eval meta_alert_name=""Persistent High Index Queue""",0
"GMC-056","Blocked Queues","indexer,queues,health_assessments","Indexer_Cluster",,"index=_internal source=*metrics.log sourcetype=splunkd group=queue 
| eval max=if(isnotnull(max_size_kb),max_size_kb,max_size) 
| eval curr=if(isnotnull(current_size_kb),current_size_kb,current_size) 
| eval fill_perc=round((curr/max)*100,2) 
| where fill_perc>=99.0 
| bucket _time span=1m 
| stats count as blockedcount by host name _time 
| eval name=case(name==""aggqueue"",""2 - Aggregation Queue"",name==""indexqueue"",""4 - Indexing Queue"",name==""parsingqueue"",""1 - Parsing Queue"",name==""typingqueue"",""3 - Typing Queue"", 1=1, name) 
| eval blockedpercentage=((blockedcount*100)/2) 
| rex field=host ""^.*\.(?<ddc>\w+)\.expertcity.com"" 
| eval dc=coalesce(ddc,""NoFQDN"") 
| eval dc-queue=dc+""-""+name 
| chart useother=F avg(blockedpercentage) as blocked_percentage by _time, dc-queue",0
"GMC-057","Report on aborted Jobs related to Workload Management Rules deployed","workload,abort,health_assessments","Scheduled_Jobs",,"index=_internal sourcetype=splunkd component=WorkloadManager 
| rex ""workload\s+rule\s+(?<workload_rule>.*?)\."" 
| rex ""The\s+search\s+(?<sid>.*?)\s"" 
| fields _time host sid workload_rule event_message 
| join sid 
    [ search index=_internal sourcetype=scheduler sid=* 
    | fields _time host sid user app savedsearch_name scheduled_time dispatch_time workload_pool] 
| stats count As Num_Aborts latest(_time) As _time latest(*) As * By app user savedsearch_name 
| `get_saved_searches_info(savedsearch_name)` 
| `strftime_format(_time)` 
| `strftime_format(updated)` 
| table _time savedsearch_name workload_rule workload_pool event_message app savedsearch_type cron_schedule updated allow_skew dispatch_earliest_time dispatch_latest_time Num_Aborts",0
"GMC-058","Report on All Splunk Messages Across the entire environment in the past 24 Hours","errors,messages,health_assessments",Health,"","index=`setup_summary_index` search_name=""splunk_rest_messages_sh_summary_tracker"" earliest=-24h@h latest=now 
| stats 
    Latest(*) As *
    Latest(timeCreated_epochSecs) As timeCreated
    By server message 
| `strftime_format(timeCreated)` 
| table timeCreated search_name server message help message_alternate severity",0
"GMC-063","Generate a list of all App Names executing jobs on the Indexer Cluster from the ITSI SHC","itsi,health_assessments","Scheduled_Jobs","Note: remove â€œ""data.search_props.search_head""=<Search Head Cluster Hosts>â€ to report on all Apps across the Cluster coming in from every search head in the environment.","host=<Indexer Cluster Hosts> ""data.search_props.search_head""=<Search Head Cluster Hosts> index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=-24h@h 
| rename ""data.*"" as ""*"", ""search_props.*"" as ""*"" 
| stats values(app) As Apps
| eval Apps=mvjoin(Apps,"","")",0
"GMC-060","Normalized CPU Usage of searches running on the Indexer Tier originating on the ITSI SHC","itsi,health_assessments","Scheduled_Jobs","Normalized CPU %: Percentage of CPU usage across all cores. 100% is equivalent to all CPU resources on the machine.
Note: replace <Search Head Cluster> with a value that will find searches originating from the ITSI search cluster.  This is usually the host name of all SH Cluster Members, you can use a wild card.  Do the same for host but this needs to be all indexers in production.","host=<Indexer Cluster Hosts> ""data.search_props.search_head""=<Search Head Cluster Hosts> index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=-24h@h 
| fields _time data.* host 
| rename ""data.*"" as ""*"", ""search_props.*"" as ""*"" 
| eval app_type=case(
    match(app,""ITSI|SA-ITOA|SA-UserAccess|itsi""), ""ITSI Apps"",
    match(app,""splunk_app_infrastructure""), ""Infrastructure App"",
    match(app,""Splunk_ML_Toolkit""), ""MLTK App"",
    match(app,""custom_app""), ""Custom Apps"",
    true(), ""Other Apps"") 
| timechart span=5m Avg(normalized_pct_cpu) AS AvgNormCPU Perc90(normalized_pct_cpu) As P90NormCPU Perc95(normalized_pct_cpu) As P95NormCPU
| foreach AvgNormCPU P90NormCPU P95NormCPU 
    [ eval <<FIELD>>=round('<<FIELD>>', 2)]",0
"GMC-061","Normalized CPU Usage of searches running on the Indexer Tier originating on the ITSI SHC Splitting by App Types","itsi,health_assessments","Scheduled_Jobs",,"host=<Indexer Cluster Hosts> ""data.search_props.search_head""=<Search Head Cluster Hosts> index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=-24h@h 
| fields _time data.* host 
| rename ""data.*"" as ""*"", ""search_props.*"" as ""*"" 
| eval app_type=case(
    match(app,""ITSI|SA-ITOA|SA-UserAccess|itsi""), ""ITSI Apps"",
    match(app,""splunk_app_infrastructure""), ""Infrastructure App"",
    match(app,""Splunk_ML_Toolkit""), ""MLTK App"",
    match(app,""custom_app""), ""Custom Apps"",
    true(), ""Other Apps"") 
| timechart span=5m Avg(normalized_pct_cpu) AS AvgNormCPU Perc90(normalized_pct_cpu) As P90NormCPU Perc95(normalized_pct_cpu) As P95NormCPU By app_type
| foreach AvgNormCPU P90NormCPU P95NormCPU 
    [ eval <<FIELD>>=round('<<FIELD>>', 2)]",0
"GMC-062A","Normalized CPU Usage of searches running on the Indexer Tier from everywhere Splitting by Search Head","itsi,health_assessments","Scheduled_Jobs","","host=<Indexer Cluster Hosts> index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=-24h@h 
| fields _time data.* host 
| rename ""data.*"" as ""*"", ""search_props.*"" as ""*"" 
| eval app_type=case(
    match(app,""ITSI|SA-ITOA|SA-UserAccess|itsi""), ""ITSI Apps"",
    match(app,""splunk_app_infrastructure""), ""Infrastructure App"",
    match(app,""Splunk_ML_Toolkit""), ""MLTK App"",
    match(app,""custom_app""), ""Custom Apps"",
    true(), ""Other Apps"") 
| timechart span=5m Avg(normalized_pct_cpu) AS AvgNormCPU Perc90(normalized_pct_cpu) As P90NormCPU Perc95(normalized_pct_cpu) As P95NormCPU By search_head
| foreach AvgNormCPU P90NormCPU P95NormCPU 
    [ eval <<FIELD>>=round('<<FIELD>>', 2)]",0
"GMC-062B","Normalized CPU Usage of searches running on the Indexer Tier from everywhere Splitting by App Type","itsi,health_assessments","Scheduled_Jobs","","host=<Indexer Cluster Hosts> index=_introspection sourcetype=splunk_resource_usage component=PerProcess earliest=-24h@h 
| fields _time data.* host 
| rename ""data.*"" as ""*"", ""search_props.*"" as ""*"" 
| eval app_type=case(
    match(app,""ITSI|SA-ITOA|SA-UserAccess|itsi""), ""ITSI Apps"",
    match(app,""splunk_app_infrastructure""), ""Infrastructure App"",
    match(app,""Splunk_ML_Toolkit""), ""MLTK App"",
    match(app,""custom_app""), ""Custom Apps"",
    true(), ""Other Apps"") 
| timechart span=5m Avg(normalized_pct_cpu) AS AvgNormCPU Perc90(normalized_pct_cpu) As P90NormCPU Perc95(normalized_pct_cpu) As P95NormCPU By app_type
| foreach AvgNormCPU P90NormCPU P95NormCPU 
    [ eval <<FIELD>>=round('<<FIELD>>', 2)]",0
"GMC-064","Scheduled Jobs on the ITSI SHC cluster","itsi,health_assessments,rest","Scheduled_Jobs","ITSI Scheduled Alerts & Reports across the entire ITSI SHC cluster.
Reporting on all Jobs that are scheduled to run on the ITSI Search Head Cluster (SHC)","
| rest splunk_server=<Search Head Cluster Hosts> /servicesNS/-/-/saved/searches timeout=0 earliest_time=-24h@h latest_time=-0h@h search=""is_scheduled=1 AND disabled=0""
| eval savedsearch_type=if((NOT 'action'==""*"" AND NOT alert.track==""*"" AND NOT alert_condition==""*"" AND 'alert_type'==""always""),""report"",""alert"") 
| rename splunk_server As Splunk_Instance eai:acl.app As app title As savedsearch_name eai:acl.sharing As sharing 
| fields Splunk_Instance app author sharing savedsearch_name savedsearch_type description is_scheduled disabled search cron_schedule next_scheduled_time realtime_schedule schedule_priority schedule_window updated allow_skew 
| stats 
    Values(*) As *
    BY app savedsearch_name 
| eval updated = strptime(updated,""%Y-%m-%dT%H:%M:%S%z"") ,updated = strftime(updated, ""%A %B %d, %Y %I:%M:%S %p %Z"") , next_scheduled_time=strptime(next_scheduled_time,""%Y-%m-%d %H:%M:%S %Z""), next_scheduled_time = strftime(next_scheduled_time, ""%A %B %d, %Y %I:%M:%S %p %Z"") 
| table savedsearch_name updated disabled is_scheduled savedsearch_type app sharing author cron_schedule next_scheduled_time realtime_schedule schedule_priority schedule_window allow_skew description search",0
"GMC-065","ITSI Scheduled Jobs Cron Scheduler Pie Chart","itsi,health_assessments,rest","Scheduled_Jobs","Analyze the results of the search in Pie Chart visualizations and tune Job schedules to stagger appropriately.
Here is an example of staggering 5 minute interval jobs:
1-59/5 * * * *
2-59/5 * * * *
3-59/5 * * * *
4-59/5 * * * *

Rerun the search after you make the adjustments to see the new data and apply additional tunings as necessary.
For 1 minute interval jobs, reconfigure to run every 5minutes instead and stagger using the same process above.","| rest splunk_server=<Search Head Cluster Hosts> /servicesNS/-/-/saved/searches timeout=0 earliest_time=-24h@h latest_time=-0h@h search=""is_scheduled=1 AND disabled=0""
| rename splunk_server As Splunk_Instance eai:acl.app As app title As savedsearch_name eai:acl.sharing As sharing 
| stats 
    Values(cron_schedule) As cron_schedule
    BY app savedsearch_name 
| stats count by cron_schedule",0
"GMC-066","Run Time Across All ITSI Jobs Across the entire ITSI Cluster","itsi,health_assessments","Scheduled_Jobs","Review overall Run Time across all jobs to check if we have any unusual spikes in run times in the last 24 hours:","host=<Search Head Cluster Hosts> index=_internal sourcetype=scheduler status=completed earliest=-24h@h
| fields _time host app run_time savedsearch_name
| timechart Max(run_time) As MaxRunTime Avg(run_time) As AvgRunTime span=1h
| foreach MaxRunTime AvgRunTime [ eval <<FIELD>>=round('<<FIELD>>', 2)]",0
"GMC-067","Run Time Across All ITSI Jobs Across the entire ITSI Cluster by App & Job Name","itsi,health_assessments","Scheduled_Jobs","Check if we have any unusual long running jobs and then zoom in on a particular job to make sure the behavior in the last 24 hours:","host=<Search Head Cluster Hosts> index=_internal sourcetype=scheduler status=completed earliest=-24h@h
| fields _time host app run_time savedsearch_name
| stats Max(run_time) As MaxRunTime Avg(run_time) As AvgRunTime by app savedsearch_name
| foreach MaxRunTime AvgRunTime [ eval <<FIELD>>=round('<<FIELD>>', 2)]
| sort 0 - MaxRunTime",0
"GMC-068","Run Time Across All ITSI Jobs Across the entire ITSI Cluster(Specific Job Name)","itsi,health_assessments","Scheduled_Jobs","Select the Job with the highest MaxRunTime and review its run time overtime in the last 24 hours:","host=<Search Head Cluster Hosts> savedsearch_name=""Indicator - Shared - 5d772a556e65145ad9069c94 - ITSI Search"" index=_internal sourcetype=scheduler status=completed earliest=-24h@h latest=-0h@h
| fields _time host app run_time savedsearch_name 
| timechart Max(run_time) As MaxRunTime Avg(run_time) As AvgRunTime span=1h
| foreach MaxRunTime AvgRunTime [ eval <<FIELD>>=round('<<FIELD>>', 2)]",0
"GMC-069","Report on all Bundle enforcer blacklist by host & app","searchbundle,health_assessments","Configuration_Files","","index=`setup_summary_index` sourcetype=""splunk:config:btool:distsearch"" 
| rex ""etc/((apps|master-apps|slave-apps)/)?[^/]+/(default|local)/(?<file>\w+\.conf)\s+\[(?<stanza>.+?)\]"" 
| multikv noheader=t 
| rex ""(?<SPLUNK_HOME>.*?)/etc/(?<app_folder>apps|master-apps|system|slave-apps)/((?<app>.*)/)?(?<directory>default|local)/(?<file>\w+\.conf)"" 
| search stanza=bundleEnforcerBlacklist app_folder!=system Column_2=""*/*"" 
| stats count by host app stanza Column_2 
| rex field=Column_2 ""(?<name>.*)\s*=\s*(?<blacklist_pattern>.*)"" 
| table host app name blacklist_pattern",0
"GMC-070","Report on indexes and sourcetypes with no data coming in","forwarding,health_assessments",SPL,"Find Indexes and sourcetypes that hasn't been receiving data from the forwarding tier","| tstats latest(_time) as Latest by host sourcetype index 
| eval current=now() 
| eval Minimum_Age=round(((current-Latest)/60)/60,2) 
| rangemap field=Minimum_Age default=Critical Normal=0-0.5 Elevated=0.5-2 Warning=2-4 
| eval stIDX=tostring(index) + "" -- "" + tostring(sourcetype) 
| stats values(stIDX) as ""Index -- Sourcetype"" list(Latest) as ""Latest Event"" list(Minimum_Age) as Minimum_Age list(range) as Threshold by host 
| convert ctime(""Latest Event"") 
| eventstats avg(Minimum_Age) as average by host 
| eval average=round(average,2) 
| sort - average 
| rename Minimum_Age as ""Hours Since Last Communication"" average as ""Average Time in Hours Since Last Communication""",0
"GMC-071","Splunk .conf configuration syntax errors","config_files,health_assessments","Configuration_Files",".conf file syntax errors within the environment.  Recommended to review and correct syntax errors identified to ensure system is functioning as expected.","index=_internal sourcetype=splunkd CASE(WARN) TERM(IniFile) 
| rex ""IniFile - (?<file>[^,]+),(?<msg>.*$)"" 
| stats count dc(host) As Num_Hosts last(host) As Last_Host values(msg) As Error_Message by file",0
"GMC-072","Indexer connectivity issues","connectivity,health_assessments","Indexer_Cluster","cannot establish connection to given indexer.  As a result, this indexer has considerably less data compared to its peers, this misconfiguration results in performance issues.
Recommended to review routing/firewall configuration to ensure on-prem forwarders are able to connect to said indexer.","index=_internal sourcetype=splunkd component=TcpOutputProc TERM(WARN) ip=* connection timed out 
| rename ip As Indexer 
| stats Values(event_message) As event_message Values(host) As Hosts dc(host) As Num_Hosts Latest(_time) As _time by Indexer",0
"GMC-073","Analyze Scheduler using the Metrics component in _internal","itsi,health_assessments,skips",Scheduler,"Set the shcluster_label to the either the Search head Cluster or a standalone Search Head.
Then proceed and select a few metrics to report on from the list of fields.","index=_internal sourcetype=splunkd component=Metrics group=searchscheduler (search_group=dmc_group_search_head OR host=sh-i-*) earliest=-4h@h latest=-0h@h 
| `get_shcluster_label(host)` 
| search host=""*"" AND shcluster_label=""shcluster1"" 
| fields _time actions_triggered completed delayed delegated delegated_scheduled delegated_waiting dispatched eligible max_lag max_running max_runtime shc_max_hist_searches shc_max_rt_searches shc_max_sched_auto_summary shc_max_sched_hist_searches shc_max_sched_rt_searches skipped total_lag total_runtime window_max_lag window_total_lag 
| fields _time shc_* skipped 
| timechart Span=5m
    Max(*) As * 
| foreach * 
    [ eval <<FIELD>>=Round('<<FIELD>>', 2)]",0
"GMC-075","Index Buckets Statistics","buckets,index,health_assessments,rest","Indexer_Cluster","Report on the following:
1. Unusually high number of Buckets.
2. Indexes with Excess Bucket Copies
3. Indexes with Excess Searchable Copies","| rest /services/cluster/master/indexes `setup_cluster_master_rest` 
| stats last(splunk_server) as host max(index_size) as index_size max(num_buckets) as num_buckets max(total_excess_bucket_copies) as total_excess_bucket_copies max(total_excess_searchable_copies) as total_excess_searchable_copies by title 
| rename title as index_name 
| eval _time=now() 
| fields _time host index_name index_size num_buckets total_excess_bucket_copies total_excess_searchable_copies",0
"GMC-076","Analyze Search Bundle Size and Replication Elapsed Time","bundle,health_assessments","Search_Head_Cluster","Report unusual bundle size and elapsed times.","index=_internal sourcetype=splunkd component=DistributedBundleReplicationManager bundle_replication_mode=baseline earliest=-24h@h latest=now 
| `get_shcluster_label(host)` 
| fields _time shcluster_label bundle_file_name bundle_size elapsed_ms replication_id 
| eval bundle_size_org=bundle_size 
| rex field=bundle_size mode=sed ""s/KB|MB//g"" 
| eval bundle_size_MB=case(match(bundle_size_org, ""KB""), round(bundle_size/1024), match(bundle_size_org, ""MB""), round(bundle_size)) 
| eval elapsed_s=round(elapsed_ms/1000,2) 
| stats latest(*) as * latest(_time) As _time by shcluster_label replication_id 
| sort 0 - _time 
| table _time shcluster_label replication_id bundle_file_name bundle_size_MB elapsed_s 
| timechart Max(elapsed_s) As elapsed_s Max(bundle_size_MB) As bundle_size_MB by shcluster_label",0
"GMC-077","ITSI Rules Engine Real-Time Job Impact on the platform",itsi,"Scheduled_Jobs",,"index=_introspection sourcetype=splunk_resource_usage component=PerProcess data.search_props.label=itsi_event_grouping 
| rename data.* as * , search_props.* as * 
| fields _time host mem_used normalized_pct_cpu t_count read_mb written_mb fd_used elapsed page_faults pct_cpu pct_memory app delta_scan_count label mode pid ppid process process_type provenance role scan_count sid status type user 
| fields _time host mem_used normalized_pct_cpu fd_used t_count page_faults read_mb written_mb scan_count pid 
| timechart span=5m
    Max(mem_used) as mem_used 
    Max(normalized_pct_cpu) as normalized_pct_cpu
    Max(fd_used) as fd_used
    Max(t_count) as t_count
    Max(page_faults) as page_faults
    Max(read_mb) as read_mb
    Max(written_mb) as written_mb
    Max(scan_count) as scan_count",0
"GMC-078","Report on Job Disk Quota Size and Time To Live Information","ttl,health_assessments","Scheduled_Jobs","For more info on Dispatch folder:
https://docs.splunk.com/Documentation/Splunk/latest/Search/Dispatchdirectoryandsearchartifacts","index=`setup_summary_index` search_name=splunk_rest_search_jobs_sh_summary_tracker provenance=scheduler Search_State != Running label!=_ACCELERATE_* NOT app IN (splunk_archiver,*global_monitoring_console) 
| table shcluster_label search_id_normalized Search_State searchEarliestTime searchLatestTime pid app label ttl owner priority provenance delegate diskUsage doneProgress eventAvailableCount eventCount isEventsPreviewEnabled isPreviewEnabled isRemoteTimeline meanPreviewPeriod numPreviews resultCount resultIsStreaming resultPreviewCount runDuration sampleRatio sampleSeed scanCount searchCanBeEventType runtime_auto_cancel runtime_auto_pause error_messages 
| stats Max(diskUsage) As diskUsage latest(*) as * by shcluster_label label app owner ttl Search_State 
| eval diskUsageGB=round(diskUsage/1024/1024/1024,2) , ttl_hours=round(ttl/60/60,0), ttl_days=round(ttl/60/60/24,0) 
| `get_saved_searches_info(shcluster_label,app,label)` 
| `get_identity_info(owner)` 
| `cron_descriptor(cron_schedule)` 
| fields shcluster_label owner emp_name emp_last emp_email app label description actions dispatch_earliest_time dispatch_latest_time cron_schedule dispatch_ttl ttl ttl_hours ttl_days diskUsage diskUsageGB Search_State cron_schedule_described 
| makemv delim="","" actions 
| join actions type=left 
    [ search index=`setup_summary_index` search_name=splunk_rest_alert_actions_summary_data 
    | fields shcluster_label Splunk_Instance app disabled title label description sharing ttl updated priority 
    | rename title as actions, ttl As action_ttl description As action_description ] 
| rename ttl As final_ttl 
| sort 0 - diskUsage 
| table shcluster_label owner emp_name emp_last emp_email app label Search_State diskUsage diskUsageGB actions action_description dispatch_earliest_time dispatch_latest_time cron_schedule cron_schedule_described dispatch_ttl action_ttl final_ttl ttl_hours ttl_days description",0
"GMC-027B","Lookup or Summary Indexing Generating Jobs outputting to lookups and may not end with stats count B","","Scheduled_Jobs","Adjust the heads command at the bottom to show the worst ones generating the most results however any job with stats count at the end should be fixed","index=`setup_summary_index` search_name=splunk_internal_scheduler_jobs_idx_summary_tracker earliest=-24h@h latest=-0h@h 
| `get_shcluster_label(Splunk_Instance)` 
| `get_saved_searches_info(shcluster_label,App,Savedsearch_Name)` 
| search savedsearch_search IN (""*|*outputlookup *"", ""*|*collect *"") savedsearch_search!=""*|*stats count"" 
| stats Max(Run_Time) as Max_Run_Time Avg(Run_Time) as Avg_Run_Time Min(Run_Time) As Min_Run_Time Min(Result_Count) As Min_Result_Count Max(Result_Count) As Max_Result_Count count As Num_Executions Latest(savedsearch_search) As savedsearch_search by shcluster_label App User Savedsearch_Name 
| eval Total_Max_Run_Time = Num_Executions * Max_Run_Time, Max_Run_Time_Human = Max_Run_Time, Total_Max_Run_Time_Human = Total_Max_Run_Time 
| `gmc_convert_runtime(Max_Run_Time_Human)` 
| `gmc_convert_runtime(Total_Max_Run_Time_Human)` 
| table shcluster_label App User Savedsearch_Name Min_Run_Time Avg_Run_Time Max_Run_Time Max_Run_Time_Human Min_Result_Count Max_Result_Count Num_Executions Total_Max_Run_Time Total_Max_Run_Time_Human savedsearch_search 
| eval Savedsearch_Name_encoded = Savedsearch_Name 
| rex field=Savedsearch_Name_encoded mode=sed ""s:%:%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g      s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| eval Job_Uri = ""https://<stack>.splunkcloud.com/en-US/manager/"" . App . ""/saved/searches?app="" . App. ""&count=10&offset=0&itemType=&owner="" . User . ""&search="" . Savedsearch_Name_encoded 
| table shcluster_label App User Savedsearch_Name Min_Run_Time Avg_Run_Time Max_Run_Time Max_Run_Time_Human Min_Result_Count Max_Result_Count Num_Executions Total_Max_Run_Time Total_Max_Run_Time_Human Job_Uri savedsearch_search 
| sort 0 - Max_Result_Count",0
"GMC-080","REST: Get indexer status information","rest,introspection,indexer","Indexer_Cluster","  average_KBps: average_KBps Average indexer throughput (kbps)
  reason:Status explanation. For a normal status, returns. The following examples show possible abnormal status reasons. idx=<indexerName> Throttling indexer, too many tsidx files in bucket=<bucketName>. Is splunk-optimize working? If not, low disk space may be the cause."" ""You are low in disk space on partition <partitionName>. Indexing is paused. Will resume when free disk space rises above <minFreeMB>
  status: Current indexer status. One of the following values: normal, throttled, stopped","| rest /services/server/introspection/indexer `setup_group_indexer_rest` Timeout=0 
    `gmc_comment(""Get indexer status information"")`
    `gmc_comment(""average_KBps: average_KBps Average indexer throughput (kbps)"")` 
    `gmc_comment(""reason:Status explanation. For a normal status, returns. The following examples show possible abnormal status reasons. idx=<indexerName> Throttling indexer, too many tsidx files in bucket=<bucketName>. Is splunk-optimize working? If not, low disk space may be the cause."" ""You are low in disk space on partition <partitionName>. Indexing is paused. Will resume when free disk space rises above <minFreeMB>"")` 
    `gmc_comment(""status: Current indexer status. One of the following values: normal, throttled, stopped"")` 
| table splunk_server title status average_KBps reason",0
"GMC-100","REST: Lists Search Head Cluster artifacts and replicas","rest,shc,artifacts","Search_Head_Cluster","Provides list of artifacts and replicas currently managed by the captain across a searchhead cluster.
This endpoint can only be accessed on the captain. The response lists all artifacts that are currently resident on the set of search head cluster members.
An artifact in search head clustering is a managed search directory. Currently, only scheduled search results directories are managed and replicated according to replication policy.
Note: Ad hoc searches are not considered artifacts and are not listed.

artifact_size: Artifact size, in bytes.

origin_guid: Guid of the origin peer where this artifact was created/search was run.

peers: Lists information about replicas of this artifact on members of this searchhead cluster.

service_after_time: Artifact service/fixup is deferred until after this time.

Request parameters
remote_sids: Required. Set this to true to return the searches that the captain is seeing. Will include adhoc searches on remote members.","| rest /services/shcluster/captain/artifacts remote_sids=1 splunk_server=SHC3 timeout=0 
| rename ""eai:acl.*"" as ""*"", ""eai:*"" as ""*"" 
| fields splunk_server app user title label service_after_time artifact_size",0
"GMC-103","REST: Lists the Search Head Cluster members","rest,shc","Search_Head_Cluster","https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTcluster#shcluster.2Fcaptain.2Fmembers","| rest /services/shcluster/captain/members splunk_server=SHC3 timeout=0 
| rename ""eai:acl.*"" as ""*"", ""eai:*"" as ""*"" 
| table splunk_server site title label adhoc_searchhead advertise_restart_required artifact_count delayed_artifacts_to_discard fixup_set host_port_pair is_captain kv_store_host_port last_heartbeat mgmt_uri no_artifact_replications peer_scheme_host_port pending_job_count preferred_captain replication_count replication_port replication_use_ssl status status_counter.Complete status_counter.NonStreamingTarget status_counter.PendingDiscard",0
"GMC-104","REST: List Search Head Cluster members artifact configuration","rest,shc,artifacts","Search_Head_Cluster","https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTcluster#shcluster.2Fmember.2Fartifacts","| rest /services/shcluster/member/artifacts splunk_server=SHC* timeout=0 
| rename eai:acl.* as *, eai:* as * 
| table splunk_server user app label status title artifact_size",0
"GMC-105","REST: Get information about dispatched search jobs","rest,shc,artifacts","Search_Head_Cluster","https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTintrospect#server.2Fstatus.2Fdispatch-artifacts","| rest /services/server/status/dispatch-artifacts splunk_server=SHC1 timeout=0 
| rename ""eai:acl.*"" as ""*"", ""eai:*"" as ""*"" 
| fields splunk_server title  count_realtime count_scheduled count_summary disk_usage_MB total_count",0
"GMC-101","REST: Get information about the search knowledge bundle replication","rest,shc,searchbundle","Search_Head_Cluster","https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTintrospect#server.2Fintrospection.2Fsearch.2Fdistributed

Get information about the search knowledge bundle replication, if the current instance is the search head. Provides details about maximum and average time to execute routine distributed search methods, including peer info, peer bundles list, and authentication token requests from search heads.","| rest /services/server/introspection/search/distributed splunk_server=SHC3 
| table splunk_server title average_baseline_file_size average_baseline_msecs average_bytes average_delta_file_size average_delta_msecs average_msecs baseline_count bundle_directory_size bundle_file_count delta_count get_auth_max_ms get_auth_mean_ms get_bundleList_max_ms get_bundleList_mean_ms get_serverInfo_max_ms get_serverInfo_mean_ms health_check_failures health_status peer_count",0
"GMC-102","REST: Provides vital statistics for distributed search framework, including details on search peer performance","rest,shc,dispatch","Search_Head_Cluster","https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTintrospect#server.2Fintrospection.2Fsearch.2Fdispatch","| rest /services/server/introspection/search/dispatch splunk_server=SHC*
| table splunk_server title Bundle_Directory_Reaper_Average_Time(ms) Bundle_Directory_Reaper_Max_Time(ms) Compute_User_Search_Quota_Average_Time(ms) Compute_User_Search_Quota_Max_Time(ms) Dispatch_Directory_Reaper_Average_Time(ms) Dispatch_Directory_Reaper_Max_Time(ms) Search_StartUp_Time_Average_Time(ms) Search_StartUp_Time_Max_Time(ms)",0
"GMC-079","Find Top 20 most used Dashboards on the SHC by App & Dashboard Name","adoption,usage",Dashboards,"","index=`setup_summary_index` search_name=splunk_internal_web_access_idx_summary_tracker 
| fields _time User Splunk_Instance App OS_Name avg_spent Dashboard_Name Latest_Access 
| `get_shcluster_label(Splunk_Instance)` 
| `get_dashboards_info(shcluster_label,App,Dashboard_Name)` 
| `get_identity_info(author)` 
| search shcluster_label=""*"" NOT Dashboard_Name IN (welcome*,overview) NOT App IN (dmc) 
| stats
    dc(User) As Num_Users count as num_hits latest(label) As Dashboard_Display_Name latest(author) As Owner latest(emp_name) As emp_name by App Dashboard_Name 
| sort 0 - num_hits 
| head 20 
| table Dashboard_Name Dashboard_Display_Name App Owner emp_name num_hits Num_Users",0
"GMC-081","Reports Looking back over 90 Days - Scheduled",gt90days,"Scheduled_Jobs","","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE
    `tstats_gmc_audit`
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    index=_audit
    AND Search_Activity.Audit_Search.search_type1 = ""scheduled""
    AND Search_Activity.Audit_Search.info1 = ""*""
    AND Search_Activity.Audit_Search.user1 = ""*""
    AND Search_Activity.Audit_Search.savedsearch_name1 = ""*""
    AND host = ""*""
    NOT Search_Activity.Audit_Search.savedsearch_name1 IN (""*global_monitoring_console*"", ""splunk_*_dashboard*"", ""splunk_*_*_lookup_*"", ""splunk_*_*_audit_search_activity_*"")
    BY _time Search_Activity.Audit_Search.search_id1 Search_Activity.Audit_Search.info1 
| `gmc_drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename info1 AS info, search_id1 As search_id 
| fields _time host available_count considered_events datamodel decompressed_slices event_count dispatch_time exec_time info result_count roles savedsearch_name scan_count search search_et search_id search_lt search_startup_time search_type searched_buckets total_run_time total_slices user 
| eval Time_Diff=round((search_lt-search_et)/60/60/24,0) 
| where Time_Diff>=90 
| `gmc_convert_runtime('total_run_time')` 
| `get_normalized_search_id(search_id)` 
| `get_lookup_reference(search)` 
| `get_macro_reference(search)` 
| `get_datamodel_reference(search)` 
| `get_index_reference(search)` 
| `get_sourcetype_reference(search)` 
| `get_source_reference(search)` 
| `get_eventtype_reference(search)` 
| `get_rest_reference(search)` 
| `get_instance_info(host)` 
| `get_shcluster_label(host)` 
| `get_identity_info(user)` 
| `fillnull_identity_info` 
| `get_saved_searches_info(shcluster_label,savedsearch_name)` 
| `cron_descriptor(cron_schedule)` 
| eval search_et=if(search_et=""0"", ""All-Time"", search_et) 
| fillnull value=""no-search_id"" search_id_normalized 
| fillnull value=0 available_count considered_events decompressed_slices event_count result_count scan_count searched_buckets total_run_time total_slices 
| fillnull value=""Undefined"" shcluster_label datamodel savedsearch_type search cron_schedule_described service_title kpi_title auth_type 
| fillnull value=""Undefined"" 
| search shcluster_label=""*"" search_id_normalized=""***"" datamodel=""*"" search=""*"" total_run_time>-1 searched_buckets>-1 Splunk_Roles=""*"" Lookup_Reference=""***"" savedsearch_type=""*"" Macro_Reference=""***"" Datamodel_Reference=""***"" Index_Reference=""***"" Sourcetype_Reference=""***"" Source_Reference=""***"" Eventtype_Reference=""***"" Rest_Reference=""***"" cron_schedule_described=""*"" service_title=""*"" kpi_title=""*"" emp_name=""*"" emp_title=""*"" emp_type=""*"" emp_status=""*"" emp_dep=""*"" emp_lob1=""*"" emp_cc=""*"" emp_city=""*"" emp_region1=""*"" emp_country=""*"" auth_type=""*"" result_count<999999999 search_lt=""*"" search_et=""*"" 
| fields  shcluster_label  app  savedsearch_name savedsearch_type service_title kpi_title dispatch_time total_run_time 'total_run_time' _time search_et search_lt Time_Diff user emp_name emp_email emp_first emp_last emp_dep emp_lob1 emp_title emp_city emp_region1 emp_country datamodel scan_count event_count result_count available_count considered_events searched_buckets total_slices decompressed_slices cron_schedule cron_schedule_described schedule_priority max_concurrent schedule_window allow_skew search 
| `normalize_search_status` 
| `normalize_search_type(search_type)` 
| eval search_lt = strftime(search_lt, ""%A %B %d, %Y %I:%M:%S %p %Z"") 
| eval search_et = strftime(search_et, ""%A %B %d, %Y %I:%M:%S %p %Z"")
| bin _time span=1d 
| stats count values(*) as * values(_time) as Time by shcluster_label  app savedsearch_name
| eval Time = strftime(Time, ""%A %B %d, %Y %I:%M:%S %p %Z"")
| fields count shcluster_label  app  savedsearch_name Time search_et search_lt Time_Diff cron_schedule user emp_email emp_dep emp_title search",0
"GMC-082","Index Queries From Dashboards or Views possibly over 90 days",gt90days,Dashboards,"","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup
| fields _time,shcluster_label,Splunk_Instance,Splunk_Roles,app,sharing,title,label,description,type,dashboard_size,author,location,updated,time_range,Report_Reference,Dashboard_Reference,Lookup_Reference,Datamodel_Reference,Macro_Reference,Index_Reference,Sourcetype_Reference,Source_Reference,Eventtype_Reference,Rest_Reference,search,BaseSearch,acl_perms_read,acl_perms_write,panel_title,num_panels,auto_refresh_delay,submitButton,autoRun 
| search NOT app IN (splunk_monitoring_console, splunk_archiver, simple_xml_examples, *global_monitoring_console,gmc*) NOT title IN (setup,home,about,settings,contents,help) 
| eval url=""http://"" . shcluster_label . "":8000/en-US/app/"" . app . ""/"" . title 
| `get_instance_roles(Splunk_Instance)` 
| `get_dashboards_usage(shcluster_label,app,title)` 
| `get_identity_info(shcluster_label,author)` 
| `get_identity_info(author)` 
| `fillnull_identity_info` 
| `get_macro_definition(shcluster_label,Macro_Reference)` 
| `strftime_format(updated)` 
| eval Num_Searches=mvcount(search) 
| eval Effective_autoRun = if(autoRun=""false"" AND submitButton=""true"" AND is_time_selector=""true"", ""false"", ""true"" ) 
| fillnull value=""false"" autoRun 
| fillnull value=""none"" submitButton acl_perms_read acl_perms_write auto_refresh_delay panel_title 
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch time_range Macro_Definition 
| search Index_Reference=""*"" 
| eval Num_Searches=mvcount(search) 
| rex field=time_range ""(?<time_range_epoch>\d{10})""
| rex field=time_range mode=sed ""s/\d{10}//"" 
| rex field=time_range mode=sed ""s/^-[0-9]d$|-[0-9]+h@h|-[0-9]+h@m|-[0-9]+m@m|-[0-9]+m$|-[0-9]+h$//""
| rex field=time_range mode=sed ""s/^-1d$|-7d@h$|^-30d@d$|^-30d$|^-[0-9]+s$|^@d$|-[0-9]+$//""
| eval time_range_epoch=min(time_range_epoch)
| eval time_diff = now() - time_range_epoch
| eval e_time_diff_in_days = round(time_diff/60/60/24,0)
| eval e_time_diff_in_days = if(e_time_diff_in_days>0,""-"" . e_time_diff_in_days . ""d*"",e_time_diff_in_days)
| eval time_range = mvappend(time_range,e_time_diff_in_days)
| where  mvfind(time_range,""d"")>0
| sort time_range
| table title time_range label shcluster_label Index_Reference app sharing acl_perms_read acl_perms_write author updated emp_name num_panels Num_Searches Effective_autoRun auto_refresh_delay submitButton autoRun  Latest_Access dashboard_size Report_Reference Dashboard_Reference Eventtype_Reference description  search",0
"GMC-083","Ad-hoc Searches looking back over 90 Days",gt90days,Other,"","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE
    `tstats_gmc_audit`
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    index=_audit
    AND Search_Activity.Audit_Search.search_type1 = ""adhoc""
    AND Search_Activity.Audit_Search.info1 = ""*""
    AND Search_Activity.Audit_Search.user1 = ""*""
    AND Search_Activity.Audit_Search.savedsearch_name1 = ""*""
    AND host = ""*""
    NOT Search_Activity.Audit_Search.savedsearch_name1 IN (""*global_monitoring_console*"", ""splunk_*_dashboard*"", ""splunk_*_*_lookup_*"", ""splunk_*_*_audit_search_activity_*"")
    BY _time Search_Activity.Audit_Search.search_id1 Search_Activity.Audit_Search.info1 
| `gmc_drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename info1 AS info, search_id1 As search_id 
| fields _time host available_count considered_events datamodel decompressed_slices event_count dispatch_time exec_time info result_count roles savedsearch_name scan_count search search_et search_id search_lt search_startup_time search_type searched_buckets total_run_time total_slices user 
| eval Time_Diff=round((search_lt-search_et)/60/60/24,2) 
| where Time_Diff>=90 
| `gmc_convert_runtime('total_run_time')` 
| `get_normalized_search_id(search_id)` 
| `get_lookup_reference(search)` 
| `get_macro_reference(search)` 
| `get_datamodel_reference(search)` 
| `get_index_reference(search)` 
| `get_sourcetype_reference(search)` 
| `get_source_reference(search)` 
| `get_eventtype_reference(search)` 
| `get_rest_reference(search)` 
| `get_instance_info(host)` 
| `get_shcluster_label(host)` 
| `get_identity_info(user)` 
| `fillnull_identity_info` 
| `get_saved_searches_info(shcluster_label,savedsearch_name)` 
| `cron_descriptor(cron_schedule)` 
| eval search_et=if(search_et=""0"", ""All-Time"", search_et) 
| fillnull value=""no-search_id"" search_id_normalized 
| fillnull value=0 available_count considered_events decompressed_slices event_count result_count scan_count searched_buckets total_run_time total_slices 
| fillnull value=""Undefined"" shcluster_label datamodel savedsearch_type search cron_schedule_described service_title kpi_title auth_type 
| fillnull value=""Undefined"" 
| search shcluster_label=""*"" search_id_normalized=""***"" datamodel=""*"" search=""*"" total_run_time>-1 searched_buckets>-1 Splunk_Roles=""*"" Lookup_Reference=""***"" savedsearch_type=""*"" Macro_Reference=""***"" Datamodel_Reference=""***"" Index_Reference=""***"" Sourcetype_Reference=""***"" Source_Reference=""***"" Eventtype_Reference=""***"" Rest_Reference=""***"" cron_schedule_described=""*"" service_title=""*"" kpi_title=""*"" emp_name=""*"" emp_title=""*"" emp_type=""*"" emp_status=""*"" emp_dep=""*"" emp_lob1=""*"" emp_cc=""*"" emp_city=""*"" emp_region1=""*"" emp_country=""*"" auth_type=""*"" result_count<999999999 search_lt=""*"" search_et=""*"" 
| eval Time_Diff=round(Time_Diff,0)
| fields shcluster_label search_type info  _time search_et search_lt Time_Diff user emp_email emp_first emp_last emp_dep emp_title search 
| `normalize_search_status` 
| `normalize_search_type(search_type)` 
| eval search_lt = strftime(search_lt, ""%A %B %d, %Y %I:%M:%S %p %Z"") 
| eval search_et = strftime(search_et, ""%A %B %d, %Y %I:%M:%S %p %Z"")
| bin _time span=1d
| stats count values(*) as * by _time user search
| fields count shcluster_label search_type info  _time search_et search_lt Time_Diff user emp_email emp_first emp_last emp_dep emp_title search",0
"GMC-084","Generate Email's for Terminated Users","terminated,sendresults","Scheduled_Jobs","DRAFT search to generate emails","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup
| search NOT app IN (splunk_*,Splunk*,alert_webhook,appsbrowser,cisco-app-ACI,SA-nix,chargeback)
| `get_instance_roles(Splunk_Instance)`
| `get_saved_searches_usage(shcluster_label, app, savedsearch_name)` 
| `get_identity_info(author)` | `fillnull_identity_info`
| `cron_descriptor(cron_schedule)`
| eval days_last_updated=round((now() - updated) / 86400 , 0)
| `strftime_format(scheduled_times)` | `strftime_format(next_scheduled_time)`
| `normalize_earliest_time(dispatch_earliest_time)` | `get_premium_app_job_class`
| fillnull value=0 action_summary_index correlationsearch_enabled dispatch_auto_cancel updated populate_lookup
| `strftime_format(updated)`
| fillnull value=""no-actions"" actions
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app savedsearch_name description savedsearch_type sharing author owner disabled is_visible is_scheduled cron_schedule scheduled_times next_scheduled_time auto_summarize dispatch_earliest_time dispatch_latest_time schedule_priority max_concurrent schedule_window allow_skew realtime_schedule alert_severity alert_track alert_type removable run_n_times run_on_startup savedsearch_search action_email_to cron_schedule_described action_summary_index_name populate_lookup_dest summary_index_name Index_Reference

| search shcluster_label=""*"" Splunk_Roles=""*"" Splunk_Instance=""*"" app=""*"" author=* sharing=user disabled=""*"" is_scheduled=""*"" savedsearch_name=""*"" savedsearch_type=""*"" savedsearch_search=""*"" Lookup_Reference=""*"" Datamodel_Reference=""*"" Macro_Reference=""*"" Index_Reference=""*"" Sourcetype_Reference=""*"" Source_Reference=""*"" Eventtype_Reference=""*"" Rest_Reference=""*"" action_email_to=""*"" auto_summarize=""*"" schedule_priority=""*"" schedule_window=""*"" realtime_schedule=""*"" run_on_startup=""*"" allow_skew=""*"" correlationsearch_enabled=""*"" action_summary_index=""*"" cron_schedule_described=""*"" emp_name=""*"" emp_type=""*"" emp_status=""Terminated"" emp_dep=""*"" emp_lob1=""*"" emp_cc=""*"" emp_title=""*"" emp_city=""*"" emp_region1=""*"" emp_country=""*"" dispatch_earliest_time_normalized=""*"" dispatch_latest_time=""*"" actions=""*"" dispatch_auto_cancel=""*"" updated=""*"" populate_lookup=""*"" populate_lookup_dest=""*"" summary_index_name=""*"" days_last_updated >= 90 |  table shcluster_label app sharing savedsearch_name savedsearch_type disabled is_scheduled updated days_last_updated description dispatch_earliest_time_normalized dispatch_latest_time cron_schedule cron_schedule_described scheduled_times next_scheduled_time realtime_schedule schedule_priority schedule_window allow_skew author emp_name emp_title emp_dep emp_lob1 emp_type emp_status run_n_times run_on_startup max_concurrent is_visible acl_perms_read acl_perms_write email email_to populate_lookup_dest script summary_index summary_index_name webhook webhook_param_url actions auto_summarize auto_summarize_cron_schedule dispatch_ttl dispatch_as logevent logevent_param_event lookup lookup_append lookup_filename script_filename 
| eval scheduled_times=mvindex(scheduled_times,0,0)
| search author=""n0003024""
| eval previous_employee=author
| eval previous_employee_name=emp_name
| `get_identity_info(author)`
| eval emp_manager=""n0211745"" `gmc_comment(""Force manager to be acceptable test user, not a C level exec"")`
| fields previous_employee previous_employee_name shcluster_label app sharing actions savedsearch_name savedsearch_type updated description  acl_* email* emp_manager
| `get_identity_info(emp_manager)`
| rename emp_email as email_to, emp_first as manager_first
| fields previous_employee previous_employee_name shcluster_label app sharing actions savedsearch_name savedsearch_type updated description  acl_* email* email_to manager_first
| eval email_body=""Hello "" . manager_first . "", Some employes who recently seperated from the firm have Splunk Objects that we are scheduling to delete in the near future. If you want to keep these items please take ownership of them. If you do nothing they will be deleted per corperate policy.""
| fields - manager_first _time
| sendresults showemail=0 showbody=0 showresults=1  subject=""Excess Employee Splunk Knowledge Objects""  msgstyle=""table {font-family:Arial;font-size:12px;border: 1px solid black;padding:3px}th {background-color:#AAAAAA;color:#fff;border-left: solid 1px #e9e9e9} td {border:solid 1px #e9e9e9}""",0
"GMC-085","Search Bundle Objects by Size","bundle,search,user","Search_Head_Cluster","Raw data suitable for stats to identify user objects in the search bundle and the size of those objects.","index=_internal source=*2020_10_19_bundle_extraction.log earliest=-7d@d 
| `get_cluster_label(host)` 
| rex field=file_name ""apps/(?<app1>.*?)/"" 
| rex field=file_name ""users/\w+/(?<app2>.*?)/"" 
| rex field=file_name ""users/(?<user>.*?)/"" 
| eval app=coalesce(app1,app2) 
| `gmc_byte2human(file_size,3)` 
| `get_identity_info(user)` 
| table cluster_label app user emp_email file_name file_size file_size_MB 
| sort 0 - file_size",0
"GMC-086","Report on all errors found in job activity by cluster",errors,Health,"","index=`setup_summary_index` search_name=splunk_rest_search_jobs_sh_summary_tracker error_messages!=""Search auto-canceled"" provenance!=""UI:Search"" 
| makemv delim=""#####"" error_messages 
| rex field=error_messages ""Error in\s\'(?<command>.*?)\'"" 
| rex field=error_messages ""Could not load lookup=(?<lookup>.*)"" 
| eval error_messages_normalized = case ( 
    match(error_messages, ""Error in '\w+'""), command . "" Command Errors"",
    match(error_messages, ""REST endpoint""), ""REST Command Errors"",
    match(error_messages, "" Could not load lookup=""), ""Error in loading lookup table:"" . lookup,
    true(), error_messages) 
| stats 
    Values(error_messages) As error_messages
    Values(label) As label
    Values(app) As Apps
    Dc(label) As Num_Jobs
    Dc(error_messages) As Num_error_messages
    count
    by provenance shcluster_label owner error_messages_normalized 
| sort - count",0
"GMC-087","Find all Scheduled Jobs running < 5min interval","frequent,ttl","Scheduled_Jobs","Any of these jobs should have their TTL manually to 2p.","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| where is_scheduled=""1"" AND disabled=""0"" 
| `cron_descriptor(cron_schedule)` 
| search cron_schedule_described IN (""Every 1 minutes*"",""Every 2 minutes*"", ""Every 3 minutes*"",""Every 4 minutes*"", ""Every minute*"") 
| table shcluster_label app savedsearch_name cron_schedule cron_schedule_described dispatch_ttl",0
"GMC-088","Find Reports and Dashboards that need the dispatch.ttl value updated",automation,"Scheduled_Jobs","to use curl add these lines to the end of the search:
| rex field=rest_uri mode=sed ""s/^.*?:8089/https:\/\/localhost:8089/g""
| eval curl_command=""curl -k --netrc-file ~/.netrc -X POST "" . rest_uri . "" -H \""Content-Type:application/json\"" -d dispatch.ttl=3p""
| fields savedsearch_name curl_command
| stats count by curl_command
| fields curl_command","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    `gmc_comment(""Scheduled Searches that happen in the 1-4 minute range"")` 
| where is_scheduled=""1"" AND disabled=""0"" 
| `cron_descriptor(cron_schedule)` 
| search cron_schedule_described IN (""Every 1 minutes*"",""Every 2 minutes*"", ""Every 3 minutes*"",""Every 4 minutes*"", ""Every minute*"") 
| eval an_event_type=if(isnull(an_event_type),""Scheduled"",NULL) 
| fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule dispatch_ttl an_event_type 
| append 
    [ from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
        `gmc_comment(""All Scheduled reports that appear in any dashboard"")` 
    | mvexpand Report_Reference 
    | where isnotnull(Report_Reference) AND Report_Reference!=""no-report-reference"" 
    | `get_saved_searches_info(shcluster_label,app,Report_Reference)` 
    | rename Report_Reference As savedsearch_name 
    | eval an_event_type=if(isnull(an_event_type),""Report"",NULL) 
    | fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule dispatch_ttl an_event_type 
    | where is_scheduled=1] 
| append 
    [| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
    | fields _time,shcluster_label,Splunk_Instance,Splunk_Roles,app,sharing,title,label,description,type,dashboard_size,author,location,updated,time_range,Report_Reference,Dashboard_Reference,Lookup_Reference,Datamodel_Reference,Macro_Reference,Index_Reference,Sourcetype_Reference,Source_Reference,Eventtype_Reference,Rest_Reference,search,BaseSearch,acl_perms_read,acl_perms_write,panel_title,num_panels,auto_refresh_delay,submitButton,autoRun 
    | search NOT app IN (splunk_monitoring_console, splunk_archiver, simple_xml_examples, *global_monitoring_console,gmc*) NOT title IN (setup,home,about,settings,contents,help) 
    | eval url=""http://"" . shcluster_label . "":8000/en-US/app/"" . app . ""/"" . title 
    | `get_instance_roles(Splunk_Instance)` 
    | `get_dashboards_usage(shcluster_label,app,title)` 
    | `get_identity_info(shcluster_label,author)` 
    | `get_identity_info(author)` 
    | `fillnull_identity_info` 
    | `get_macro_definition(shcluster_label,Macro_Reference)` 
    | `strftime_format(updated)` 
    | eval Num_Searches=mvcount(search) 
    | eval Effective_autoRun = if(autoRun=""false"" AND submitButton=""true"" AND is_time_selector=""true"", ""false"", ""true"" ) 
    | fillnull value=""false"" autoRun 
    | fillnull value=""none"" submitButton acl_perms_read acl_perms_write auto_refresh_delay panel_title 
    | fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch time_range Macro_Definition 
    | search dashboard_size <= 10000000 Days_Latest_Access <= ""90"" Days_Latest_Access >= ""0"" num_panels >= -1 Num_Searches >= -1 
    | `get_latest_access_human(Days_Latest_Access)` 
    | eval Num_Searches=mvcount(search) 
    | mvexpand Report_Reference 
    | where isnotnull(Report_Reference) AND Report_Reference!=""no-report-reference"" 
    | `get_saved_searches_info(shcluster_label,app,Report_Reference)` 
    | rename Report_Reference As savedsearch_name 
    | eval an_event_type=if(isnull(an_event_type),""Dash Called"",NULL) 
    | fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule cron_schedule_described dispatch_ttl an_event_type] 
| `cron_descriptor(cron_schedule)` 
| fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule cron_schedule_described dispatch_ttl an_event_type 
| where dispatch_ttl!=""3p"" AND shcluster_label=""*""
    `gmc_comment(""Temporary holding value of 3p being used in place of 2p while we wait to change the default"")` 
| eval fudge_author = case ( 'sharing' == ""user"", 'author', 'sharing' == ""global"", ""nobody"", 'sharing' == ""app"", ""nobody"") 
| eval shcluster_label_rest=if(shcluster_label=""stack-shc"", ""stack"", shcluster_label . ""-stack"") 
| eval encoded_title=savedsearch_name 
| rex field=encoded_title mode=sed ""s:%:%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g
     s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| eval rest_uri=""https://"" . shcluster_label_rest . "".splunkcloud.com:8089/servicesNS/"" . fudge_author . ""/"" . app . ""/saved/searches/"" . encoded_title 
| rename _time As KV_Store_Update , updated As Job_Update_Time 
| `strftime_format(Job_Update_Time)` 
| `strftime_format(KV_Store_Update)` 
| `get_identity_info(author)` 
| eval curl_options=""-d dispatch.ttl=2p"" 
| fields KV_Store_Update Job_Update_Time shcluster_label app savedsearch_name sharing author emp_first emp_last emp_email is_scheduled cron_schedule_described dispatch_ttl rest_uri curl_options an_event_type 
| table savedsearch_name rest_uri curl_options",0
"GMC-089","Report on All Deployment Servers",deployments,"Knowledge_Objects",,"index=_internal host IN (vxkip-hmrsds01 vxkip-hmrsds02 vxkip-hmrsds03 vxpip-hhspds01 vxpip-hmrsds01 vxpip-hmrsds02 vxpip-hmrsds03 vxpip-hmrsds04 vxrip-hmrsds01 vxrip-hmrsds02) ""Splunk_TA_windows"" 
| stats values(app) As Apps by host sc 
| rename sc as DS_Server_Class 
| fields _time host DS_Server_Class Apps 
| join host DS_Server_Class 
    [ search index=_internal component=ClientSessionsManager host IN (vxkip-hmrsds01 vxkip-hmrsds02 vxkip-hmrsds03 vxpip-hhspds01 vxpip-hmrsds01 vxpip-hmrsds02 vxpip-hmrsds03 vxpip-hmrsds04 vxrip-hmrsds01 vxrip-hmrsds02) 
    | stats dc(ip) as Num_Clients values(name) As DS_Apps by host sc 
    | rename sc as DS_Server_Class 
    | fields _time host DS_Server_Class Num_Clients ] 
| table host DS_Server_Class Num_Clients Apps",0
"GMC-090","URI Creation","automation,uri","Scheduled_Jobs","Draft format to create URI from variable values.","| makeresults 
| eval author=""dfreed"", sharing=""app"", shcluster_label=""*"", app=""search"", savedsearch_name=""Testing Report for Z to Change Settings on."" 
| eval fudge_author = case (
    'sharing' == ""user"", 'author',
    'sharing' == ""global"", ""nobody"",
    'sharing' == ""app"", ""nobody"") 
| eval shcluster_label_rest=if(shcluster_label=""stack-shc"", ""stack"", shcluster_label . ""-stack"") 
| eval job_url=""https://"" . shcluster_label_rest . "".splunkcloud.com/app/"" . app . ""/search?s="" . savedsearch_name 
| eval encoded_title=savedsearch_name 
| eval job_property=case(""disable""=""disable"" OR ""disable""=""enable"", ""/disable"", true(), """") 
| eval curl_options=case(!match(""disable"", ""delete|disable|enable""), ""-d disable=\""1\"""", true(), """") 
| rex field=encoded_title mode=sed
    ""s:%:%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g
     s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| eval rest_uri=""\""https://"" . shcluster_label_rest . "".splunkcloud.com:8089/servicesNS/"" . fudge_author . ""/"" . app . ""/saved/searches/"" . encoded_title . ""\"""" 
| eval Request=case(""disable""=""delete"", ""DELETE"", true(), ""POST"") 
| table savedsearch_name app sharing author Request rest_uri",0
"GMC-091","Report on all Scheduled Jobs that drifted from the factory default","drifted,automation","Scheduled_Jobs","These are scheduled jobs that got their scheduling properties such as schedule window set incorrectly.","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| search shcluster_label=* disabled=0 is_scheduled=1 schedule_window=0 
| append 
    [| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    | search shcluster_label=* disabled=0 is_scheduled=1 allow_skew=0 ] 
| append 
    [| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    | search shcluster_label=* disabled=0 is_scheduled=1 sharing=user (allow_skew!=5m OR schedule_window!=auto OR schedule_priority!=default) ] 
| append 
    [| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    | search shcluster_label=* realtime_schedule!=1 ] 
| fields shcluster_label app author sharing savedsearch_name cron_schedule schedule_window allow_skew schedule_priority realtime_schedule is_scheduled disabled 
| search NOT savedsearch_name IN (_ScheduledView__*) 
| stats last(*) as * by shcluster_label savedsearch_name 
| fields shcluster_label app author sharing savedsearch_name cron_schedule is_scheduled disabled schedule_window allow_skew schedule_priority realtime_schedule


| eval schedule_window=""auto"", allow_skew=""5m"", schedule_priority=""default"", realtime_schedule=""1"" 
| fields shcluster_label app author sharing savedsearch_name cron_schedule is_scheduled disabled schedule_window allow_skew schedule_priority realtime_schedule 
| eval fudge_author = case ( 'sharing' == ""user"", 'author', 'sharing' == ""global"", ""nobody"", 'sharing' == ""app"", ""nobody"") 
| eval shcluster_label_rest=if(shcluster_label=""stack-shc"", ""stack"", shcluster_label . ""-stack"") 
| eval encoded_title=savedsearch_name 
| rex field=encoded_title mode=sed ""s:%:%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g
     s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| eval rest_uri=""https://"" . shcluster_label_rest . "".splunkcloud.com:8089/servicesNS/"" . fudge_author . ""/"" . app . ""/saved/searches/"" . encoded_title 
| rename _time As KV_Store_Update , updated As Job_Update_Time 
| `strftime_format(Job_Update_Time)` 
| `strftime_format(KV_Store_Update)` 
| `get_identity_info(author)` 
| eval curl_options=""-d allow_skew=\""5m\"" -d schedule_window=\""auto\"" -d realtime_schedule=\""1\"" -d schedule_priority=\""default\""""
| table savedsearch_name rest_uri curl_options an_event_type 
| rex field=rest_uri mode=sed ""s/^.*?:8089/https:\/\/localhost:8089/g"" 
| eval curl_command=""curl -k --netrc-file ~/.netrc -X POST "" . rest_uri . "" -H \""Content-Type:application/json\"" "" . curl_options 
| fields savedsearch_name curl_command 
| stats count by curl_command 
| fields curl_command",0
"GMC-092","GMC 091 converted to provide executable CURL script commands","automation,uri","Scheduled_Jobs","Note that some objects will fail to execute the CURL script. These objects are items with broken searches that throw errors, searches that use private datamodels, etc. This is expected behavior.","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| search shcluster_label=stack-shc disabled=0 is_scheduled=1 schedule_window=0 
| append 
    [| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    | search shcluster_label=stack-shc disabled=0 is_scheduled=1 allow_skew=0 ] 
| append 
    [| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    | search shcluster_label=stack-shc disabled=0 is_scheduled=1 sharing=user (allow_skew!=5m OR schedule_window!=auto OR schedule_priority!=default) ] 
| append 
    [| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
    | search shcluster_label=stack-shc realtime_schedule!=1 ] 
| fields shcluster_label app author sharing savedsearch_name cron_schedule schedule_window allow_skew schedule_priority realtime_schedule is_scheduled disabled 
| search NOT savedsearch_name IN (_ScheduledView__*) 
| stats last(*) as * by shcluster_label savedsearch_name 
| fields shcluster_label app author sharing savedsearch_name cron_schedule is_scheduled disabled schedule_window allow_skew schedule_priority realtime_schedule


| eval schedule_window=""auto"", allow_skew=""5m"", schedule_priority=""default"", realtime_schedule=""1"" 
| fields shcluster_label app author sharing savedsearch_name cron_schedule is_scheduled disabled schedule_window allow_skew schedule_priority realtime_schedule 
| eval fudge_author = case ( 'sharing' == ""user"", 'author', 'sharing' == ""global"", ""nobody"", 'sharing' == ""app"", ""nobody"") 
| eval shcluster_label_rest=if(shcluster_label=""stack-shc"", ""stack"", shcluster_label . ""-stack"") 
| eval encoded_title=savedsearch_name 
| rex field=encoded_title mode=sed ""s:%:%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g
     s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| eval rest_uri=""https://"" . shcluster_label_rest . "".splunkcloud.com:8089/servicesNS/"" . fudge_author . ""/"" . app . ""/saved/searches/"" . encoded_title 
| rename _time As KV_Store_Update , updated As Job_Update_Time 
| `strftime_format(Job_Update_Time)` 
| `strftime_format(KV_Store_Update)` 
| `get_identity_info(author)` 
| eval curl_options=""-d allow_skew=\""5m\"" -d schedule_window=\""auto\"" -d realtime_schedule=\""1\"" -d schedule_priority=\""default\""""
| table savedsearch_name rest_uri curl_options an_event_type 
| rex field=rest_uri mode=sed ""s/^.*?:8089/https:\/\/localhost:8089/g"" 
| eval curl_command=""curl -k --netrc-file ~/.netrc -X POST "" . rest_uri . "" -H \""Content-Type:application/json\"" "" . curl_options 
| fields savedsearch_name curl_command 
| stats count by curl_command 
| fields curl_command",0
"GMC-093A","Summarize Archived Knowledge Objects into the gmc_summary index",automation,"Scheduled_Jobs","This example prepares a list of saved searches belonging to terminated users.  if you have a different use case, replace the where clause with the appropriate filter from GMC SH-17.

Make sure to set testmode=false after you prepare the search that finds all of the knowledge objects scheduled for deletion","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| `get_identity_info(author)` 

| where shcluster_label=""stack-shc"" AND emp_status=""Terminated"" AND ( (is_scheduled=0 OR disabled=1) OR (is_scheduled=1 AND disabled=0 AND isnull(actions)) ) 


| stats 
    Values(correlationsearch_related_searches) As correlationsearch_related_searches
    Values(actions) As actions 
    Values(email_to) As email_to
    Values(acl_perms_read) As acl_perms_read Values(acl_perms_write) As acl_perms_write
    Last(*) As *
    BY shcluster_label app sharing author savedsearch_name 
| rename savedsearch_name As ko_name , savedsearch_search As search 
| eval _time=now() , ko_type=""saved_search"" 
| table _time,ko_name,ko_type,acl_perms_read,acl_perms_write,actions,allow_skew,app,author,correlationsearch_enabled,correlationsearch_label,correlationsearch_related_searches,cron_schedule,data,description,dispatch_as,dispatch_earliest_time,dispatch_latest_time,dispatch_ttl,email_to,notable_drilldown_name,notable_drilldown_search,notable_next_steps,notable_rule_description,notable_rule_title,notable_security_domain,notable_severity,onprem_url,realtime_schedule,schedule_priority,schedule_window,search,sharing,shcluster_label,updated 
| foreach correlationsearch_related_searches acl_perms_* actions email_to 
    [ eval <<FIELD>>=mvjoin(mvsort(mvdedup('<<FIELD>>')), ""#####"") ] 
| eval search_name=""splunk_rest_saved_searches_sh_archive_summary_data"" 
| collect index=`setup_summary_index` testmode=true 
| stats count",0
"GMC-093B","Populate the Knowledge Objects Archive KV Store on the SHC",automation,"Scheduled_Jobs","Run GMC-093A first to prepare the data in the gmc_summary index for the KV Store.  This search runs on the SHC not the GIS since the KV Store lives there.","index=`setup_summary_index` search_name=splunk_rest_saved_searches_sh_archive_summary_data earliest=-7d@d 
| stats Latest(*) As * By shcluster_label app sharing author ko_name 
| foreach correlationsearch_related_searches acl_perms_* actions email_to 
    [ makemv delim=""#####"" <<FIELD>> ] 
| eval _time=now() , ko_type=""saved_search"" 
| table _time,ko_name,ko_type,acl_perms_read,acl_perms_write,actions,allow_skew,app,author,correlationsearch_enabled,correlationsearch_label,correlationsearch_related_searches,cron_schedule,data,description,dispatch_as,dispatch_earliest_time,dispatch_latest_time,dispatch_ttl,email_to,notable_drilldown_name,notable_drilldown_search,notable_next_steps,notable_rule_description,notable_rule_title,notable_security_domain,notable_severity,onprem_url,realtime_schedule,schedule_priority,schedule_window,search,sharing,shcluster_label,updated 
| search NOT 
    [| inputlookup splunk_ko_archive_kv_store_lookup 
    | fields shcluster_label app sharing author ko_name ] 
    `gmc_comment(""| outputlookup splunk_ko_archive_kv_store_lookup createinapp=true create_empty=true override_if_empty=false append=true | stats count"")`",0
"GMC-094","Script to create URL's for the dispatch.ttl update process","automation,uri","Scheduled_Jobs","","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
  `gmc_comment(""Scheduled Searches that happen in the 1-4 minute range"")`
| where is_scheduled=""1"" AND disabled=""0"" 
| `cron_descriptor(cron_schedule)` 
| search cron_schedule_described IN (""Every 1 minutes*"",""Every 2 minutes*"", ""Every 3 minutes*"",""Every 4 minutes*"", ""Every minute*"") 
| eval an_event_type=if(isnull(an_event_type),""Scheduled"",NULL)
| fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule dispatch_ttl an_event_type
| append 
    [ from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
    `gmc_comment(""All Scheduled reports that appear in any dashboard"")`
    | mvexpand Report_Reference 
    | where isnotnull(Report_Reference) AND Report_Reference!=""no-report-reference"" 
    | `get_saved_searches_info(shcluster_label,app,Report_Reference)` 
    | rename Report_Reference As savedsearch_name 
    | eval an_event_type=if(isnull(an_event_type),""Report"",NULL)
    | fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule dispatch_ttl an_event_type
    | where is_scheduled=1] 
| append
    [| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| fields _time,shcluster_label,Splunk_Instance,Splunk_Roles,app,sharing,title,label,description,type,dashboard_size,author,location,updated,time_range,Report_Reference,Dashboard_Reference,Lookup_Reference,Datamodel_Reference,Macro_Reference,Index_Reference,Sourcetype_Reference,Source_Reference,Eventtype_Reference,Rest_Reference,search,BaseSearch,acl_perms_read,acl_perms_write,panel_title,num_panels,auto_refresh_delay,submitButton,autoRun 
| search NOT app IN (splunk_monitoring_console, splunk_archiver, simple_xml_examples, *global_monitoring_console,gmc*) NOT title IN (setup,home,about,settings,contents,help) 
| eval url=""http://"" . shcluster_label . "":8000/en-US/app/"" . app . ""/"" . title 
| `get_instance_roles(Splunk_Instance)` 
| `get_dashboards_usage(shcluster_label,app,title)` 
| `get_identity_info(shcluster_label,author)` 
| `get_identity_info(author)` 
| `fillnull_identity_info` 
| `get_macro_definition(shcluster_label,Macro_Reference)` 
| `strftime_format(updated)` 
| eval Num_Searches=mvcount(search) 
| eval Effective_autoRun = if(autoRun=""false"" AND submitButton=""true"" AND is_time_selector=""true"", ""false"", ""true"" ) 
| fillnull value=""false"" autoRun 
| fillnull value=""none"" submitButton acl_perms_read acl_perms_write auto_refresh_delay panel_title 
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch time_range Macro_Definition 
| search dashboard_size <= 10000000 Days_Latest_Access <= ""90"" Days_Latest_Access >= ""0"" num_panels >= -1 Num_Searches >= -1 
| `get_latest_access_human(Days_Latest_Access)` 
| eval Num_Searches=mvcount(search) 
| mvexpand Report_Reference 
| where isnotnull(Report_Reference) AND Report_Reference!=""no-report-reference"" 
| `get_saved_searches_info(shcluster_label,app,Report_Reference)` 
| rename Report_Reference As savedsearch_name 
| eval an_event_type=if(isnull(an_event_type),""Dash Called"",NULL)
| fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule cron_schedule_described dispatch_ttl an_event_type]
| `cron_descriptor(cron_schedule)` 
| fields _time shcluster_label app savedsearch_name sharing author updated is_scheduled disabled cron_schedule cron_schedule_described dispatch_ttl an_event_type
| where dispatch_ttl=""3p"" AND shcluster_label=""*""
`gmc_comment(""Temporary holding value of 3p being used in place of 2p while we wait to change the default"")`
| eval fudge_author = case ( 'sharing' == ""user"", 'author', 'sharing' == ""global"", ""nobody"", 'sharing' == ""app"", ""nobody"") 
| eval shcluster_label_rest=if(shcluster_label=""stack-shc"", ""stack"", shcluster_label . ""-stack"") 
| eval encoded_title=savedsearch_name 
| rex field=encoded_title mode=sed ""s:%:%25:g s:':%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g
     s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| eval rest_uri=""https://"" . shcluster_label_rest . "".splunkcloud.com:8089/servicesNS/"" . fudge_author . ""/"" . app . ""/saved/searches/"" . encoded_title 
| rename _time As KV_Store_Update , updated As Job_Update_Time 
| `strftime_format(Job_Update_Time)` 
| `strftime_format(KV_Store_Update)` 
| `get_identity_info(author)` 
| eval curl_options=""-d dispatch.ttl=2p"" 
| fields KV_Store_Update Job_Update_Time shcluster_label app savedsearch_name sharing author emp_first emp_last emp_email is_scheduled cron_schedule_described dispatch_ttl rest_uri curl_options an_event_type",0
"GMC-106","Total count of Knowledge Objects by Instance/Cluster","health_assessments","Knowledge_Objects",,"| inputlookup splunk_rest_admin_eventtypes_sh_kv_store_lookup where shcluster_label=gmc 
| stats count 
| append 
    [| inputlookup splunk_rest_admin_lookup_table_files_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_admin_transforms_lookup_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_alert_actions_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_configs_conf_props_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_macros_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_models_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_props_calcfields_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_props_extractions_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_props_fieldaliases_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_props_lookups_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_transforms_extractions_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_data_ui_views_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| append 
    [| inputlookup splunk_rest_saved_searches_sh_kv_store_lookup where shcluster_label=gmc 
    | stats count ] 
| stats sum(count) as KOs",0
"GMC-110","Generate a List of /24 CIDR Blocks",cidr,Infrastructure,"","index=* `comment(""Generate a List of /24 CIDR Blocks - Version 2.0.0"")` 
| regex _raw=""(?<!\d)\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}(?!\d)"" `comment(""Keep only search results whose ""_raw"" field contains IP addresses using a negative lookbehind assertion at the beginning of the expression."")` 
| rex ""(?<octet1>25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.(?<octet2>25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.(?<octet3>25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\.(?<octet4>25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9]$)"" 
| rex ""[vV][eE][rR][sS][iI][oO][nN]:\s*(?<Version>[\d.]+)"" 
| where octet1 < 224 AND octet1 != 0 AND octet1 != 127 AND octet2 < 256 AND octet3 < 256 AND octet4 < 256 
| eval octet1 = IF ( MATCH (octet1, ""^0""), LTRIM (octet1, ""0""), octet1) 
| eval octet2_len = LEN (octet2), octet3_len = LEN (octet3) 
| eval octet2 = IF (MATCH (octet2, ""^0"") AND octet2_len > 1, LTRIM (octet2, ""0""), octet2), octet3 = IF (MATCH (octet3, ""^0"") AND octet3_len > 1, LTRIM (octet3, ""0""), octet3) 
| eval IP_Address = octet1. ""."" .octet2. ""."" .octet3. ""."" .octet4 
| eval CIDR_Range = octet1. ""."" .octet2. ""."" .octet3. "".0/24"" 
| eval IP_Address_Class = CASE (octet1 < 127 AND octet1 > 0, ""Class_A"", octet1 > 127 AND octet1 < 192, ""Class_B"", octet1 > 191 AND octet1 < 224, ""Class_C"") 
| eval Network = CASE (IP_Address_Class = ""Class_A"", octet1. "".0.0.0"", IP_Address_Class = ""Class_B"", octet1. ""."" .octet2. "".0.0"", IP_Address_Class = ""Class_C"", octet1. ""."" .octet2. ""."" .octet3. "".0"" ) 
| eval Netmask = ""255.255.255.0"", Widlcard_Bits=""0.0.0.255"" 
| eval First_IP = octet1. ""."" .octet2. ""."" .octet3. "".0"" , Last_IP=octet1. ""."" .octet2. ""."" .octet3. "".255"" 
| stats VALUES(IP_Address_Class) AS IP_Address_Class VALUES(Network) AS Network VALUES(Netmask) AS Netmask VALUES(Widlcard_Bits) AS Widlcard_Bits VALUES(First_IP) AS First_IP VALUES(Last_IP) AS Last_IP VALUES(IP_Address) AS IP_Addresses DC(IP_Address) AS IPs_In_Subnet BY CIDR_Range 
| table CIDR_Range IP_Address_Class Network Netmask Widlcard_Bits First_IP Last_IP IPs_In_Subnet 
| sort 0 - IPs_In_Subnet
    `comment(""| search CIDR_Range IN (""10.*"", ""172.16.*"", ""192.168.*"")"")` `comment(""RFC-1918 only, remove this line to get all CIDR Blocks"")`",0
"GMC-111","List of Domain Controllers By Domain","",Infrastructure,"","index IN (""msad"", ""appmsad"") sourcetype=MSAD:NT6:Health 
| eval host = UPPER (host), Domain = UPPER (DomainDNSName), Forest = UPPER (ForestName), Domain_Controller = UPPER (Server) 
| stats VALUES(Site) AS Site(s) VALUES(Domain_Controller) AS ""Domain Controller(s)"" DC(Domain_Controller) AS ""# of DCs"" BY Domain",0
"GMC-112","List of Domain Controllers Site and subnet information",cidr,Infrastructure,"","index=msad sourcetype=""*:SiteInfo"" Type=Subnet NOT Name IN (""10.0.0.0*"")
| fields Name ForestName Location Site 
| rex field=Name ""\/(?<bits>\d+)"" 
| eval Site=trim(Site), Location=trim(Location), Name=trim(Name)
| rename Name as cidr
| eval facility_id=if(isnotnull(Site),""facility_id:""+Site,null())
| eval facility_name=if(isnotnull(Location),""facility_name:""+Location,null())
| eval facility_id=if(match(facility_id, ""facility_id:$""), null(), facility_id)
| eval facility_name=if(match(facility_name, ""facility_name:$""), null(), facility_name)
| eval cidr_category = mvappend(facility_id,facility_name)
| stats values(cidr_category) as cidr_category by cidr
| eval cidr_category=mvsort(mvdedup(cidr_category))
| eval cidr_category=mvjoin(cidr_category,""|"")
| table cidr cidr_pci_domain cidr_category cidr_priority cidr_bunit cidr_owner
| where isnotnull(cidr)
| outputlookup create_empty=false createinapp=true seckit_idm_pre_cidr_category.csv",0
"GMC-107","Report on btool output using the scripts in the description","","Configuration_Files","/opt/splunk/bin/splunk cmd btool limits list --debug | grep -v ""/opt/splunk/etc/system/default/"" | awk '{print $2, $3, $4, $1}' | awk '{ ""date \""+%m/%d/%Y %H:%M:%S %z\""""| getline date; if ($1 ~ /^\[/) stanza=$1; else print ""stanza_name=""stanza, ""property_name="" $1, ""property_value="" $3, ""config_file_path=""$4, date, ""config_file=limits.conf""}' | awk '{print $5,$6,$7"","",$8"","",$4"","",$1"","",$2"","",$3}' >> /opt/splunk/var/log/splunk/btool.log


run the above after replacing limits with server, then do the same for web and distsearch","index=_internal sourcetype=splunk_btool config_file=* 
| fields _time config_file config_file_path stanza_name property_name property_value 
| stats latest(*) as * by config_file config_file_path stanza_name property_name 
| table config_file config_file_path stanza_name property_name property_value",0
"GMC-113","ULIMIT reporting",ulimit,Health,"","index=_internal sourcetype=splunkd ulimit (""open files"" OR ""user processes"" OR ""data file size"" OR ""data segment size"") 
| rex ""files:\s+(?<open_files>\d+)"" 
| rex ""processes:\s+(?<user_processes>\d+)"" 
| rex ""data\sfile\ssize\:\s+(?<data_file_size>\w+)"" 
| rex ""data\ssegment\ssize\:\s+(?<data_segment_size>\w+)"" 
| stats latest(open_files) AS open_files latest(user_processes) AS user_processes latest(data_segment_size) AS data_segment_size latest(data_file_size) AS data_file_size by host 
| lookup assets.csv host OUTPUT search_group 
| eval Status=case(open_files<=8192 OR user_processes<=8192 OR data_segment_size!=""unlimited"" OR data_file_size!=""unlimited"", ""severe"",1=1 ,""low"") 
| sort - Status 
| rename open_files AS ""Open Files (-n)"" user_processes AS ""User Processes (-u)"" data_segment_size AS ""Data Segment Size (-d)"" data_file_size AS ""Data File Size (-f)"" search_group AS ""Server Role(s)""",0
"GMC-115","Alert - Splunk Search Head Cluster Searches Running Longer than 1 hour","","Search_Head_Cluster","","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE 
    latest(Search_Activity.Introspection_Search_Usage.app3) AS app
    max(Search_Activity.Introspection_Search_Usage.total_run_time3) AS total_run_time
    max(Search_Activity.Introspection_Search_Usage.fd_used) AS fd_used
    max(Search_Activity.Introspection_Search_Usage.mem_used) AS mem_used
    max(Search_Activity.Introspection_Search_Usage.normalized_pct_cpu) AS normalized_pct_cpu
    latest(Search_Activity.Introspection_Search_Usage.provenance) AS provenance
    max(Search_Activity.Introspection_Search_Usage.read_mb) AS read_mb
    latest(Search_Activity.Introspection_Search_Usage.savedsearch_name3) AS savedsearch_name
    max(Search_Activity.Introspection_Search_Usage.scan_count3) AS scan_count
    latest(Search_Activity.Introspection_Search_Usage.search_type3) AS search_type
    max(Search_Activity.Introspection_Search_Usage.t_count) AS t_count
    latest(Search_Activity.Introspection_Search_Usage.user3) AS user
    max(Search_Activity.Introspection_Search_Usage.written_mb) AS written_mb
    latest(Search_Activity.Introspection_Search_Usage.search_head) as search_head
    latest(Search_Activity.Introspection_Search_Usage.mode) as mode
    latest(Search_Activity.Introspection_Search_Usage.ppid) as ppid
    values(host) as host
    latest(_time) as _time
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Introspection_Search_Usage
    index=_introspection
    AND Search_Activity.Introspection_Search_Usage.app3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.search_type3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.user3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.savedsearch_name3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.search_head = ""*""
    AND Search_Activity.Introspection_Search_Usage.pid = ""*""
    AND Search_Activity.Introspection_Search_Usage.total_run_time3 > 3600
    AND Search_Activity.Introspection_Search_Usage.mode = ""*""
    AND host=""*""
    Search_Activity.Introspection_Search_Usage.app3!=""splunk_global_monitoring_console""
    BY Search_Activity.Introspection_Search_Usage.search_id Search_Activity.Introspection_Search_Usage.pid 
| `gmc_drop_dm_object_name(Search_Activity.Introspection_Search_Usage)` 
| eval mem_used=round(mem_used/1024, 2) 
| `get_shcluster_info(host)` 
| where shcluster_label=""cluster"" 
| `gmc_convert_runtime('total_run_time')` 
| `get_identity_info(shcluster_label,user)`
| table shcluster_label host app user pid ppid full_name Department Title email total_run_time 'total_run_time' mem_used normalized_pct_cpu provenance savedsearch_name",0
"GMC-117","Daily Total Index Size by Cluster By Index Over Time Trellis View report",license,Licensing,"","index=`setup_summary_index` search_name=splunk_internal_index_license_usage_idx_summary_tracker Index_Name=""***"" idxcluster_label=""*"" 
| fields _time search_name idxcluster_label Lic_Pool Lic_Pool_Size type Index_Name Lic_Sourcetype Lic_Source Lic_Host License_Usage 
| makemv delim=""|"" Lic_Sourcetype 
| makemv delim=""|"" Lic_Source 
| makemv delim=""|"" Lic_Host 
| `gmc_byte2mb(License_Usage)` 
| `gmc_byte2gb(License_Usage)` 
| `gmc_byte2tb(License_Usage)` 
| where License_Usage_MB>=0 
| bin _time span=1d 
| stats 
    sum(License_Usage) AS License_Usage
    by _time idxcluster_label Index_Name 
| timechart
    sum(License_Usage) AS License_Usage
    by Index_Name fixedrange=f span=1d 
| foreach ""*"" 
    [ eval <<FIELD>>=round('<<FIELD>>'/1024/1024/1024, 3)]",0
"GMC-118","Dashboard Details by User","",Dashboards,"Summary detail of Dashboards on the SHC","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| fields shcluster_label Splunk_Instance app sharing title label description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference search BaseSearch 
| `get_instance_roles(Splunk_Instance)` 
| `get_dashboards_usage(shcluster_label,app,title)` 
| `get_identity_info(author)` 
| `strftime_format(updated)` 
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch emp_name emp_ttl emp_dep emp_lob emp_cc emp_type emp_status work_city work_region work_country 
| search shcluster_label=""*"" Splunk_Roles=""*"" Splunk_Instance=""*"" app=""*"" author=* sharing=* title=""***"" label=""***"" Report_Reference=""***"" Dashboard_Reference=""***"" Lookup_Reference=""***"" Datamodel_Reference=""***"" Macro_Reference=""***"" Index_Reference=""***"" Sourcetype_Reference=""***"" Source_Reference=""***"" Eventtype_Reference=""***"" Rest_Reference=""***"" BaseSearch=""*"" dashboard_size <= 10000000 search=""***"" Days_Latest_Access <= ""30"" Days_Latest_Access >= ""0"" emp_type=""*"" emp_status=""*"" emp_dep=""*"" emp_lob=""*"" emp_cc=""*"" work_city=""*"" work_region=""*"" work_country=""*"" 
| eval search=mvdedup(mvsort(search)) 
| `get_latest_access_human(Days_Latest_Access)` 
| fields shcluster_label app sharing title label Days_Latest_Access_Display Latest_Access description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference BaseSearch search emp_*
| stats values(*) as * dc(search) as Num_Searches count by shcluster_label app title 
| eval #=1 
| table # shcluster_label app sharing title label Days_Latest_Access_Display Latest_Access description author emp_name emp_ttl emp_dep emp_cc emp_lob emp_type updated Index_Reference 
| `rename_common_fields` 
| `rename_identity_fields` 
| `rename_reference_fields` 
| `rename_dashboards_fields`",0
"GMC-119","Dashboard Details by User v2","",Dashboards,"","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| fields shcluster_label Splunk_Instance app sharing title label description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference search BaseSearch 
| `get_instance_roles(Splunk_Instance)` 
| `get_dashboards_usage(shcluster_label,app,title)` 
| `get_identity_info(author)` 
| `strftime_format(updated)` 
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch emp_name emp_ttl emp_dep emp_lob emp_cc emp_type emp_status work_city work_region work_country 
| search shcluster_label=""*"" Splunk_Roles=""*"" Splunk_Instance=""*"" app=""*"" author=* sharing=* title=""***"" label=""***"" Report_Reference=""***"" Dashboard_Reference=""***"" Lookup_Reference=""***"" Datamodel_Reference=""***"" Macro_Reference=""***"" Index_Reference=""***"" Sourcetype_Reference=""***"" Source_Reference=""***"" Eventtype_Reference=""***"" Rest_Reference=""***"" BaseSearch=""*"" dashboard_size <= 10000000 search=""***"" Days_Latest_Access <= ""9999999"" Days_Latest_Access >= ""0"" emp_type=""*"" emp_status=""*"" emp_dep=""*"" emp_lob=""*"" emp_cc=""*"" work_city=""*"" work_region=""*"" work_country=""*"" 
| eval search=mvdedup(mvsort(search)) 
| `get_latest_access_human(Days_Latest_Access)` 
| fields shcluster_label app sharing title label Days_Latest_Access_Display Latest_Access description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference BaseSearch search 
| stats values(*) as * dc(search) as Num_Searches count by shcluster_label app title 
| eval #=1 
| table # shcluster_label app sharing title label Days_Latest_Access_Display Latest_Access description type dashboard_size author emp_name emp_lob updated 
| `rename_common_fields` 
| `rename_identity_fields` 
| `rename_reference_fields` 
| `rename_dashboards_fields`",0
"GMC-120","Dashboard Resource Usage by User","",Dashboards,"Dashboard resource usage summary by user
- CPU / Mem","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE
    `tstats_gmc_introspection`
    max(""Search_Activity.Introspection_Search_Usage.pct_cpu"") as pct_cpu
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Introspection_Search_Usage 
    AND index=_introspection AND Search_Activity.Introspection_Search_Usage.app3 = ""*"" 
    AND Search_Activity.Introspection_Search_Usage.search_type3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.user3 = ""***""
    AND Search_Activity.Introspection_Search_Usage.savedsearch_name3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.search_head = ""*"" 
    AND Search_Activity.Introspection_Search_Usage.pid = ""*""
    AND Search_Activity.Introspection_Search_Usage.total_run_time3 >= -1
    AND Search_Activity.Introspection_Search_Usage.mem_used >= 0
    AND Search_Activity.Introspection_Search_Usage.mode = ""*""
    AND host=""*""
    Search_Activity.Introspection_Search_Usage.app3!=""splunk_global_monitoring_console""
    BY Search_Activity.Introspection_Search_Usage.search_id Search_Activity.Introspection_Search_Usage.pid 
| `gmc_drop_dm_object_name(Search_Activity.Introspection_Search_Usage)` 
| fields _time host search_id search_head app user acceleration_id delta_scan_count total_run_time fd_used mem_used normalized_pct_cpu pct_cpu page_faults pid ppid provenance read_mb savedsearch_name scan_count search_type t_count written_mb mode 
| `gmc_convert_runtime('total_run_time')` 
| `get_normalized_search_id(search_id)` 
| `get_provenance_fields(provenance)` 
| `get_instance_info(host)` 
| `get_shcluster_info(host)` 
| `get_search_concurrency(shcluster_label, search-concurrency)` 
| `get_saved_searches_info(shcluster_label,app,savedsearch_name)` 
| `get_dashboards_info(shcluster_label,app,Provenance_KO)` 
| `get_dashboards_info(shcluster_label,Provenance_KO)` 
| `get_dashboards_usage(shcluster_label,app,Provenance_KO)` 
| `get_identity_info(user)` 
| `get_cron_schedule(cron_schedule)` 
| fillnull value=""no-search_id"" search_id_normalized 
| fillnull value=999999 Days_Latest_Dashboard_Access 
| fillnull value=0 total_run_time mem_used normalized_pct_cpu 
| fillnull value=""Undefined"" shcluster_label Splunk_Roles search_id_normalized datamodel Provenance_Type Provenance_KO_Type Provenance_KO emp_lob emp_dep work_city work_region work_country savedsearch_type Lookup_Reference Datamodel_Reference 
| search shcluster_label=""cluster"" search_id_normalized=""***"" datamodel=""*"" Provenance_Type=""UI"" Provenance_KO_Type=""Dashboard"" Provenance_KO=""*"" emp_lob=""*"" emp_dep=""*"" work_city=""*"" work_region=""*"" work_country=""*"" Splunk_Roles=""*"" savedsearch_type=""*"" Lookup_Reference=""***"" Datamodel_Reference=""***"" 
| stats Max(normalized_pct_cpu) as ""Normalized CPU %"" Max(pct_cpu) as pct_cpu
Max(mem_used) as ""Memory used in GB""
    By user Provenance_KO 
|  rename Provenance_KO as ""Dashboard Name""
|  where pct_cpu > 0",0
"GMC-121","Dashboard Usage Details by User Count","",Dashboards,"summary of dashboard usage by user and count of times accessed in the Prod VSI SHC","index=_internal sourcetype=splunk_web_access method=GET status=200
    NOT view IN (""home"" , ""search"" , ""dashboard*"", ""alert*"" , ""check_alerts"", ""report*"" , ""field_extractor"" , ""job_manager"" , ""dataset*"" , ""pivot"" , ""show_source"" , ""charting"" , ""data_model*"" , ""flashtimeline"" , ""integrity_check_of_installed_files"" , ""licenseusage"" , ""live_tail"" , ""mod_setup"" , ""orphaned_scheduled_searches"") 
    user!=""-"" 
| rename view As title host As Splunk_Instance 
| fields _time Splunk_Instance app title user spent 
| `get_instance_roles(Splunk_Instance)` 
| `get_shcluster_label(Splunk_Instance)` 
| `get_dashboards_info(shcluster_label,app,title)` 
| search shcluster_label=""*"" Splunk_Instance=""*"" Splunk_Roles=""*"" app=""*"" title=""*"" 
| stats  count AS access_count by user title",0
"GMC-122","Dashboard and relationship to Jobs","",Dashboards,"","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| fields shcluster_label Splunk_Instance app sharing title label description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference search BaseSearch 
| `get_instance_roles(Splunk_Instance)` 
| `get_dashboards_usage(shcluster_label,app,title)` 
| `get_identity_info(author)` 
| `strftime_format(updated)` 
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch emp_name emp_ttl emp_dep emp_lob emp_cc emp_type emp_status work_city work_region work_country 
| search shcluster_label=""*"" Splunk_Roles=""*"" Splunk_Instance=""*"" app=""*"" author=* sharing=* title=""***"" label=""***"" Report_Reference=""***"" Dashboard_Reference=""***"" Lookup_Reference=""***"" Datamodel_Reference=""***"" Macro_Reference=""***"" Index_Reference=""***"" Sourcetype_Reference=""***"" Source_Reference=""***"" Eventtype_Reference=""***"" Rest_Reference=""***"" BaseSearch=""*"" dashboard_size <= 10000000 search=""***"" Days_Latest_Access <= ""9999999"" Days_Latest_Access >= ""0"" emp_type=""*"" emp_status=""*"" emp_dep=""*"" emp_lob=""*"" emp_cc=""*"" work_city=""*"" work_region=""*"" work_country=""*"" 
| eval search=mvdedup(mvsort(search)) 
| `get_latest_access_human(Days_Latest_Access)` 
| fields shcluster_label app sharing title label Days_Latest_Access_Display Latest_Access description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference BaseSearch search 
| stats values(*) as * dc(search) as Num_Searches count by shcluster_label app title 
| eval #=1 
| table # shcluster_label app sharing title label Days_Latest_Access_Display Latest_Access description author updated Report_Reference
| `rename_common_fields` 
| `rename_identity_fields` 
| `rename_reference_fields` 
| `rename_dashboards_fields`",0
"GMC-123","Datamodel constraints","",Dashboards,"","| rest /services/datamodel/model splunk_server_group=dmc_group_search_head 
| fields title description acceleration splunk_server 
| where like(acceleration,""%false%"") 
| rex field=description ""\{\""search\"":\""(?<constraint>[^\""]+)\"""" 
| fields title constraint splunk_server 
| rex field=constraint ""\(\`cim_(?<cim_macro_name>.*?)_indexes\`\)"" 
| join type=outer cim_macro_name splunk_server 
    [| rest /services/admin/macros splunk_server_group=dmc_group_search_head 
    | search eai:acl.app=Splunk_SA_CIM title=cim*_indexes 
    | fields title definition splunk_server 
    | rex field=title ""cim_(?<cim_macro_name>.*?)_indexes"" 
    | rex max_match=0 field=definition ""index=(?<indexes_searched>\w+)"" 
    | fields cim_macro_name indexes_searched definition splunk_server] 
| eval constraint=replace(constraint,""\(\`cim_[^\`]+\`\)"",coalesce(definition,"""")) 
| fields - cim_macro_name definition 
| table splunk_server title constraint indexes_searched",0
"GMC-125","GMC Alert when Hot1 Volume Exceeds 95% Usage","",Alerting,"","index=_introspection sourcetype=splunk_disk_objects component=Volumes search_group=dmc_group_indexer earliest=-15m@m latest=now
| rename data.* AS * name AS Volume max_size AS Volume_Capacity total_size AS Volume_Usage volume_path AS Volume_Path 
| where Volume=""hot1"" 
| eval Used_Perc = round(Volume_Usage/Volume_Capacity * 100 , 2) 
| `gmc_mb2tb(Volume_Usage)`| `gmc_mb2tb(Volume_Capacity)`
| stats
    Max(Used_Perc) as Used_Perc 
    Latest(_time) as _time
    Max(*) as *
    by Volume 
| table _time Volume Volume_Capacity_TB Volume_Usage_TB Used_Perc
| where Used_Perc > 90",1
"GMC-126","GMC Alert when large number of bucket fixups are occurring in the environment","",Alerting,"","index=_internal search_group=dmc_group_cluster_master component=Metrics sourcetype=splunkd name=cmmaster_* group=subtask_* event_message=""*fix*"" host IN (iaasn00090514, psin10p232) (to_fix_search_factor>0 OR to_fix_rep_factor>0) earliest=-5m@m  latest=now
| fields _time host splunk_server to_fix_search_factor to_fix_rep_factor 
| `get_idxcluster_label(splunk_server)` 
| eval to_fix_total = round(to_fix_search_factor + to_fix_rep_factor, 0), to_fix_search_factor=round(to_fix_search_factor), to_fix_rep_factor=round(to_fix_rep_factor) 
| stats 
    Max(_time) as _time
    Max(to_fix_search_factor) as to_fix_search_factor
    Max(to_fix_rep_factor) as to_fix_rep_factor 
    Max(to_fix_total) as to_fix_total 
    by idxcluster_label 
| table _time idxcluster_label to_fix_search_factor to_fix_rep_factor to_fix_total 
| where to_fix_total > 1000",1
"GMC-127","GMC ALL Searches Over Two Hours","",GMC,"Report on all searches across UAT & Prod that exceed 2 hour run time","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE 
    latest(Search_Activity.Introspection_Search_Usage.app3) AS app
    max(Search_Activity.Introspection_Search_Usage.total_run_time3) AS total_run_time
    max(Search_Activity.Introspection_Search_Usage.fd_used) AS fd_used
    max(Search_Activity.Introspection_Search_Usage.mem_used) AS mem_used
    max(Search_Activity.Introspection_Search_Usage.normalized_pct_cpu) AS normalized_pct_cpu
    latest(Search_Activity.Introspection_Search_Usage.provenance) AS provenance
    max(Search_Activity.Introspection_Search_Usage.read_mb) AS read_mb
    latest(Search_Activity.Introspection_Search_Usage.savedsearch_name3) AS savedsearch_name
    max(Search_Activity.Introspection_Search_Usage.scan_count3) AS scan_count
    latest(Search_Activity.Introspection_Search_Usage.search_type3) AS search_type
    max(Search_Activity.Introspection_Search_Usage.t_count) AS t_count
    latest(Search_Activity.Introspection_Search_Usage.user3) AS user
    max(Search_Activity.Introspection_Search_Usage.written_mb) AS written_mb
    latest(Search_Activity.Introspection_Search_Usage.search_head) as search_head
    latest(Search_Activity.Introspection_Search_Usage.mode) as mode
    latest(Search_Activity.Introspection_Search_Usage.ppid) as ppid
    values(host) as host
    latest(_time) as _time
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Introspection_Search_Usage
    index=_introspection earliest=-60m@s
    AND Search_Activity.Introspection_Search_Usage.app3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.search_type3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.user3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.savedsearch_name3 = ""*""
    AND Search_Activity.Introspection_Search_Usage.search_head = ""*""
    AND Search_Activity.Introspection_Search_Usage.pid = ""*""
    AND Search_Activity.Introspection_Search_Usage.total_run_time3 > 7200
    AND Search_Activity.Introspection_Search_Usage.mode = ""*""
    AND host=""*""
    Search_Activity.Introspection_Search_Usage.app3!=""splunk_global_monitoring_console""
    BY Search_Activity.Introspection_Search_Usage.search_id Search_Activity.Introspection_Search_Usage.pid 
| `gmc_drop_dm_object_name(Search_Activity.Introspection_Search_Usage)` 
| eval mem_used=round(mem_used/1024, 2) 
| `get_shcluster_info(host)` 
| `gmc_convert_runtime('total_run_time')` 
| `get_identity_info(shcluster_label,user)`
| table shcluster_label host app user pid ppid email total_run_time 'total_run_time' mem_used normalized_pct_cpu",0
"GMC-128","GMC Data Model Audit DataSet Searches","",GMC,"Trimmed Down Summary of Audit Searches.  May be used for ChargeBack info","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE
    `tstats_gmc_audit`
    latest(_time) AS _time
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    index=_audit
    AND Search_Activity.Audit_Search.search_type1 = ""*""
    AND Search_Activity.Audit_Search.info1 = ""completed""
    AND Search_Activity.Audit_Search.user1 = ""***""
    AND Search_Activity.Audit_Search.savedsearch_name1 = ""*""
    AND host IN (iaasn00030686,iaasln00001933,iaasln00001935,iaasln00001937,iaasln00001938,iaasn00008920,iaasn00008921,iaasn00033501,iaasn00033528,iaasn00033578,iaasn00041951)
    NOT Search_Activity.Audit_Search.savedsearch_name1 IN (""*splunk_global_monitoring_console*"", ""splunk_*_dashboard*"", ""splunk_*_*_lookup_*"", ""splunk_*_*_audit_search_activity_*"")
    Search_Activity.Audit_Search.info1!=""granted""
    BY Search_Activity.Audit_Search.search_id Search_Activity.Audit_Search.info1 
| `gmc_drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename info1 AS info 
| fields _time host available_count considered_events datamodel decompressed_slices event_count dispatch_time exec_time info result_count roles savedsearch_name scan_count search search_et search_id search_lt search_startup_time search_type searched_buckets total_run_time total_slices user 
| `get_normalized_search_id(search_id)` 
| `get_instance_info(host)` 
| `get_shcluster_info(host)` 
| where shcluster_label=""*"" 
| `get_identity_info(user)` 
| table search_id_normalized shcluster_label host search_type info dispatch_time total_run_time _time search_et search_lt Time_Diff user emp_name emp_dep emp_lob emp_ttl work_city work_region work_country scan_count event_count result_count available_count considered_events searched_buckets 
| `normalize_search_type(search_type)` 
| `rename_common_fields` 
| `rename_reference_fields` 
| `rename_saved_searches_fields` 
| `rename_identity_fields` 
| `rename_gmc_audit_fields`",0
"GMC-132","Index bucket information",bucket,Indexes,"","earliest=-30m index=_introspection component=indexes source=""/opt/splunk/var/log/introspection/disk_objects.log"" 
| dedup data.name host 
| stats sum(data.total_size) as indexSize min(data.bucket_dirs.home.event_min_time) as event_min_time max(data.bucket_dirs.home.event_max_time) as event_max_time sum(data.bucket_dirs.home.hot_bucket_count) as hot_bucket_count sum(data.bucket_dirs.home.warm_bucket_count) as warm_bucket_count sum(data.total_event_count) as total_event_count sum(data.total_raw_size) as total_raw_size sum(data.datamodel_summary_size) as DM_size by data.name 
| eventstats sum(indexSize) as totalDataSize count as indexCount 
| eval event_max_time=strftime(event_max_time,""%m/%d/%y %H:%M:%S"") 
| eval event_min_time=strftime(event_min_time,""%m/%d/%y %H:%M:%S"") 
| eval percOfTotal=('indexSize'/'totalDataSize')*100 
| fillnull value=0 
| sort - indexSize 
| eval ""Compression Rate""=(indexSize/total_raw_size)*100 
| rename data.name as ""Index Name"" event_max_time AS ""Latest Event"" event_min_time AS ""Earliest Event""",0
"GMC-133","Indexer Health Information Production","",Health,"","| rest /servicesNS/-/-/admin/system-info splunk_server_group=""*PROD-IDXC-*"" timeout=0 
| rename ""eai:acl.*"" AS ""*"", ""eai:*"" AS ""*"" , transparent_hugepages.* AS transparent_hugepages_* , ulimits.* AS ulimits_* , splunk_server AS Splunk_Instance 
| fields - ""perms.*"", ""can_*"", id, published, removable, modifiable, updated 
| fields Splunk_Instance 
| `get_instance_info(Splunk_Instance)` 
| `strftime_format(startup_time)` 
| table Splunk_Instance Splunk_Roles cluster_label startup_time",0
"GMC-134","License Usage - Previous 30 Days","",Licensing,"","index=_internal sourcetype=splunkd source=*license_usage.log* `license_master` type=""RolloverSummary"" earliest=-90d@d latest=-0d@d 
| fields _time slave b 
| `get_idxcluster_label_guid(slave)` 
| eval Volume=round(((((b / 1024) / 1024) / 1024) / 1024),1) 
| timechart span=1d fixedrange=false usenull=f useother=f limit=0
    sum(Volume) AS Volume by idxcluster_label 
| addtotals fieldname=Total_License_Usage",0
"GMC-135","Prod Monitor Apps Lookup Files",,Miscellaneous,"Prod Apps Lookup Alert The below list of App Lookup files are lager than the current threshold of 200mb.","index=_internal sourcetype=""splunk:monitor:lookup:apps"" earliest_time=-60m latest_time=now host IN (iaasn00033501, iaasn00033528, iaasn00033578, iaasn00030686, iaasn00008920, iaasn00008921, iaasn00041951, iaasn00042924, iaasln00001933, iaasln00001935, iaasln00001937, iaasln00001938)
| stats latest(file_size) as file_size by app file_name 
| eval file_size_kb = round (file_size / 1024, 2), file_size_mb = round (file_size / 1024 / 1024, 2), file_size_gb = round (file_size / 1024 / 1024 / 1024, 2)
| table app file_name   file_size_mb  
| sort 0 - file_size_mb
| where (file_size_mb) > 200",1
"GMC-136","Prod Monitor Users Lookup Files",,Miscellaneous,"Prod Lookup file alert The below list of App Lookup files are lager than the current threshold of 200mb.","index=_internal sourcetype=""splunk:monitor:lookup:users"" earliest_time=-60m latest_time=now host IN (iaasn00033501, iaasn00033528, iaasn00033578, iaasn00030686, iaasn00008920, iaasn00008921, iaasn00041951, iaasn00042924, iaasln00001933, iaasln00001935, iaasln00001937, iaasln00001938)
| stats latest(file_size) as file_size by app user file_name 
| eval file_size_kb = round (file_size / 1024, 2), file_size_mb = round (file_size / 1024 / 1024, 2), file_size_gb = round (file_size / 1024 / 1024 / 1024, 2)
| table app user file_name   file_size_mb 
| sort 0 - file_size_mb
|  where (file_size_mb) > 200",1
"GMC-137","Report on Completed Search Types last day by Search Type",,Miscellaneous,"","| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=FALSE 
    count AS Num_Searches
    FROM DATAMODEL=GMC
    WHERE nodename=Search_Activity.Audit_Search
    AND Search_Activity.Audit_Search.info1=""completed"" 
    AND earliest=-1d@d latest=-0d@d
    BY Search_Activity.Audit_Search.search_type1 
| `drop_dm_object_name(Search_Activity.Audit_Search)` 
| rename search_type1 as Search_Type
| table Search_Type Num_Searches",0
"GMC-138","Report on Splunk File System Usage",disk,"Resource_Usage","","index=_introspection sourcetype=splunk_disk_objects 
| fields host data.* _time 
| rename data.* as * 
| stats latest(*) as * by host 
| eval free_TB=round(free/1024/1024,2), available_TB=round(available/1024/1024,2), capacity_TB=round(capacity/1024/1024,2) 
| table host mount_point fs_type capacity_TB available_TB free_TB",0
"GMC-139","Report_on_highest_number_of_events_by_sourcetype_enriched_with_sourcetype_metadata",,Miscellaneous,"","| metadata type=sourcetypes index=* 
| `get_sourcetype_info(sourcetype)` 
| table sourcetype totalCount firstTime lastTime recentTime author app sharing category rename SHOULD_LINEMERGE LINE_BREAKER TIME_PREFIX TIME_FORMAT MAX_TIMESTAMP_LOOKAHEAD TRUNCATE TZ KV_MODE EVENT_BREAKER_ENABLE EVENT_BREAKER DATETIME_CONFIG INDEXED_EXTRACTIONS LEARN_SOURCETYPE TRANSFORMS updated 
| sort 0 - totalCount",0
"GMC-140","Jobs that have scheduled execution time (cron)","","Scheduled_Jobs","","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| fields shcluster_label Splunk_Instance app sharing title label description type dashboard_size author location updated Report_Reference Dashboard_Reference Lookup_Reference Datamodel_Reference Macro_Reference Index_Reference Sourcetype_Reference Source_Reference Eventtype_Reference Rest_Reference search BaseSearch 
| `get_instance_roles(Splunk_Instance)` 
| `get_dashboards_usage(shcluster_label,app,title)` 
| `get_identity_info(shcluster_label,author)` 
| `strftime_format(updated)` 
| fillnull value=""Undefined"" Splunk_Roles Splunk_Instance app sharing title label description type dashboard_size author search BaseSearch emp_name emp_ttl emp_dep emp_lob emp_cc emp_type emp_status work_city work_region work_country 
| search shcluster_label=""cluster"" Splunk_Roles=""*"" Splunk_Instance=""*"" app=""*"" author=* sharing=* title=""***"" label=""***"" Report_Reference=""***"" Dashboard_Reference=""***"" Lookup_Reference=""***"" Datamodel_Reference=""***"" Macro_Reference=""***"" Index_Reference=""***"" Sourcetype_Reference=""***"" Source_Reference=""***"" Eventtype_Reference=""***"" Rest_Reference=""***"" BaseSearch=""*"" dashboard_size <= 10000000 search=""***"" Days_Latest_Access <= ""9999999"" Days_Latest_Access >= ""0"" emp_type=""*"" emp_status=""*"" emp_dep=""*"" emp_lob=""*"" emp_cc=""*"" work_city=""*"" work_region=""*"" work_country=""*"" 
| stats count by Report_Reference 
|  `get_saved_searches_info(Report_Reference)`
| where is_scheduled=1
| table Report_Reference cron_schedule",0
"GMC-141","SHC User Dashboards Searches",,Miscellaneous,"","index=_internal sourcetype=splunk_web_access method=GET status=200
    NOT view IN (""home"" , ""search"" , ""dashboard*"", ""alert*"" , ""check_alerts"", ""report*"" , ""field_extractor"" , ""job_manager"" , ""dataset*"" , ""pivot"" , ""show_source"" , ""charting"" , ""data_model*"" , ""flashtimeline"" , ""integrity_check_of_installed_files"" , ""licenseusage"" , ""live_tail"" , ""mod_setup"" , ""orphaned_scheduled_searches"") 
    user!=""-"" 
| rename view As title host As Splunk_Instance
| fields _time Splunk_Instance app title user spent 
| `get_instance_roles(Splunk_Instance)` 
| `get_shcluster_label(Splunk_Instance)` 
| `get_dashboards_info(shcluster_label,app,title)`
| search shcluster_label=""*""
| stats dc(search) As Number_Of_Searches dc(user) As Number_of_Users count As NumDashboardAccess by shcluster_label app title
| sort - ""Number_Of_Searches""",0
"GMC-142","Search Duration Skynet search",,Miscellaneous,"","index=_audit TERM(action=search) ( TERM(info=completed) OR ( TERM(info=granted) apiStartTime ""search='search"")) NOT ""search_id='rsa_*"" 
| eval u=case( searchmatch(""user=splunk-system-user OR user=nobody OR search_id=*scheduler_*""), ""Scheduler"", searchmatch((""search_id='1*"")), ""AdHocUser"", 1=1, ""AdHocSaved"") 
| eval search_id=md5(search_id), 
    search_et=if(search_et=""N/A"", 0,
    search_et), search_lt=if(search_lt=""N/A"", exec_time, search_lt), 
    et_diff=case(exec_time>search_et, (exec_time-search_et)/60, 1=1, (search_lt-search_et)/60), searchStrLen=len(search) 
| stats partitions=10 
    sum(searchStrLen) AS searchStrLen
    count
    first(et_diff) AS et_diff
    first(u) as u
    values(search) AS search 
    BY search_id 
| search searchStrLen>0 et_diff=* count>1 
| eval et_range = case(et_diff<=0, ""WTF"", et_diff<2, ""0_1m"", et_diff<6, ""1_5m"", et_diff<11, ""2_10m"", et_diff<16, ""3_15m"", et_diff<=65, ""4_60m"", et_diff<=4*60+10, ""5_4h"", et_diff<=24*60+10, ""6_24h"", et_diff<=7*24*60+10, ""7_7d"", et_diff<=30*24*60+10, ""8_30d"", et_diff<=90*24*60+10, ""9_90d"", 1=1, ""10_>90d"") 
| chart count by et_range, u 
| eval Total=AdHocUser + AdHocSaved + Scheduler 
| eventstats sum(AdHocUser) AS uTotal sum(AdHocSaved) AS aTotal, sum(Scheduler) AS sTotal, sum(Total) AS tTotal 
| eval AdHocUserPerc=round((AdHocUser*100)/uTotal,3), AdHocSavedPerc=round((AdHocSaved*100)/aTotal,3), SchedulerPerc=round((Scheduler*100)/sTotal, 3), TotalPerc=round((Total*100)/tTotal, 3) 
| addcoltotals 
| eval et_range=if(isnull(et_range), ""8_Total"", et_range) 
| fields - aTotal sTotal tTotal, uTotal 
| rex mode=sed field=et_range ""s/\d+_(.*)/\1/g"" 
| accum TotalPerc AS TotalPercCumulative 
| eval TotalPercCumulative=if(TotalPercCumulative<101, round(TotalPercCumulative, 1), """")",0
"GMC-143","Splunk Indexer Cluster CPU Utilization in the last 24 hours by Cluster",,Miscellaneous,"","(component=Hostwide index=_introspection search_group=""dmc_customgroup_PROD-IDXC-*"" sourcetype=splunk_resource_usage) earliest=-1d@d latest=-0d@d 
| eval total_cpu_usage=('data.cpu_system_pct' + 'data.cpu_user_pct') 
| `get_idxcluster_label(host)` 
| bin _time span=10s 
| stats latest(total_cpu_usage) as total_cpu_usage by idxcluster_label host _time 
| timechart minspan=10s perc90(total_cpu_usage) as cpu_usage by idxcluster_label",0
"GMC-144","Splunk SHC Concurrency Calculations",,Miscellaneous,"","| rest splunk_server_group=""*PROD-SHC-1"" /services/configs/conf-limits timeout=0 
| search title IN (search, scheduler, typeahead, concurrency) 
| table splunk_server title base_max_searches max_searches_per_cpu max_searches_perc auto_summary_perc 
| sort max_searches_perc 
| filldown max_searches_perc 
| sort base_max_searches 
| filldown base_max_searches 
| sort max_searches_per_cpu 
| filldown max_searches_per_cpu 
| sort auto_summary_perc 
| filldown auto_summary_perc 
| eval number_of_cpus = 32 
| eval splunk_server=upper(splunk_server) 
| eval max_searches_perc = if ( match(splunk_server, ""IAASN00030686|IAASN00033501|IAASN00008920|IAASLN00001933|IAASN00041951""), 50, max_searches_perc) 
| eval max_hist_searches = (max_searches_per_cpu * number_of_cpus) + base_max_searches 
| eval max_hist_searches_cluster = 11 * max_hist_searches 
| eval max_hist_scheduled_searches = (max_hist_searches * max_searches_perc) / 100 
| eval max_hist_scheduled_searches_cluster = 11 * max_hist_scheduled_searches 
| table splunk_server max_hist_scheduled_searches_cluster max_hist_searches_cluster max_hist_searches max_hist_scheduled_searches base_max_searches max_searches_per_cpu number_of_cpus max_searches_perc auto_summary_perc 
| dedup splunk_server",0
"GMC-145","Splunk Search Head Cluster Memory Utilization in the last 24 hours",,Miscellaneous,"","(component=Hostwide index=_introspection search_group=""dmc_customgroup_PROD-SHC-1"" sourcetype=splunk_resource_usage) earliest=-1d@d latest=-0d@d 
| eval pct_mem_usage=(('data.mem_used' / 'data.mem') * 100), server=host 
| bin _time span=10s 
| stats latest(pct_mem_usage) as dedup_pct_mem_usage by server _time 
| timechart minspan=10s Perc90(dedup_pct_mem_usage) as pct_mem_usage",0
"GMC-149","Workload Manager Aborted Jobs",,Miscellaneous,"","index=_internal sourcetype=splunkd component=WorkloadManager 
| rex ""workload\s+rule\s+(?<workload_rule>.*?)\."" 
| rex ""The\s+search\s+(?<sid>.*?)\s"" 
| fields _time host sid workload_rule event_message 
| join sid 
    [ search index=_internal sourcetype=scheduler sid=* 
    | fields _time host sid user app savedsearch_name scheduled_time dispatch_time workload_pool] 
| stats count As Num_Aborts latest(_time) As _time latest(*) As * By app user savedsearch_name 
| `get_saved_searches_info(savedsearch_name)` 
| `strftime_format(_time)` 
| `strftime_format(updated)` 
| table _time savedsearch_name workload_rule workload_pool event_message app savedsearch_type cron_schedule updated allow_skew dispatch_earliest_time dispatch_latest_time Num_Aborts",0
"GMC-152","private_splunk_getCloudIndexesInfo",rest,Indexes,"Get indexes information from all available indexers.","| rest 
    [ rest splunk_server=local /services/server/info 
    | eval splunk_server = if(server_roles == ""cluster_search_head"" AND instance_type == ""cloud"", ""idx*"", ""local"") 
    | return splunk_server] 
    /services/data/indexes datatype=all 
| stats sum(currentDBSizeMB) as value_currentDBSizeMB 
    max(maxTime) as value_maxTime 
    min(minTime) as value_minTime 
    first(*) as * 
    by title 
| rename value_currentDBSizeMB AS currentDBSizeMB 
    value_maxTime AS maxTime 
    value_minTime AS minTime 
| join type=outer title 
    [ eventcount 
        [ rest splunk_server=local /services/server/info 
        | eval splunk_server = if(server_roles == ""cluster_search_head"" AND instance_type == ""cloud"", ""idx*"", ""local"") 
        | return splunk_server] 
        summarize=f index=* 
    | stats sum(count) AS totalEventCount by index 
    | rename index AS title] 
| eval totalEventCount = if(isnotnull(totalEventCount), totalEventCount, 0)",0
"GMC-153","apps_application_inventory",,Miscellaneous,"Gathers all active and applications across your Splunk deployment, what instance type they are installed on, and the version. This check is critical to determine which content should be migrated to Splunk Cloud.","| rest /servicesNS/-/-/apps/local/ splunk_server=* 
| search core=0 disabled=0 
| join splunk_server 
[| rest /services/server/info splunk_server=* 
| fields splunk_server server_roles 
| mvexpand server_roles
| sort server_roles
| mvcombine server_roles delim="":"" 
| nomv server_roles ]
| fields server_roles label title version author description 
| rename server_roles AS instance_roles
| dedup instance_roles title 
| eventstats dc(title) AS distinct_apps 
| eval value=distinct_apps, limit=250
| eval severity_level=if(distinct_apps>250,2,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""apps_application_inventory""
| `scma_summary_index`",0
"GMC-154","apps_local_user_content",,Miscellaneous,"This search checks for any private knowledge objects in your environment that needs to be migrated. Currently searches for transforms, props, savedsearches, tags, eventtypes, lookups, and dashboards.","| makeresults 
| eval config=""transforms,props,savedsearches,tags,eventtypes"" 
| makemv config delim="","" 
| mvexpand config 
| map maxsearches=10 search=""| rest /servicesNS/-/-/configs/conf-$config$ splunk_server=* 
| search eai:acl.sharing=user
| eval config=\""$config$\""
    | fields eai:acl.app eai:userName eai:acl.sharing config"" 
| append 
    [| rest /servicesNS/-/-/data/ui/views splunk_server=* 
    | search eai:acl.sharing=user 
    | eval config=""dashboard"" 
    | fields eai:acl.sharing eai:userName eai:acl.app config] 
| append 
    [| rest /servicesNS/-/-/data/lookup-table-files splunk_server=* 
    | search eai:acl.sharing=user 
    | eval config=""lookup-file"" 
    | fields eai:acl.sharing eai:userName eai:acl.app config]
    | rename eai:acl.app AS app eai:userName AS user
    | stats count AS object_count by config app user
    | sort - object_count
    | eval severity_level=2
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""apps_local_user_content""
| `scma_summary_index`",1
"GMC-155","apps_premium_application_migration_count",,Miscellaneous,"","| inputlookup scma_customer_details
| fields license_es license_exchange license_itsi license_pci license_vmware
| transpose
| rename ""row 1"" AS volume
| stats count(eval(volume>0)) AS premium_apps
| eval severity_level=if(premium_apps>0,1,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""apps_premium_application_migration_count""
| `scma_summary_index`
description = Checks for selected premium applications to be installed in Splunk Cloud to assist with PS level of effort.",1
"GMC-156","apps_splunk_apps_to_vet",,Miscellaneous,"Compares the currently installed Splunk applications to which applications are vetted for Splunk Cloud and which ones need reviewed and submitted through the application vetting process.","| rest /servicesNS/-/-/apps/local/ splunk_server=* 
| search core=0 disabled=0 
| join splunk_server 
    [| rest /services/server/info splunk_server=* 
    | fields splunk_server server_roles 
    | mvexpand server_roles 
    | sort server_roles 
    | mvcombine server_roles delim="":"" 
    | nomv server_roles] 
| fillnull version value=""No Version Set"" 
| rename title AS appid server_roles AS instance_roles
| fields instance_roles label appid version 
| dedup instance_roles appid version 
| lookup splunkbase_apps.csv app_version AS version appid 
| makemv delim="", "" product_compatibility 
| rex field=version_compatibility ""(?<latest_major_version>[^,]+)"" 
| eval hosting_location=if(isnull(uid),""Private App"",""Splunkbase"") 
| eval app_compatibility_splunk_cloud=if(mvfind(product_compatibility,""Splunk\sCloud"")=1,""passed"",""failed"") 
| eval app_compatibility_splunk_version=if(latest_major_version>=8, ""passed"", ""failed"") 
| eval app_status=case(hosting_location==""Splunkbase"" AND app_compatibility_splunk_cloud==""passed"" AND app_compatibility_splunk_version==""passed"", ""Splunk Cloud Compatible"",1==1,""Needs Additional Review"") 
| sort app_status 
| join type=outer appid 
    [| inputlookup splunkbase_apps.csv 
    | rex field=version_compatibility ""(?<latest_major_version>[^,]+)"" 
    | search product_compatibility=""*Splunk Cloud*"" latest_major_version>=8 
    | eval new_version=app_version 
    | rex mode=sed field=new_version ""s/\.//g"" 
    | eventstats max(new_version) AS max_version by appid 
    | where new_version=max_version 
    | rename app_version AS lastest_splunk_cloudv8_supported_version 
    | fields lastest_splunk_cloudv8_supported_version appid] 
| join type=outer appid 
    [| rest /servicesNS/-/-/properties splunk_server=* 
    | search title!=""passwords"" title!=""app"" 
    | stats count by title 
    | fields - count 
    | map maxsearches=10000 search=""| scmabtool $title$
| eval conf=\""$title$\"".\"".conf\"""" 
    | stats dc(conf) AS local_confs by app 
    | rename app AS appid] 
| fillnull local_confs value=0 
| eval hosting_location=if(isnull(uid),""Private App"",""Splunkbase"") 
| eval app_compatibility_splunk_cloud=if(mvfind(product_compatibility,""Splunk\sCloud"")=1,""passed"",""failed"") 
| eval app_compatibility_splunk_version=if(latest_major_version>=8, ""passed"", ""failed"") 
| eval app_status=case(hosting_location==""Splunkbase"" AND app_compatibility_splunk_cloud==""passed"" AND app_compatibility_splunk_version==""passed"" AND local_confs=0, ""Splunk Cloud Compatible"",1==1,""Needs Additional Review"") 
| sort app_status 
| table app_status appid local_confs hosting_location version lastest_splunk_cloudv8_supported_version app_compatibility* 
| appendpipe 
    [| stats count(eval(app_status=""Needs Additional Review"")) AS apps_to_review count(eval(app_status=""Splunk Cloud Compatible"")) AS cloud_compatible_apps] 
| eval severity_level=if(app_status==""Needs Additional Review"",2,0) 
| rename app_status AS ""App_Vetting_Status"" appid AS Application hosting_location AS ""App_Type"" version AS ""App_Version"" lastest_splunk_cloudv8_supported_version AS ""Latest_App_Version_Supported_Splunk_CloudV8"" app_compatibility_splunk_cloud AS ""App_Compatibility_Splunk_Cloud_Vetted_Check"" app_compatibility_splunk_version AS ""App_Compatibility_Splunk_Version_V8_Check"" 
| eval Latest_App_Version_Supported_Splunk_CloudV8=case(App_Type==""Private App"",""N/A"",App_Type==""Splunkbase"" AND isnull(Latest_App_Version_Supported_Splunk_CloudV8),""Not V8+ Supported on Cloud"",1=1,Latest_App_Version_Supported_Splunk_CloudV8)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""apps_splunk_apps_to_vet""
| `scma_summary_index`",1
"GMC-157","apps_top_app_hits",,Miscellaneous,"Identifies the most popular applications accessed in your environment over the last week.   Apps that are not listed here have not been used in this timeframe.","index=_internal sourcetype=splunkd_ui_access method=GET uri=""*/app/*"" user!=cmon_user user!=""-"" earliest=-7d@d | rex field=uri ""app/(?<app>[^/]+)/"" | top limit=50 app | eval _time=now() | join [|inputlookup scma_customer_details | table customer_name] | eval severity_level=0 | `scma_drop_fields` | eval check_name=""apps_top_app_hits"" | `scma_summary_index`",0
"GMC-159","environment_instance_inventory",,Miscellaneous,"This search collects the instances configured in your environment, their Splunk version, hardware resources, and role. This search does not collect server names or IP addresses.","| rest splunk_server=* /servicesNS/-/-/server/info 
| eval cpu_core_count = if(isnotnull(numberOfVirtualCores), numberOfVirtualCores, numberOfCores) 
| mvexpand server_roles 
| sort server_roles 
| mvcombine server_roles delim="":"" 
| nomv server_roles 
| rename server_roles AS instance_roles
| eval physical_memory_gb = round(physicalMemoryMB / 1024, 0) 
| fields splunk_server instance_roles version cpu_core_count physical_memory_gb os_name 
| stats dc(splunk_server) AS instance_count by instance_roles version cpu_core_count physical_memory_gb os_name 
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""environment_instance_inventory""
| `scma_summary_index`",0
"GMC-160","forwarders_count_and_inventory",,Miscellaneous,"This check validates the number of forwarders actively sending data over the last 24 hours and their version compatibility with Splunk Cloud.","index=_internal sourcetype=splunkd (connectionType=cooked OR connectionType=cookedSSL) (fwdType=* group=tcpin_connections guid=* destPort!=""-"") earliest=-24h@h 
| stats dc(hostname) AS unique_forwarders by destPort, version fwdType 
| eval latest_supported_version=""7.3"", status=if(version<latest_supported_version,""!!out of support- consider upgrading!!"",""Supported"") 
| appendpipe [| stats count AS events sum(unique_forwarders) AS total_forwarders sum(eval(severity_level=2)) AS value]
| eval severity_level=case(status=""!!out of support- consider upgrading!!"",2,status=""Supported"",0,events=0,2,1=1,""-1"") 
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| rename destPort AS dest_port, fwdType AS fwd_type
| `scma_drop_fields`
| eval check_name=""forwarders_count_and_inventory""
| `scma_summary_index`",0
"GMC-161","forwarders_intermediary_forwarder_check",,Miscellaneous,"This check determines if there are any intermediate forwarders in the environment prior to data being indexed.","index=_internal sourcetype=splunkd source=*metrics.log group=tcpin_connections earliest=-24h@h 
| eval host=case(match(host, ""^idx-i-.*\.splunkcloud\.com$""), ""indexers"", 1==1, host) 
| rename hostname as from host as to 
| stats count by from to 
| eventstats values(to) AS upstream_s2s 
| eval from=case(isnotnull(mvfind(upstream_s2s, from)), from, 1==1, ""source"") 
| fields - upstream_s2s 
| stats count AS forwarder_count by from to 
| eval x=mvappend(from,to) 
| stats dc(x) AS forwarder_hops 
| eval forwarder_hops=if(forwarder_hops=0,0,forwarder_hops-2) 
| eval status=if(forwarder_hops>0,""Intermediate Forwarders Present - Review Architecture"",""No Intermediate Forwarders Present"") 
| eval severity_level=if(forwarder_hops>0,1,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""forwarders_intermediary_forwarder_check""
| `scma_summary_index`",1
"GMC-162",getIndexesList,,Miscellaneous,"Get indexes information from the indexers, this search is needed to populate the cloud indexes manager page.","| rest splunk_server=idx* /services/data/indexes datatype=all 
| where isInternal == 0 
| stats values(eai:acl.app) as eai:acl.app 
        values(datatype) as datatype 
        values(disabled) as disabled 
        values(frozenTimePeriodInSecs) as frozenTimePeriodInSecs 
        values(isVirtual) as isVirtual 
        values(maxTotalDataSizeMB) as maxTotalDataSizeMB 
        values(maxGlobalDataSizeMB) as maxGlobalDataSizeMB 
        values(maxGlobalRawDataSizeMB) as maxGlobalRawDataSizeMB 
        values(archiver.selfStorageProvider) as archiver.selfStorageProvider 
        values(archiver.selfStorageBucket) as archiver.selfStorageBucket 
        values(archiver.selfStorageBucketFolder) as archiver.selfStorageBucketFolder 
        values(archiver.coldStorageProvider) as archiver.coldStorageProvider 
        values(archiver.coldStorageRetentionPeriod) as archiver.coldStorageRetentionPeriod 
        values(archiver.enableDataArchive) as archiver.enableDataArchive 
        values(archiver.maxDataArchiveRetentionPeriod) as archiver.maxDataArchiveRetentionPeriod 
        by title 
| join type=outer title [rest splunk_server=local count=0 /services/data/distributed-indexes 
                         | fields title rawSizeBytes eventCount startEpoch endEpoch 
                         | rename eventCount AS totalEventCount startEpoch AS minTime endEpoch AS maxTime] 
| eval totalRawSizeMB = if(isnotnull(rawSizeBytes), round(rawSizeBytes / 1024 / 1024, 2), 0) 
| eval totalEventCount = if(isnotnull(totalEventCount), totalEventCount, 0)",0
"GMC-163",getMetricTransformsList,,Miscellaneous,"Get the list of metric schema transforms for all the indexers","|rest splunk_server=idx*  /services/data/transforms/metric-schema",0
"GMC-164","indexing_auto_assigned_sourcetypes",,Miscellaneous,"This search captures all data sources that do not have their sourcetype set properly and Splunk was forced to guess them.","| tstats summariesonly=t count AS event_count dc(source) AS source dc(host) AS hosts WHERE index=* earliest=-7d@d GROUPBY sourcetype index | regex sourcetype=""\-\d+$|too_small"" | eval severity_level=2 | eval _time=now() | join [|inputlookup scma_customer_details | table customer_name] | `scma_drop_fields` | eval check_name=""indexing_auto_assigned_sourcetypes"" | `scma_summary_index`",1
"GMC-165","indexing_data_hygiene_check",,Miscellaneous,"This search captures all data sources indexed over the last 30 days to create an inventory of active sources in your environment.","(index=_internal source=*splunkd.log* splunk_server=* (log_level=ERROR OR log_level=WARN) (component=AggregatorMiningProcessor OR component=DateParserVerbose OR component=LineBreakingProcessor)) earliest=-7d@d 
| rex field=event_message ""Context: source(::|=)(?<context_source>[^\\|]*?)\\|host(::|=)(?<context_host>[^\\|]*?)\\|(?<context_sourcetype>[^\\|]*?)\\|"" 
| eval data_source=if((isnull(data_source) AND isnotnull(context_source)),context_source,data_source), data_host=if((isnull(data_host) AND isnotnull(context_host)),context_host,data_host), data_sourcetype=if((isnull(data_sourcetype) AND isnotnull(context_sourcetype)),context_sourcetype,data_sourcetype) 
| stats count(eval(component==""LineBreakingProcessor"" OR component==""DateParserVerbose"" OR component==""AggregatorMiningProcessor"")) as total_issues dc(data_host) AS ""Host Count"" dc(data_source) AS ""Source Count"" count(eval(component==""LineBreakingProcessor"")) AS ""Line Breaking Issues"" count(eval(component==""DateParserVerbose"")) AS ""Timestamp Parsing Issues"" count(eval(component==""AggregatorMiningProcessor"")) AS ""Aggregation Issues"" by data_sourcetype | sort - total_issues 
| rename data_sourcetype as Sourcetype, total_issues as ""Total Issues"" 
| eval severity_level=2
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_data_hygiene_check""
| `scma_summary_index`",1
"GMC-166","indexing_data_retention_check",,Miscellaneous,"This search determines the average retention in Days configured for all indexes in your environment and compares it to the default Splunk Cloud retention.","| rest splunk_server=* /servicesNS/-/-/data/indexes 
| search isInternal=0 disabled=0 
| eval retentionInDays=round(frozenTimePeriodInSecs/86400,0) 
| stats max(retentionInDays) AS max_retentionInDays avg(retentionInDays) AS avg_retentionInDays 
| eval value=round(avg_retentionInDays,0)
| eval limit=90
| eval avg_retentionInDays=round(avg_retentionInDays,0) 
| eval severity_level=if(avg_retentionInDays>90,2,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_data_retention_check""
| `scma_summary_index`",0
"GMC-167","indexing_data_source_inventory",,Miscellaneous,"This search captures all data sources indexed over the last 7 days to create an inventory of active sources in your environment.","| tstats count dc(host) AS host_count WHERE index=* earliest=-7d@d BY index sourcetype | sort 0 count desc | eval severity_level=0 | eval _time=now() | join [|inputlookup scma_customer_details | table customer_name] | `scma_drop_fields` | eval check_name=""indexing_data_source_inventory"" | `scma_summary_index`",1
"GMC-168","indexing_index_counts",,Miscellaneous,"This search counts the unique number of active indexes with data in them and compares it to the maximum number allowed in Splunk Cloud, which is 400.","| rest splunk_server=* /servicesNS/-/-/data/indexes 
| search isInternal=0 disabled=0
| where currentDBSizeMB > 0
| stats dc(title) AS total_indexes
| eval value=total_indexes
| eval limit=400
| eval severity_level=if(total_indexes>=limit,3,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_index_counts""
| `scma_summary_index`",0
"GMC-169","indexing_index_inventory",,Miscellaneous,"This search reviews all index definitions in the environment across all instances and identifies any potential mismatch entries between hosts for the same index.","| rest splunk_server=* /servicesNS/-/-/data/indexes 
| search isInternal=0 disabled=0
| eval retentionInDays=round(frozenTimePeriodInSecs/86400,0)
| stats dc(splunk_server) AS distinct_splunk_indexers max(retentionInDays) AS max_retentionInDays avg(retentionInDays) AS avg_retentionInDays min(retentionInDays) AS min_retentionInDays by title
| eval severity_level=if(max_retentionInDays!=avg_retentionInDays,2,0)
| sort - max_retentionInDays
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_index_inventory""
| `scma_summary_index`",0
"GMC-170","indexing_smart_store_check",,Miscellaneous,"This search checks to determine if SmartStore is currently deployed in your environment. SmartStore is specifically important when considering migrating historical data since it requires Professional Services to move the data via Snowball.","| rest /servicesNS/-/-/configs/conf-indexes splunk_server=* 
| search path=s3* 
| stats dc(splunk_server) AS smartstore_instance_count 
| eval severity_level=if(smartstore_instance_count>0,2,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_smart_store_check""
| `scma_summary_index`",1
"GMC-171","indexing_splunk_cloud_storage_blocks",,Miscellaneous,"Checks to calculate the estimated number of storage blocks in Splunk Cloud based on data ingestion volumes and current index retention settings.","index=_internal source=*license_usage.log type=Usage earliest=-30d@d latest=@d
| eval gb=b/1024/1024/1024 
| bin span=1d _time 
| stats sum(gb) AS gb by idx _time
| stats avg(gb) AS avg_volume_gb by idx
| eval avg_volume_gb=round(avg_volume_gb,2)
| rename idx AS index
| join index
[| rest splunk_server=* /servicesNS/-/-/data/indexes
| search isInternal=0 disabled=0 
| eval retentionInDays=round(frozenTimePeriodInSecs/86400,0) 
| stats avg(retentionInDays) AS avg_retentionInDays by title
| rename title AS index]
| eval default_cloud_retention=90
| eval adjusted_retention_days=if(avg_retentionInDays<=default_cloud_retention,0,avg_retentionInDays-default_cloud_retention)
| eval splunk_cloud_block_size_gb=500
| eval splunk_cloud_estimated_blocks=(adjusted_retention_days*avg_volume_gb)/splunk_cloud_block_size_gb
| stats avg(avg_retentionInDays) AS avg_retentionInDays sum(splunk_cloud_estimated_blocks) AS storage_blocks_to_purchase
| eval storage_blocks_to_purchase=round(storage_blocks_to_purchase,0)
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_splunk_cloud_storage_blocks""
| `scma_summary_index`",1
"GMC-172","indexing_total_data_on_disk",,Miscellaneous,"This search calculates the total amount of data kept on disk to determine the amount of effort required by Professional Services to migrate your historical data into Splunk Cloud, if applicable. Not all Customers migrate their data into Splunk Cloud and this should be discussed with your Splunk Account Team for more details on the process.","| dbinspect index=* earliest=1 latest=now() 
| stats max(sizeOnDiskMB) as bucketSize by bucketId splunk_server index 
| eval total_size_gb=bucketSize/1024 
| stats sum(total_size_gb) as total_size_gb by index 
| addcoltotals label=""TOTAL"" labelfield=index 
| sort 0 total_size_gb desc 
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""indexing_total_data_on_disk""
| `scma_summary_index`",1
"GMC-173","inputs_hec_check",,Miscellaneous,"Checks to determine if the HTTP Event Collector (HEC) is used in your environment and to aid in the PS migration planning.","| rest splunk_server=* /services/data/inputs/http 
| search disabled=0 
| eval enabled = if(disabled == 1, ""No"", ""Yes"") 
| eval Token=substr(title, 8) 
| table Token, enabled 
| eval severity_level=if(isnotnull(Token),1,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""inputs_hec_check""
| `scma_summary_index`",1
"GMC-174","inputs_modular_input_check",,Miscellaneous,"Checks to determine modular/scripted inputs in the environment and if the application is likely Splunk Cloud vetted.","| rest splunk_server=* /servicesNS/-/-/data/inputs/all 
| search disabled=0 eai:type!=monitor (eai:acl.app!=splunk_instrumentation eai:acl.app!=introspection_generator_addon eai:acl.app!=splunk_monitoring_console eai:acl.app!=splunk_app_addon-builder eai:acl.app!=Splunk_TA_nix eai:acl.app!=Splunk_TA_windows) 
| fields eai:type title eai:acl.app group splunk_server 
| rename eai:acl.app AS app_name title AS input_title eai:type AS input_type 
| join splunk_server 
[| rest /services/server/info splunk_server=* 
| fields splunk_server server_roles 
| mvexpand server_roles 
| sort server_roles 
| mvcombine server_roles delim="":"" 
| nomv server_roles] 
| rename server_roles AS instance_roles
| join type=outer splunk_server app_name 
[| rest /servicesNS/-/-/apps/local/ splunk_server=* 
| search core=0 disabled=0 
| rename title AS app_name 
| fields app_name version splunk_server] 
| dedup input_title input_type app_name 
| lookup splunkbase_apps.csv app_version AS version appid AS app_name 
| makemv delim="", "" product_compatibility 
| rex field=version_compatibility ""(?<latest_major_version>[^,]+)"" 
| eval hosting_location=if(isnull(uid),""Private App"",""Splunkbase"") 
| eval app_compatibility_splunk_cloud=if(mvfind(product_compatibility,""Splunk\sCloud"")=1,""passed"",""failed"") 
| eval app_compatibility_splunk_version=if(latest_major_version>=8, ""passed"", ""failed"") 
| eval app_status=case(hosting_location==""Splunkbase"" AND app_compatibility_splunk_cloud==""passed"" AND app_compatibility_splunk_version==""passed"", ""Likely Splunk Cloud Compatible"",1==1,""Needs Additional Review"") 
| table instance_roles input_title input_type app_name app_status 
| eventstats dc(input_title) AS input_count 
| eval severity_level=if(input_count>50,2,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""inputs_modular_input_check""
| `scma_summary_index`",1
"GMC-175","licensing_current_entitlements",,Miscellaneous,"This search collects the all valid Splunk licenses installed in your deployment","| rest splunk_server=* /servicesNS/-/-/licenser/licenses 
| search status=""VALID"" (label!=""Splunk Forwarder"" label!=""Splunk Free"" label!=""Splunk Light Free"") 
| fields label type quota expiration_time group_id stack_id status creation_time 
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""licensing_current_entitlements""
| `scma_summary_index`",0
"GMC-176","licensing_current_volumes",,Miscellaneous,"This search analyzes your average daily Core licensing limit to the actual volume generated over the last 30 days.","index=_internal source=*license_usage.log* type=""RolloverSummary"" earliest=-30d@d latest=@d
| bin span=1d _time
| stats sum(b) AS daily_volume_gb by _time
| eval daily_volume_gb = round(daily_volume_gb/1024/1024/1024,2)
| appendpipe [| stats max(daily_volume_gb) AS max_daily_volume_gb avg(daily_volume_gb) AS avg_daily_volume_gb]
| eval value=avg_daily_volume_gb
| rename _time AS license_time
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name license_core]
| `scma_drop_fields`
| eval check_name=""licensing_current_volumes""
| `scma_summary_index`",0
"GMC-177","licensing_premium_app_balanced",,Miscellaneous,"Calculates the ration between Premium Applications and the Splunk Enterprise Core license for Splunk Cloud.","| inputlookup scma_customer_details
| fields - _key cloud_provider cloud_env customer_name encryption_at_rest
| rename license_core AS licensecore
| foreach license_* [ eval balanced_<<FIELD>>=case(license_<<MATCHSTR>>=0,""N/A:"".<<FIELD>>,((license_<<MATCHSTR>>/licensecore)*100)<20,""No, seek Deal Desk approval for unbalanced Premium apps below 20% capacity:"".(<<FIELD>>/licensecore*100),license_<<MATCHSTR>> > licensecore,""Premium license larger than Core license:"".(<<FIELD>>/licensecore*100),1==1,""OK:"".<<FIELD>>/licensecore*100)]
| fields - license*
| transpose column_name=balance_check
| rename ""row 1"" AS status1
| rex field=status1 ""(?<status>.+)\:(?<value>.+)""
| fields - status1
| eval limit=""20%""
| eval value=round(value,0)
| eval severity_level=case(status==""N/A"",-1,status==""No, seek Deal Desk approval for unbalanced Premium apps below 20% capacity"",2,status==""OK"",0,status==""Premium license larger than Core license"",3,1=1,""OK"")
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""licensing_premium_app_balanced""
| `scma_summary_index`",0
"GMC-178","precheck_core_license_set",,Miscellaneous,"Checks to determine if the Core license value was set during the initial setup of the Splunk Cloud Migration Assessment Instructions dashboard.","| inputlookup scma_customer_details | table license_core | eval severity_level=if(license_core=0,3,0) | eval _time=now() | eval value=license_core | join [|inputlookup scma_customer_details | table customer_name] | `scma_drop_fields` | eval check_name=""precheck_core_license_set"" | `scma_summary_index`",0
"GMC-179","precheck_customer_set",,Miscellaneous,"This checks to ensure the Customer Name was properly set during the initial setup of the Splunk Cloud Migration Assessment Instructions dashboard.","| inputlookup scma_customer_details | table customer_name | eval severity_level=if(len(customer_name)<=1,3,0) | eval _time=now() | eval check_name=""precheck_customer_set"" | `scma_summary_index`",0
"GMC-180","precheck_data_access",,Miscellaneous,"Validates results and access to the rest endpoint, _introspection, _audit, and _internal.","| makeresults 
| eval type=""audittrail,license_usage.log,splunk_resource_usage,splunkd,rest"" 
| makemv delim="","" type 
| mvexpand type
| join type type=left 
[| tstats dc(host) AS hosts where ((index=_internal sourcetype=splunkd) OR (index=_introspection sourcetype=splunk_resource_usage) OR (index=_audit sourcetype=audittrail)) earliest=-30m@m [| rest /services/server/info splunk_server=* | fields splunk_server | rename splunk_server AS host] by index sourcetype | rename sourcetype AS type
| append [|tstats dc(host) AS hosts where index=_internal source=*license_usage.log TERM(Usage) earliest=-24h@h by index | eval type=""license_usage.log""]
| append [| rest /services/server/info splunk_server=* | eval type=""rest"" | stats dc(splunk_server) AS hosts by type]]
| fillnull hosts value=0
| eval message=if(hosts>0,""Results Present - Verify that the Total Number of Instances is correct."",""No Results Found! Check Data Access and Monitoring Console Config Before Proceeding!"")
| table index type message hosts 
| eval severity_level=if(hosts=0,3,0)
| rename type AS data_source hosts AS total_number_of_instances
| sort - Index
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""precheck_data_access""
| `scma_summary_index`",0
"GMC-181","precheck_unconfigured_mc_instances",,Miscellaneous,"This checks the status of distributed peers to ensure all of them are properly configured to desired role within the Monitoring Console.","| rest splunk_server=local /services/search/distributed/peers
| search status=Up disabled=0 
| eval os = os_name 
| fields guid title peerName host host_fqdn server_roles search_groups cpu_arch os numberOfCores physicalMemoryMB version 
| rename title AS peerURI peerName AS serverName host_fqdn AS machine numberOfCores AS cpu_count physicalMemoryMB AS mem version AS splunk_version server_roles AS inherited_server_roles host AS instance
| where isnull(mvfind(search_groups,""dmc_group_""))
| appendpipe [| stats count | eval severity_level=if(count>0,3,0)] 
| eval value=count, limit=0
| eval severity_level=if(severity_level>0,3,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""precheck_unconfigured_mc_instances""
| `scma_summary_index`",0
"GMC-182",s2modecheck,,Miscellaneous,"check _internal index path to see if remotePath is configured. If yes, S2 is enabled, if not, it is Non-S2 stack.","| rest splunk_server=idx* /services/data/indexes/_internal datatype=all f=title f=remotePath f=disabled",0
"GMC-184","search_default_search_limits",,Miscellaneous,"Determines if any of the default search limits for max_searches_per_cpu or base_max_searches.","| rest splunk_server=* /servicesNS/-/-/configs/conf-limits
| fields splunk_server base_max_searches max_searches_per_cpu
| search base_max_searches=*
| eval default_base_max_searchces=6, default_max_searches_per_cpu=1
| eval status=if(base_max_searches>default_base_max_searches OR max_searches_per_cpu>default_max_searches_per_cpu,""!!Defaults Modified - Needs Reviewed!!"",""OK"")
| eval severity_level=if(status!=""OK"",2,0)
| fields - splunk_server
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_default_search_limits""
| `scma_summary_index`",1
"GMC-185","search_es_correlation_rules",,Miscellaneous,"Calculates the total number of ES correlation rules enabled in your environment. There is a limit of 60 ES correlation rules in Splunk Cloud.","| rest splunk_server=* count=0 /servicesNS/-/-/saved/searches 
| search ""action.correlationsearch.enabled""=1 AND disabled=""0"" is_scheduled=1
| fields title is_scheduled
|  stats dc(title) AS value
| eval limit=60
| eval severity_level=case(value>=60,2,value=0,-1,value>0 AND value<60,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_es_correlation_rules""
| `scma_summary_index`",0
"GMC-186","search_es_dma_limits",,Miscellaneous,"Calculates the total number of ES CIM or standard CIM data model accelerations enabled in your environment. There is a default limit of 9 ES CIM data model accelerations for Splunk Cloud.","| rest splunk_server=* /servicesNS/-/-/data/models
| search acceleration=1 
| rename acceleration.cron_schedule as cron title as datamodel eai:acl.app AS app 
| search app=""DA-ESS-ThreatIntelligence"" OR app=""SA-NetworkProtection"" OR app=""SA-ThreatIntelligence"" OR app=""SA-UEBA"" OR app=""Splunk_SA_CIM"" 
| stats dc(datamodel) AS num_data_models_es
| eval value=num_data_models_es, limit=9
| eval severity_level=case(num_data_models_es>=9,2,num_data_models_es=0,-1,num_data_models_es>0 AND num_data_models_es<9,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_es_dma_limits""
| `scma_summary_index`",0
"GMC-187","search_itsi_aggregation_limits",,Miscellaneous,"Calculates the total number of ITSI notable aggregation policies in your environment. There is a default limit of 15 ITSI aggregation policies for Splunk Cloud.","| rest splunk_server=* /servicesNS/nobody/SA-ITOA/event_management_interface/notable_event_aggregation_policy report_as=text 
| spath input=value 
| rename {}.disabled AS disabled {}.title AS title
| fields disabled title
| eval x=mvzip(title,disabled,""~"")
| mvexpand x 
| rex field=x ""(?<title>[^\~]+)\~(?<disabled>\d+)""
| search disabled=0
| stats count AS num_notable_event_aggregation_policies_itsi
| eval value=num_notable_event_aggregation_policies_itsi, limit=15
| eval severity_level=case(num_notable_event_aggregation_policies_itsi<=15 AND num_notable_event_aggregation_policies_itsi>0,0,num_notable_event_aggregation_policies_itsi>15,2,num_notable_event_aggregation_policies_itsi=0,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_itsi_aggregation_limits""
| `scma_summary_index`",0
"GMC-188","search_itsi_correlation_search_limits",,Miscellaneous,"Calculates the total number of ITSI correlation searches in your environment. There is a default limit of 15 ITSI correlation searches for Splunk Cloud.","| rest splunk_server=* /services/event_management_interface/correlation_search report_as=text 
| spath input=value 
| rename {}.name AS name, {}.search AS search 
| eval x=mvzip(name,search) 
| mvexpand x 
| eval x = split(x,"","") 
| eval name=mvindex(x,0) 
| eval search=mvindex(x,1) 
| stats dc(name) AS num_correlations_searches_itsi 
| eval value=num_correlations_searches_itsi, limit=15
| eval severity_level=case(num_correlations_searches_itsi<=15 AND num_correlations_searches_itsi>0,0,num_correlations_searches_itsi>15,2,num_correlations_searches_itsi=0,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_itsi_correlation_search_limits""
| `scma_summary_index`",0
"GMC-189","search_itsi_service_limits",,Miscellaneous,"Calculates the total number of ITSI services in your environment. There is a default limit of 5,000 ITSI services for Splunk Cloud.","| rest splunk_server=* /servicesNS/nobody/SA-ITOA/itoa_interface/service fields=""title"" report_as=text
| spath input=value
| rename {}.title AS service
| mvexpand service
| stats dc(service) AS num_services
| eval value=num_services, limit=5000
| eval severity_level=case(num_services<=5000 AND num_services>0,0,num_services>5000,2,num_services=0,-1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_itsi_service_limits""
| `scma_summary_index`",0
"GMC-190","search_long_running_searches",,Miscellaneous,"Identifies searches that are running for longer than 5 minutes over the last business week.","index=_audit action=search user=* user!=splunk-system-user search_id=* (info=granted OR info=completed) earliest=-6d@w1 latest=-1d@w6
| rex field=apiStartTime ""'(?<start_time>[^']+)'""
| rex field=apiEndTime ""'(?<end_time>[^']+)'"" 
| rex ""search\=(?<search_string>.+)\,\sautojoin""
| eval range=if(start_time==""ZERO_TIME"",""All Time"", tostring(strptime(end_time, ""%a %b %d %H:%M:%S %Y"") - strptime(start_time, ""%a %b %d %H:%M:%S %Y""),""duration""))
| eval range2=if(start_time==""ZERO_TIME"",""All Time"", strptime(end_time, ""%a %b %d %H:%M:%S %Y"") - strptime(start_time, ""%a %b %d %H:%M:%S %Y"")) 
| stats max(event_count) AS event_count values(start_time) AS earliest values(end_time) AS latest count values(range) AS range values(range2) AS range2 values(search) AS search values(user) AS User max(total_run_time) AS run_time(sec) by search_id savedsearch_name host 
| where count>1
| rename search_id AS SID range AS search_range run_time(sec) as run_time_sec
| search run_time_sec >= 300
| table savedsearch_name earliest latest search_range event_count run_time_sec
| sort 0 - run_time_sec
| appendpipe [| stats dc(run_time_sec) AS events]
| eval severity_level = case(run_time_sec>14400,""3"", run_time_sec>3600,""2"", run_time_sec>=300,""1"",events>=0,0,1=1,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_long_running_searches""
| `scma_summary_index`",0
"GMC-191","search_real_time_searches",,Miscellaneous,"Identifies any active real-time searches that are in the environment.","| rest /servicesNS/-/-/saved/searches splunk_server=* 
| fields eai:acl.app title dispatch.earliest_time dispatch.latest_time disabled
| search disabled=0 dispatch.earliest_time=rt* OR dispatch.latest_time=rt*
| stats count AS value
| eval severity_level=if(value>0,2,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_real_time_searches""
| `scma_summary_index`",0
"GMC-192","search_saved_search_inventory",,Miscellaneous,"Captures all enabled saved searches in Splunk to assist with building an inventory","| rest /servicesNS/-/-/saved/searches splunk_server=* 
| search disabled=0
| fields title eai:acl.app
| rename eai:acl.app AS app_context
| appendpipe [| stats dc(title) AS enabled_saved_searches]
| sort enabled_saved_searches
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_saved_search_inventory""
| `scma_summary_index`",1
"GMC-193","search_search_concurrency",,Miscellaneous,"Calculates the total search concurrency in the environment over the last 7 days.","index=_introspection data.search_props.sid::* component=PerProcess sourcetype=splunk_resource_usage earliest=-7d@d latest=@d 
| eval elapsed='data.elapsed', sid='data.search_props.sid', type='data.search_props.type' 
| bin span=30s _time 
| stats dc(sid) AS concurrency by _time 
| bin span=1d _time 
| stats avg(concurrency) AS average_daily_concurrency by _time 
| stats avg(average_daily_concurrency) AS total_concurrency_avg
| eval total_concurrency_avg=round(total_concurrency_avg,0)
| join 
[|inputlookup scma_customer_details 
| fields license_* 
| eval is_premium=if(license_es>0 OR license_pci>0 OR license_vmware>0 OR license_exchange>0,""premium_app_stack"",""core_only"") 
| join type=left license_core 
[| inputlookup override_lic.csv
| fields license_core concurrency_limit_core concurrency_limit_premium]] 
| eval severity_level=case((is_premium==""core_only"" AND total_concurrency_avg>concurrency_limit_core),2,(is_premium==""premium_app_stack"" AND total_concurrency_avg>concurrency_limit_premium),2,days_calculated=0,2,1==1,0) 
| eval limit=if(is_premium==""core_only"",concurrency_limit_core,concurrency_limit_premium)
| eval value=total_concurrency_avg
| eval _time=now()
| table _time is_premium total_concurrency_avg severity_level value limit
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_search_concurrency""
| `scma_summary_index`",0
"GMC-194","search_search_periodicity",,Miscellaneous,"This search is used to calculate search usage patterns over the last 7 days to determine how far back your users generally search. It is important to note that in Splunk Cloud and with SmartStore, searches over 90+ days will perform differently given the nature of SmartStore and how buckets are moved between local cache on the indexers and S3 buckets.","index=_audit search_id!=""'rsa_*"" search_id!=""'RemoteStorageRetrieveBuckets_*"" search_id!=""'searchparsetmp_*"" search_id!=""'remote_*"" search_id=* (info=completed OR info=cancelled) NOT mcatalog earliest=-7d@d latest=@d
| rex field=search_et ""'?(?<start_time>[^']+)'?"" 
| rex field=search_lt ""'?(?<end_time>[^']+)'?"" 
| eval range=if(start_time==""N/A"",""All Time"", (end_time-start_time)/86400)
| eval search_range_in_days=case(range==""All Time"",""ALL TIME"", range<1,""0-1"", range>=1 AND range<=7,""1-7"", range>7 AND range<=14,""7-14"", range>14 AND range<21,""14-21"", range>=21 AND range<90,""21-90"", range>90,""90+"") 
| eval search_range_in_days=if(match(search, ""^[\s\']*\|""), ""ALL TIME W/LEADING PIPE"", 'search_range_in_days')
| stats count by search_range_in_days 
| sort search_range_in_days 
| eventstats sum(count) as perc 
| eval perc=round(count*100/perc,2) 
| eval status=if((search_range_in_days=""90+"" OR search_range_in_days=""ALL TIME"") AND perc>20,""High Search Periodicity Over 90 Days"",""OK"")
| eval severity_level=if(status!=""OK"",2,0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_search_periodicity""
| `scma_summary_index`",0
"GMC-195","search_skipped_searches",,Miscellaneous,"Checks skipped searches over the last business week.","index=_internal source=*scheduler.log status=skipped earliest=-6d@w1 latest=-1d@w6
| eval name=if(match(savedsearch_name,""_ACCELERATE*""),""Acceleration"",savedsearch_name) 
| stats count by name,search_type app reason
| sort - count
| appendpipe [|stats dc(name) AS unique_skipped_searches sum(count) AS total_skipped_searches]
| eval value=total_skipped_searches
| eval severity_level=case(total_skipped_searches>0 AND total_skipped_searches<=10000,0,total_skipped_searches>10000 AND total_skipped_searches<=50000,2,total_skipped_searches>50000,3,1=1,0)
| eval _time = now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_skipped_searches""
| `scma_summary_index`",0
"GMC-196","search_svc_qualification_check",,Miscellaneous,"Checks current search activity and future ingest volume to determine candidacy for Splunk Cloud Workload Pricing.","index=_audit search_id!=""'rsa_*"" search_id!=""'RemoteStorageRetrieveBuckets_*"" search_id!=""'searchparsetmp_*"" search_id!=""'remote_*"" search_id=* total_run_time earliest=-7d@d latest=@d 
| eval host_sid = 'host'."":"".'search_id'
| bin _time span=1d
| stats dc(host_sid) AS search_count by _time
| stats avg(search_count) AS svc_search_count_daily_avg
| join
[ | inputlookup scma_customer_details
| fields license_*]
| eval is_premium=if(license_es>0 OR license_pci>0 OR license_vmware>0 OR license_exchange>0,""yes"",""no"")
| eval svc_recommendation=case(license_core < 2000,""Does not qualify at this data volume"", license_core>=2000 AND license_core < 5000,""Seek Deal Desk for approval for this volume"", license_core >= 5000,""Approved"")
| eval svc_utilization=case(svc_search_count_daily_avg <= 50000,""0%"",svc_search_count_daily_avg>50000 AND svc_search_count_daily_avg <= 100000,""25%"", svc_search_count_daily_avg>100000 AND svc_search_count_daily_avg<= 200000,""50%"", svc_search_count_daily_avg > 200000 AND svc_search_count_daily_avg <= 300000,""75%"",svc_search_count_daily_avg>300000,""100%"")
| eval severity_level=case(svc_recommendation==""Does not qualify at this data volume"",-1,svc_recommendation==""Seek Deal Desk for approval for this volume"",1,svc_recommendation==""Approved"",0)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_svc_qualification_check""
| `scma_summary_index`",0
"GMC-197","search_sweet_spot_check",,Miscellaneous,"This check identifies the selected data volumes and environments to determine if it is within the allowed default volumes for Splunk Cloud.","| inputlookup scma_customer_details
| fields license_* cloud_provider cloud_env
| makemv cloud_env 
| mvexpand cloud_env 
| foreach license_es license_exchange license_pci license_vmware license_itsi [ eval premium_apps=mvappend(premium_apps,if(<<FIELD>> >0,""<<FIELD>>"",null()))] 
| eval premium_apps=if(isnull(premium_apps),""none_none"",premium_apps)
| mvexpand premium_apps
| rename cloud_env AS environment cloud_provider AS provider 
| rex field=premium_apps ""\w+\_(?<premium_apps>\w+)""
| join environment premium_apps provider 
    [| inputlookup sweet_spot_matrix.csv 
    | eval environment_premium_app = 'environment' . ""-"" . 'premium_apps' ] 
| eval limit_value=case(premium_apps==""es"",license_es."":"".sweet_spot_ceiling,premium_apps==""pci"",license_pci."":"".sweet_spot_ceiling,premium_apps==""vmware"",license_vmware."":"".sweet_spot_ceiling,premium_apps==""exchange"",license_exchange."":"".sweet_spot_ceiling,premium_apps==""itsi"",license_itsi."":"".sweet_spot_ceiling,premium_apps==""none"",license_core."":"".sweet_spot_ceiling)
| eval check_sweet_spot_core_none=if(environment_premium_app != ""core-none"", null(),if((license_core < sweet_spot_floor OR license_core > sweet_spot_ceiling) AND license_core > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_core_es=if(environment_premium_app != ""core-es"", null(),if((license_es < sweet_spot_floor OR license_es > sweet_spot_ceiling) AND license_es > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_core_itsi=if(environment_premium_app != ""core-itsi"", null(),if((license_itsi < sweet_spot_floor OR license_itsi > sweet_spot_ceiling) AND license_itsi > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_core_vmware=if(environment_premium_app != ""core-vmware"", null(),if((license_vmware < sweet_spot_floor OR license_vmware > sweet_spot_ceiling) AND license_vmware > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_core_exchange=if(environment_premium_app != ""core-exchange"", null(),if((license_exchange < sweet_spot_floor OR license_exchange > sweet_spot_ceiling) AND license_exchange > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_core_pci=if(environment_premium_app != ""core-pci"", null(),if((license_pci < sweet_spot_floor OR license_pci > sweet_spot_ceiling) AND license_pci > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_hipaa_none=if(environment_premium_app != ""hipaa-none"", null(),if((license_core < sweet_spot_floor OR license_core > sweet_spot_ceiling) AND license_core > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_hipaa_es=if(environment_premium_app != ""hipaa-es"", null(),if((license_es < sweet_spot_floor OR license_es > sweet_spot_ceiling) AND license_es > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_hipaa_itsi=if(environment_premium_app != ""hipaa-itsi"", null(),if((license_itsi < sweet_spot_floor OR license_itsi > sweet_spot_ceiling) AND license_itsi > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_hipaa_vmware=if(environment_premium_app != ""hipaa-vmware"", null(),if((license_vmware < sweet_spot_floor OR license_vmware > sweet_spot_ceiling) AND license_vmware > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_hipaa_exchange=if(environment_premium_app != ""hipaa-exchange"", null(),if((license_exchange < sweet_spot_floor OR license_exchange > sweet_spot_ceiling) AND license_exchange > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_hipaa_pci=if(environment_premium_app != ""hipaa-pci"", null(),if((license_pci < sweet_spot_floor OR license_pci > sweet_spot_ceiling) AND license_pci > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_pci_none=if(environment_premium_app != ""pci-none"", null(),if((license_core < sweet_spot_floor OR license_core > sweet_spot_ceiling) AND license_core > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_pci_es=if(environment_premium_app != ""pci-es"", null(),if((license_es < sweet_spot_floor OR license_es > sweet_spot_ceiling) AND license_es > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_pci_itsi=if(environment_premium_app != ""pci-itsi"", null(),if((license_itsi < sweet_spot_floor OR license_itsi > sweet_spot_ceiling) AND license_itsi > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_pci_vmware=if(environment_premium_app != ""pci-vmware"", null(),if((license_vmware < sweet_spot_floor OR license_vmware > sweet_spot_ceiling) AND license_vmware > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_pci_exchange=if(environment_premium_app != ""pci-exchange"", null(),if((license_exchange < sweet_spot_floor OR license_exchange > sweet_spot_ceiling) AND license_exchange > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_pci_pci=if(environment_premium_app != ""pci-pci"", null(),if((license_pci < sweet_spot_floor OR license_pci > sweet_spot_ceiling) AND license_pci > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_fedramp_none=if(environment_premium_app != ""fedramp-none"", null(),if((license_core < sweet_spot_floor OR license_core > sweet_spot_ceiling) AND license_core > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_fedramp_es=if(environment_premium_app != ""fedramp-es"", null(),if((license_es < sweet_spot_floor OR license_es > sweet_spot_ceiling) AND license_es > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_fedramp_itsi=if(environment_premium_app != ""fedramp-itsi"", null(),if((license_itsi < sweet_spot_floor OR license_itsi > sweet_spot_ceiling) AND license_itsi > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_fedramp_vmware=if(environment_premium_app != ""fedramp-vmware"", null(),if((license_vmware < sweet_spot_floor OR license_vmware > sweet_spot_ceiling) AND license_vmware > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_fedramp_exchange=if(environment_premium_app != ""fedramp-exchange"", null(),if((license_exchange < sweet_spot_floor OR license_exchange > sweet_spot_ceiling) AND license_exchange > 0, ""Seek Deal Desk approval for large volumes."", ""Passed"")) 
| eval check_sweet_spot_fedramp_pci=if(environment_premium_app != ""fedramp-pci"", null(),if((license_pci < sweet_spot_floor OR license_pci > sweet_spot_ceiling) AND license_pci > 0, ""Seek Deal Desk approval for large volumes."", ""Passed""))
| fields check* limit_value
| foreach check_* [|eval <<FIELD>>=<<FIELD>>.""-"".limit_value]
| fields - limit_value
| stats first(*) AS *
| transpose column_name=sweet_spot_check
| rex field=""row 1"" ""(?<status>.+)\-(?<value>\d+)\:(?<limit>\d+)""
| eval severity_level=if(status!=""Passed"",3,0)
| eval _time=now()
| table sweet_spot_check status severity_level value limit _time
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""search_sweet_spot_check""
| `scma_summary_index`",0
"GMC-198","splunk-virtual-core-usage",,Miscellaneous,"","index=_introspection (host=sh* OR host=idx* OR host=idm* OR host=c0m1*) sourcetype=splunk_resource_usage component=Hostwide 
| join [| inputlookup svc ] 
| rename svc_license_type as License 
| eval 
    virtual_cores_per_svc = if(isnotnull(svc_indexer_multiplier) AND match(host, ""idx.*""), svc_indexer_multiplier, 2), 
    provisioned_svcs = 'data.virtual_cpu_count' / virtual_cores_per_svc, 
    user_svc = 'data.cpu_user_pct' * provisioned_svcs / 100, 
    total_svc = ('data.cpu_user_pct' + 'data.cpu_system_pct') * provisioned_svcs / 100 
| bin span=5m _time 
| rex field=host ""(?<role>\w+)-"" 
| stats max(user_svc) as splunk_svc 
    max(total_svc) as utilization 
    max(provisioned_svcs) as provisioned_svcs 
    max(License) as License by host _time role 
| stats sum(utilization) as utilization 
    max(License) as License 
    sum(provisioned_svcs) as Provisioned by _time role 
| eventstats sum(utilization) as total_util 
    sum(Provisioned) as total_prov by _time 
| stats max(utilization) as utilization 
    max(total_util) as total_util 
    max(License) as License 
    max(Provisioned) as Provisioned 
    max(total_prov) as total_prov by role 
| collect index=summary source=""splunk-virtual-core-usage""",0
"GMC-199","splunk_cities_geo_sh_csv_lookup_report",,Miscellaneous,"GMC City Database CSV Lookup Report","| inputlookup splunk_cities_geo_sh_csv_lookup 
| fields country region city region_code country_iso2 country_iso3 subregion latitude longitude 
| append 
    [| makeresults 
    | fields - _time 
    | eval country=""United States"" , region=""New York"" , city=""Stormville"", region_code=""NY"", country_iso2=""US"", country_iso3=""USA"", subregion=""Northern America"", latitude=""41.5704"", longitude=""-73.7454"" 
    | fields country region city region_code country_iso2 country_iso3 subregion latitude longitude ] 
| fillnull value=""Undefined"" region 
| stats Last(*) As * by country region cities
| eval region=if(region=""Undefined"", null(), region) 
| sort 0 country region city 
| table country region city region_code country_iso2 country_iso3 subregion latitude longitude 
| `gmc_comment(""| outputlookup splunk_cities_geo_sh_csv_lookup createinapp=true create_empty=true override_if_empty=false | stats count"")`",0
"GMC-208","splunk_rest_buckets_by_index","","Indexer_Cluster","","| rest /services/cluster/master/indexes `gmc_setup_cluster_master_rest` 
| stats last(splunk_server) as host max(index_size) as index_size max(num_buckets) as num_buckets max(total_excess_bucket_copies) as total_excess_bucket_copies max(total_excess_searchable_copies) as total_excess_searchable_copies by title 
| rename title as index_name 
| eval _time=now() 
| fields _time host index_name index_size num_buckets total_excess_bucket_copies total_excess_searchable_copies ",0
"GMC-209","splunk_rest_buckets_by_peer","","Indexer_Cluster","","| rest /services/cluster/master/peers `gmc_setup_cluster_master_rest` 
| fields label bucket_count_by_index* 
| untable label indexname bucketcount 
| rename label as host 
| rex mode=sed field=indexname ""s/bucket_count_by_index\.//"" 
| eval _time=now() 
| fields _time host indexname bucketcount 
| rename indexname AS index_name, bucketcount AS num_buckets ",0
"GMC-210","splunk_rest_cluster_status","","Indexer_Cluster","","| rest /services/cluster/master/generation/master `gmc_setup_cluster_master_rest` 
| fields num_buckets was_forced 
| appendcols 
    [| rest /services/cluster/master/health `gmc_setup_cluster_master_rest` 
    | fields all_data_is_searchable all_peers_are_up cm_version_is_compatible no_fixup_tasks_in_progress pre_flight_check replication_factor_met search_factor_met site_replication_factor_met site_search_factor_met] 
| appendcols 
    [| rest /services/cluster/master/status `gmc_setup_cluster_master_rest` 
    | fields splunk_server service_ready_flag searchable_rolling rolling_restart_or_upgrade rolling_restart_flag restart_progress.* maintenance_mode] 
| appendcols 
    [| rest /services/cluster/master/info `gmc_setup_cluster_master_rest` 
    | fields active_bundle.checksum, active_bundle.timestamp apply_bundle_status.status reload_bundle_issued last_validated_bundle.checksum last_validated_bundle.is_valid_bundle last_validated_bundle.timestamp latest_bundle.checksum latest_bundle.timestamp previous_active_bundle.checksum previous_active_bundle.timestamp] 
| eval _time=now() 
| eval host=splunk_server 
| rename active_bundle.checksum as active_bundle, last_validated_bundle.checksum as last_validated_bundle, latest_bundle.checksum as latest_bundle, num_buckets as total_buckets, previous_active_bundle.checksum as previous_active_bundle 
| fields _time host all_data_is_searchable,all_peers_are_up no_fixup_tasks_in_progress pre_flight_check cm_version_is_compatible search_factor_met, replication_factor_met site_search_factor_met site_replication_factor_met service_ready_flag searchable_rolling rolling_restart_or_upgrade rolling_restart_flag reload_bundle_issued restart_progress.* maintenance_mode active_bundle, active_bundle.timestamp apply_bundle_status.status last_validated_bundle last_validated_bundle.is_valid_bundle last_validated_bundle.timestamp latest_bundle latest_bundle.timestamp previous_active_bundle previous_active_bundle.timestamp was_forced total_buckets ",0
"GMC-211","splunk_rest_index_status","","Indexer_Cluster","","| rest /services/cluster/master/indexes `gmc_setup_cluster_master_rest` 
| fields title, is_searchable, replicated_copies_tracker*, searchable_copies_tracker*, num_buckets, index_size splunk_server 
| rename replicated_copies_tracker.*.* as rp**, searchable_copies_tracker.*.* as sb** 
| eval is_searchable = if((is_searchable == 1) or (is_searchable == ""1""), ""Yes"", ""No"") 
| eval index_size_gb = round(index_size / 1024 / 1024 / 1024, 0) 
| eval host=splunk_server 
| rename title AS index_name 
| eval _time=now() 
| fields _time host index_name is_searchable num_buckets index_size_gb rp* sb* ",0
"GMC-215","splunk_rest_kvstore_status","",Lookups,"","| rest /services/kvstore/status `gmc_setup_group_search_head_rest` 
| fields splunk_server current.disabled, current.port, current.replicationStatus , current.status, current.backupRestoreStatus 
| eval _time=now() 
| eval host=splunk_server 
| rename current.disabled as disabled,current.port as port, current.replicationStatus as replicationStatus , current.status as status, current.backupRestoreStatus as backupRestoreStatus 
| fields _time host backupRestoreStatus disabled port replicationStatus status",0
"GMC-216","splunk_rest_peer_status","","Indexer_Cluster","","| rest /services/cluster/master/peers `gmc_setup_cluster_master_rest` 
| eval _time=now() 
| rename label as host, active_bundle_id as active_bundle, latest_bundle_id as latest_bundle 
| fields _time host active_bundle last_validated_bundle latest_bundle apply_bundle_status.invalid_bundle.bundle_validation_errors apply_bundle_status.invalid_bundle.invalid_bundle_id apply_bundle_status.reasons_for_restart apply_bundle_status.restart_required_for_apply_bundle apply_bundle_status.status bucket_count heartbeat_started indexing_disk_space is_valid_bundle last_heartbeat pending_job_count primary_count primary_count_remote restart_required_for_applying_dry_run_bundle search_state_counter.Searchable search_state_counter.SearchablePendingMask status status_counter.Complete status_counter.StreamingSource status_counter.StreamingTarget summary_replication_count ",0
"GMC-217","splunk_rest_roles","",Identities,"","| rest /services/authorization/roles `gmc_setup_group_search_head_rest` 
| rename title as splunk_role, splunk_server as host 
| fields host splunk_role capabilities cumulativeRTSrchJobsQuota cumulativeSrchJobsQuota defaultApp deleteIndexesAllowed grantable_roles imported_capabilities imported_roles imported_rtSrchJobsQuota imported_srchDiskQuota imported_srchFilter imported_srchIndexesAllowed imported_srchIndexesDefault imported_srchJobsQuota imported_srchTimeWin rtSrchJobsQuota srchDiskQuota srchFilter srchIndexesAllowed srchIndexesDefault srchJobsQuota srchTimeWin 
| append 
    [| rest /services/authorization/roles `gmc_setup_cluster_master_rest` 
    | rename title as splunk_role, splunk_server as host 
    | fields host splunk_role capabilities cumulativeRTSrchJobsQuota cumulativeSrchJobsQuota defaultApp deleteIndexesAllowed grantable_roles imported_capabilities imported_roles imported_rtSrchJobsQuota imported_srchDiskQuota imported_srchFilter imported_srchIndexesAllowed imported_srchIndexesDefault imported_srchJobsQuota imported_srchTimeWin rtSrchJobsQuota srchDiskQuota srchFilter srchIndexesAllowed srchIndexesDefault srchJobsQuota srchTimeWin] 
| append 
    [| rest /services/authorization/roles `gmc_setup_idm_rest` 
    | rename title as splunk_role, splunk_server as host 
    | fields host splunk_role capabilities cumulativeRTSrchJobsQuota cumulativeSrchJobsQuota defaultApp deleteIndexesAllowed grantable_roles imported_capabilities imported_roles imported_rtSrchJobsQuota imported_srchDiskQuota imported_srchFilter imported_srchIndexesAllowed imported_srchIndexesDefault imported_srchJobsQuota imported_srchTimeWin rtSrchJobsQuota srchDiskQuota srchFilter srchIndexesAllowed srchIndexesDefault srchJobsQuota srchTimeWin] 
| eval capabilities=mvjoin(capabilities,"","")
    , imported_capabilities=mvjoin(imported_capabilities,"","")
    , deleteIndexesAllowed=mvjoin(deleteIndexesAllowed,"","")
    , grantable_roles=mvjoin(grantable_roles,"","")
    , imported_roles=mvjoin(imported_roles,"","")
    , imported_srchFilter=mvjoin(imported_srchFilter,"","")
    , imported_srchIndexesAllowed=mvjoin(imported_srchIndexesAllowed,"","")
    , imported_srchIndexesDefault=mvjoin(imported_srchIndexesDefault,"","")
    , srchFilter=mvjoin(srchFilter,"","")
    , srchIndexesAllowed=mvjoin(srchIndexesAllowed,"","")
    , srchIndexesDefault=mvjoin(srchIndexesDefault,"","") 
| nomv capabilities 
| nomv imported_capabilities 
| nomv deleteIndexesAllowed 
| nomv grantable_roles 
| nomv imported_roles 
| nomv imported_srchFilter 
| nomv imported_srchIndexesAllowed 
| nomv imported_srchIndexesDefault 
| nomv srchFilter 
| nomv srchIndexesAllowed 
| nomv srchIndexesDefault 
| eval _time=now() 
| fields _time host splunk_role *",0
"GMC-218","splunk_rest_saved_searches_sh_drift_monitoring",,Miscellaneous,"","| inputlookup splunk_rest_saved_searches_sh_monitoring_kv_store_lookup 
| table shcluster_label app savedsearch_name disabled is_scheduled cron_schedule auto_summarize dispatch_earliest_time dispatch_latest_time schedule_priority max_concurrent schedule_window allow_skew realtime_schedule 
| rename disabled As disabled_old is_scheduled As is_scheduled_old cron_schedule As cron_schedule_old auto_summarize As auto_summarize_old dispatch_earliest_time As dispatch_earliest_time_old dispatch_latest_time As dispatch_latest_time_old schedule_priority As schedule_priority_old max_concurrent As max_concurrent_old schedule_window As schedule_window_old allow_skew As allow_skew_old realtime_schedule As realtime_schedule_old 
| lookup splunk_rest_saved_searches_sh_kv_store_lookup shcluster_label app savedsearch_name OUTPUT is_scheduled As is_scheduled_new disabled As disabled_new cron_schedule As cron_schedule_new auto_summarize As auto_summarize_new dispatch_earliest_time As dispatch_earliest_time_new dispatch_latest_time As dispatch_latest_time_new schedule_priority As schedule_priority_new max_concurrent As max_concurrent_new schedule_window As schedule_window_new allow_skew As allow_skew_new realtime_schedule As realtime_schedule_new 
| where is_scheduled_old!=is_scheduled_new OR disabled_old!=disabled_new OR cron_schedule_old!=cron_schedule_new OR auto_summarize_old!=auto_summarize_new OR dispatch_earliest_time_old!=dispatch_earliest_time_new OR dispatch_latest_time_old!=dispatch_latest_time_new OR schedule_priority_old!=schedule_priority_new OR max_concurrent_old!=max_concurrent_new OR schedule_window_old!=schedule_window_new OR allow_skew_old!=allow_skew_new OR realtime_schedule_old!=realtime_schedule_new 
| join savedsearch_name 
    [ search index=_internal sourcetype=splunkd_ui_access method=POST uri=""*/saved/searches*"" NOT file IN (acl, dispatch, searches) earliest=-24h@h latest=now 
    | fields _time clientip file host status user user uri 
    | rex field=uri ""\N+servicesNS/[^/]+/(?<app>[^/]+)/[^/]+/[^/]+/(?<savedsearch_name>\N+)$"" 
    | rex field=savedsearch_name mode=sed ""s/%20/ /g"" 
    | fields savedsearch_name user ] 
| table shcluster_label app savedsearch_name user is_scheduled_old is_scheduled_new disabled_old disabled_new cron_schedule_old cron_schedule_new auto_summarize_old auto_summarize_new dispatch_earliest_time_old dispatch_earliest_time_new dispatch_latest_time_old dispatch_latest_time_new schedule_priority_old schedule_priority_new max_concurrent_old max_concurrent_new schedule_window_old schedule_window_new allow_skew_old allow_skew_new realtime_schedule_old realtime_schedule_new",0
"GMC-219","splunk_rest_shcluster_status","","Search_Head_Cluster","","| rest /services/shcluster/captain/info `gmc_setup_shc_rest` 
| fields splunk_server elected_captain label maintenance_mode initialized_flag min_peers_joined_flag rolling_restart_flag 
| rename splunk_server as host, label as current_captain 
| append 
    [| rest /services/shcluster/captain/members `gmc_setup_shc_rest` 
    | fields advertise_restart_required is_captain label last_heartbeat no_artifact_replications pending_job_count preferred_captain replication_count status title 
    | rename label as host, title as guid] 
| stats values(elected_captain) as elected_captain values(current_captain) as current_captain values(maintenance_mode) as maintenance_mode values(initialized_flag) as initialized_flag values(min_peers_joined_flag) as min_peers_joined_flag values(rolling_restart_flag) as rolling_restart_flag values(advertise_restart_required) as advertise_restart_required values(guid) as guid values(is_captain) as is_captain values(last_heartbeat) as last_heartbeat values(no_artifact_replications) as no_artifact_replications values(pending_job_count) as pending_job_count values(preferred_captain) as preferred_captain values(replication_count) as replication_count values(status) as status by host 
| eval _time=now() 
| fields _time host *",0
"GMC-220","splunk_rest_users","",Identities,"","| rest /services/authentication/users `gmc_setup_search_head_search` 
| fields splunk_server title roles type realname locked-out last_successful_login email tz 
| append 
    [| rest /services/authentication/users `gmc_setup_cluster_master_rest` 
    | fields splunk_server title roles type realname locked-out last_successful_login email tz] 
| append 
    [| rest /services/authentication/users `gmc_setup_idm_rest`
    | fields splunk_server title roles type realname locked-out last_successful_login email tz] 
| eval _time=now() 
| rename splunk_server as host, title as splunk_user 
| eval roles=mvjoin(roles,"","") 
| nomv roles 
| fields _time host *",0
"GMC-221","splunk_saved_searches_drift_alert",,Miscellaneous,"Monitor for changes made to any or specific (configurable) Saved Searches changes in the environment and send an automated Alert via GMC.","index=_internal sourcetype=splunkd_ui_access method=POST uri=""*/saved/searches*"" NOT file IN (acl, dispatch, searches) 
| fields - useragent 
| fields _time clientip file host status user user uri 
| rex field=uri ""\N+servicesNS/[^/]+/(?<app>[^/]+)/[^/]+/[^/]+/(?<savedsearch_name>\N+)$"" 
| search savedsearch_name IN (""*"") `gmc_comment(""Add any specific Reports/Alerts you like to monitor for or leave to monitor for all changes"")` 
| `get_shcluster_label(host)` 
| `get_saved_searches_info(shcluster_label,app,savedsearch_name)` 
| fields _time shcluster_label clientip file host app status user uri savedsearch_search cron_schedule is_scheduled schedule_priority schedule_window action_email_to allow_skew auto_summarize realtime_schedule savedsearch_type savedsearch_name dispatch_earliest_time dispatch_latest_time action_email updated 
| rename 
    disabled AS old_disabled
    updated as old_updated
    cron_schedule as old_cron_schedule 
    is_scheduled as old_is_scheduled 
    dispatch_earliest_time as old_dispatch_earliest_time
    dispatch_latest_time as old_dispatch_latest_time 
    action_email as old_action_email 
    action_email_to as old_action_email_to
    schedule_priority as old_schedule_priority 
    schedule_window as old_schedule_window
    action_email_to as old_action_email_to
    allow_skew as old_allow_skew
    auto_summarize as old_auto_summarize
    realtime_schedule as old_realtime_schedule 
    savedsearch_search as old_savedsearch_search 
    savedsearch_type as old_savedsearch_type 
| fields shcluster_label app savedsearch_name old_* _time clientip file host status user uri 
| join shcluster_label app savedsearch_name 
    [| rest /servicesNS/-/-/saved/searches splunk_server_group=dmc_group_search_head timeout=0 
    | eval savedsearch_type=if((NOT 'action'==""*"" AND NOT alert.track==""*"" AND NOT alert_condition==""*"" AND 'alert_type'==""always""),""report"",""alert"") 
    | rename eai:acl.app AS app, splunk_server AS Splunk_Instance, title AS savedsearch_name, alert.severity AS alert_severity, alert.track AS alert_track, dispatch.earliest_time AS dispatch_earliest_time, dispatch.latest_time AS dispatch_latest_time , eai:acl.sharing as sharing , eai:acl.removable as removable action.email.to AS action_email_to 
    | rename action.email AS action_email action.logevent AS action_logevent action.logevent.param.event AS action_logevent_param_event action.lookup AS action_lookup action.lookup.append AS action_lookup_append action.lookup.filename AS action_lookup_filename action.output_message AS action_output_message action.output_message.param.msgid AS action_output_message_param_msgid action.output_message.param.name AS action_output_message_param_name action.populate_lookup AS action_populate_lookup action.script AS action_script action.script.filename AS action_script_filename action.summary_index AS action_summary_index auto_summarize.cron_schedule AS auto_summarize_cron_schedule auto_summarize.max_concurrent AS auto_summarize_max_concurrent dispatch.ttl AS dispatch_ttl auto_summarize.dispatch.earliest_time AS auto_summarize_dispatch_earliest_time auto_summarize.dispatch.latest_time AS auto_summarize_dispatch_latest_time action.correlationsearch.enabled AS action_correlationsearch_enabled action.correlationsearch.label AS action_correlationsearch_label action.correlationsearch.related_searches AS action_correlationsearch_related_searches 
    | `strptime_format(updated)` 
    | `get_shcluster_label(Splunk_Instance)` 
    | fields shcluster_label app savedsearch_name updated disabled is_scheduled cron_schedule auto_summarize dispatch_earliest_time dispatch_latest_time schedule_priority schedule_window allow_skew realtime_schedule action_email_to search action_email savedsearch_type 
    | stats 
        last(*) AS * 
        BY shcluster_label app savedsearch_name 
    | rename 
        cron_schedule as new_cron_schedule 
        is_scheduled as new_is_scheduled
        dispatch_earliest_time as new_dispatch_earliest_time 
        dispatch_latest_time as new_dispatch_latest_time 
        action_email as new_action_email 
        action_email_to as new_action_email_to
        schedule_priority as new_schedule_priority
        schedule_window as new_schedule_window 
        allow_skew as new_allow_skew
        auto_summarize as new_auto_summarize 
        realtime_schedule as new_realtime_schedule 
        search as new_savedsearch_search
        savedsearch_type as new_savedsearch_type 
        updated AS new_updated
        disabled as new_disabled 
    | fields shcluster_label app savedsearch_name new_*] 
| stats values(*) as * latest(_time) as _time by shcluster_label app savedsearch_name user 
| convert ctime(*_updated) 
| table _time user clientip status host shcluster_label app savedsearch_name 
    old_savedsearch_type new_savedsearch_type
    old_updated new_updated
    old_disabled new_disabled
    old_cron_schedule new_cron_schedule
    old_is_scheduled new_is_scheduled 
    old_dispatch_earliest_time new_dispatch_earliest_time
    old_dispatch_latest_time new_dispatch_latest_time
    old_action_email new_action_email
    old_action_email_to new_action_email_to
    old_action_email_to new_action_email_to
    old_schedule_priority new_schedule_priority
    old_schedule_window new_schedule_window
    old_allow_skew new_allow_skew
    old_auto_summarize new_auto_summarize
    old_realtime_schedule new_realtime_schedule
    old_savedsearch_search new_savedsearch_search",0
"GMC-225","users_admin_users",,Miscellaneous,"This check determines if there are a high number of users (>10%) with the admin role.","| rest /servicesNS/-/-/authentication/users splunk_server=* 
| top roles 
| rename count AS users 
| eval value=users, limit=0
| search roles=""admin"" 
| eval status=if(percent>10,""Large Number of Admin Users - Review User Roles and Admin Role Differences in Splunk Cloud (sc_admin)"",""OK"") 
| eval severity_level=if(status=""OK"",0,1)
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""users_admin_users""
| `scma_summary_index`",0
"GMC-226","users_authentication_mechanism",,Miscellaneous,"This check determines which authentication method is currently used to authenticate users in the environent.","| rest splunk_server=* /services/authentication/providers/services/active_authmodule | fields active_authmodule | eval severity_level=if(active_authmodule=""Splunk"",-1,1) | eval _time=now() | join [|inputlookup scma_customer_details | table customer_name] | `scma_drop_fields` | eval check_name=""users_authentication_mechanism"" | `scma_summary_index`",1
"GMC-227","users_total_users",,Miscellaneous,"Checks the distinct number of users logged into Splunk over the last 30 days.","index=_audit sourcetype=audittrail ""login attempt"" info=succeeded earliest=-30d@d 
| stats dc(user) AS distinct_users_authenticated 
| eval severity_level=0
| eval _time=now()
| join [|inputlookup scma_customer_details | table customer_name]
| `scma_drop_fields`
| eval check_name=""users_total_users""
| `scma_summary_index`",1
"GMC-000","Adding Searches to GMC 101 DB",101,SPL,"","| from lookup:splunk_rest_saved_searches_sh_kv_store_lookup 
| search app=""gmc_backup"" 
| streamstats count 
| eval count=count+114 
| eval UC_Counter = printf(""%03d"", count) 
| eval app = ""GMC-"" . UC_Counter 
| rename savedsearch_name As view description as text savedsearch_search as app_version 
| fields app, view, panel, classification, text, app_version, disabled 
```| outputlookup help_entries createinapp=true create_empty=true override_if_empty=false append=true ```",0
"GMC-228","Find Future Time Stamps by index","_time","Indexer_Cluster","","| rest splunk_server_group=dmc_group_indexer /services/data/indexes search=""totalEventCount!=0"" 
| eval coldPath.maxDataSizeMB=if('coldPath.maxDataSizeMB' = 0, null(), 'coldPath.maxDataSizeMB') 
| eval homePath.maxDataSizeMB=if('homePath.maxDataSizeMB' = 0, null(), 'homePath.maxDataSizeMB') 
| eval roof=min((coalesce('homePath.maxDataSizeMB', 4294967295) + coalesce('coldPath.maxDataSizeMB', 4294967295)), maxTotalDataSizeMB) 
| eval span=tostring(currentDBSizeMB) + "" / "" + tostring(roof) + "" MB"" 
| eval PercentFull=tostring(round(currentDBSizeMB * 100 / roof)) + ""%"" 
| eval ""Total Events""=tostring(totalEventCount,""commas"") 
| stats first(span) AS ""Capacity vs Limit"" by splunk_server title minTime maxTime ""Total Events"" PercentFull 
| rename splunk_server AS Indexer title AS Index minTime AS ""Oldest Event"" maxTime AS ""Newest Event"" 
| table Indexer Index ""Capacity vs Limit"" ""Oldest Event"" ""Newest Event"" ""Total Events"" PercentFull 
| sort - PercentFull",0
"GMC-229","mcatalog command example",mcatalog,Commands,"","| mcatalog 
    values(_dims) AS Dimension 
    WHERE metric_name IN (spl.mlog.searchscheduler.*, spl.mlog.search_concurrency.*, spl.mlog.realtime_search_data.*, spl.mlog.search_health_metrics.*, spl.mlog.search_pool.*) 
    AND index=_metrics 
    BY metric_name 
| rex field=metric_name ""spl.mlog.(?<group>\w+).(?<name>\w+).?(?<metric>\w+)?"" 
| table metric_name Dimension group name metric",0
"GMC-230","mstats command example",mstats,Commands,"","| mstats sum(_value) AS Total 
    WHERE index=_metrics 
    AND metric_name=spl.mlog.search_concurrency.search_queue_metrics.enqueue_seaches_count 
    BY group metric_name",0
"GMC-231","msearch command examples",msearch,Commands,"The msearch command is used to return raw data-points in a metrics index.
It is NOT designed to work at scale since returning every metric event over a huge data set is prohibitively expensive.  Having said that, there are many use cases and workflows where seeing raw data points in the metrics index can be helpful, mostly around GDI/data onboarding/data discovery use cases.

The filter predicate can contain predicates involving dimensions or metric_name ONLY.
To constrain _time, one has to do it via API (TimePicker or specifying -[earliest/latest] as argument in CLI)
 
The index/es must be metrics indexes.  Only Quake version buckets are processed,
old buckets that are pre-Quake will be silently ignored by command.

Examples:
| msearch index=_metrics
| msearch index=_metrics filter=""metric_name=cpu.idle""
| msearch index=_metrics filter=""metric_name=cpu.idle host=ronnie app=splunk""
| msearch index=* filter=""app=slack""","| msearch index=_metrics earliest=-1h@h latest=-1m@m filter=""metric_name=cpu.idle host=ronnie app=splunk""",0
"GMC-232","GMC Data Model Info","","Data_Models","","| datamodel GMC 
| spath output=calculations objects{}.calculations{} 
| stats count by calculations 
| rex field=calculations ""\""owner\"":\""(?<Dataset>.*?)\"""" 
| rex field=Dataset mode=sed ""s/Search_Activity.//g"" 
| rex field=calculations ""\""displayName\"":\""(?<Field_Display_Name>.*?)\"""" 
| rex field=calculations ""\""fieldName\"":\""(?<Field_Name>.*?)\"""" 
| rex field=calculations ""\""expression\"":\""(?<Eval_Expression>.*?)\""}"" 
| rex field=Eval_Expression ""(ISNUM|isnull|ISNULL|match)\s*\('?(data)?\.?(search_props)?\.?(?<Org_Field_Name>\w+)"" 
| rex field=Eval_Expression mode=sed ""s/\\\n/\n/g"" 
| eval Org_Field_Name=if(Dataset=""Internal_Scheduler"" AND (Field_Display_Name=""Status"" OR Field_Display_Name=""info""), ""status"", Org_Field_Name) 
| sort 0 Dataset Field_Display_Name 
| table Dataset Field_Display_Name Field_Name Org_Field_Name Eval_Expression",0
"GMC-233","User browser name and version information","users,ui",Usage,"","index=_internal sourcetype=splunkd_ui_access useragent=* status=200 
| rex field=useragent ""(?<Browser_and_Version>(MSIE|Trident|(?!Gecko.+)Firefox|(?!AppleWebKit.+Chrome.+)Safari(?!.+Edge)|(?!AppleWebKit.+)Chrome(?!.+Edge)|(?!AppleWebKit.+Chrome.+Safari.+)Edge|AppleWebKit(?!.+Chrome|.+Safari)|Gecko(?!.+Firefox))(?: |\/)([\d\.apre]+)|Splunk.*?\s)"" 
| rex field=useragent ""(Version/|iPhone\sOS\s)(?<Safari_Version>.*?)\s"" 
| rex field=Browser_and_Version ""(?<Browser_Name>.*?)/"" 
| rex field=Browser_and_Version ""/(?<Browser_Version>.*?)$"" 
| rex field=useragent ""(?<Architecture>(Win64|x86_64))"" 
| rex field=useragent ""(?<OS_Name>(Linux|Macintosh|Windows\s*\w+|iPhone|iPad))"" 
| rex field=useragent ""(Linux\s|Macintosh;\s|i\w+;\s|Windows\sNT\s)(?<OS_Version>.*?)[;)]"" 
| eval Browser_Version = if(isnotnull(Safari_Version), Safari_Version, Browser_Version) 
| fillnull value=""unknown"" 
| fields clientip OS_Name OS_Version Browser_Name Browser_Version Architecture 
| lookup cim_http_status_lookup status OUTPUT status_action status_description status_type 
| stats values(*) as * count by clientip Browser_Name Browser_Version OS_Name OS_Version Architecture 
| table clientip Browser_Name Browser_Version OS_Name OS_Version Architecture Architecture count",0
"GMC-234B","Health info Detailed",rest,Health,"","| rest splunk_server_group=dmc_group_indexer /servicesNS/-/-/admin/health-report/splunkd/details 
| fields title splunk_server health features.* 
| rename features.* As *",0
"GMC-234A","Health info Basic",rest,Health,"","| rest splunk_server_group=dmc_group_indexer /servicesNS/-/-/admin/health-report 
| fields title splunk_server health",0
"GMC-235","Concurrency Limits",rest,"Search_Head_Cluster","","| rest splunk_server_group=dmc_group_indexer /servicesNS/-/-/admin/server-status-limits-concurrency 
| table splunk_server title max_*",0
"GMC-236","Report on users and their group memberships so we can make decisions how to classify the groups","",Identities,"| tstats SUMMARIESONLY=TRUE ALLOW_OLD_SUMMARIES=TRUE 
    VALUES(Authentication.app) AS app
    count 
    FROM DATAMODEL=Authentication 
    WHERE NODENAME=Authentication 
    AND Authentication.app IN (win:batch, win:service)
    AND sourcetype = ""XmlWinEventLog""
    BY  Authentication.user 
| `drop_dm_object_name(Authentication)`","index=appmsadmon sourcetype=ActiveDirectory objectCategory=""CN=Person,CN=Schema,CN=Configuration*"" 
| makemv delim=""|"" objectClass 
| eval User_Groups = memberOf 
| makemv delim=""|"" User_Groups 
| fields sAMAccountName distinguishedName displayName userPrincipalName memberOf DC description lastLogon whenCreated whenChanged User_Groups 
| stats values(*) as * by sAMAccountName 
| table sAMAccountName displayName userPrincipalName DC User_Groups lastLogon whenCreated whenChanged",0
"GMC-237","Extract Search SPL from Audit & Internal & Format it",searches,Search,"","index=_audit sourcetype=audittrail action=search info IN (completed) search=* 
| eval Raw=_raw 
| rex mode=sed field=Raw ""s/[\n\r]//g"" 
| rex mode=sed field=Raw ""s/\s{2,}/ /g"" 
| rex field=Raw ""search\=\'(?<Search_Job>.*?)',\s"" 
| rex mode=sed field=Search_Job ""s/^search\s+//g"" 
| rex mode=sed field=Search_Job ""s/REST/rest/g"" 
| rex mode=sed field=Search_Job ""s/^rest/| rest/g"" 
| rex mode=sed field=Search_Job ""s/^\|rest/| rest/g"" 
| rex mode=sed field=Search_Job ""s/^\|inputlookup /| inputlookup /g"" 
| eval Search_Job=trim(Search_Job) 
| stats count by Search_Job 
| append 
    [ search index=_internal sourcetype=splunkd component=DispatchManager id=* search=* 
    | eval Raw=_raw 
    | rex mode=sed field=Raw ""s/[\n\r]//g"" 
    | rex mode=sed field=Raw ""s/\s{2,}/ /g"" 
    | rex mode=sed field=Raw ""s/\\\n//g"" 
    | rex field=Raw ""search=\""search\s+(?<Search_Job>.*?)$"" 
    | rex mode=sed field=Search_Job ""s/\"",\sreason=.*?$//g"" 
    | eval Search_Job=trim(Search_Job) 
    | stats count by Search_Job ] 
| rex mode=sed field=Search_Job ""s/\|/\n|/g"" 
| eval Search_Job=trim(Search_Job) 
| rex mode=sed field=Search_Job ""s/^rest/| rest/g"" 
| stats count by Search_Job 
| fields - count",0
"GMC-238","Using eval in stats","stats,eval",SPL,"count as num_data_samples max(eval(if(_time >= relative_time(maxtime, ""-1d@d""), count, null))) as ""count"" avg(eval(if(_time<relative_time(maxtime, ""-1d@d""), count,null))) as avg stdev(eval(if(_time<relative_time(maxtime, ""-1d@d""), count, null))) as stdev by ""dest""

count as total, count(eval('summary.complete'<1 AND 'summary.is_inprogress'=0)) as enabled

count sparkline as trend sum(eval(action=""success"")) as success_count sum(eval(action=""failure"")) as failure_count values(Workstation_Name) as Workstations earliest(_time) as earliestTime latest(_time) as latestTime by User, src_ip

count sparkline sum(eval(AUT_code=""AUT24326"")) as success_count sum(eval(AUT_code=""AUT24327"")) as failure_count 

values(ua) as ua count(eval(match(signature_id,""(?i)fclog0[^1]""))) as bad_login count(eval(match(signature_id,""(?i)fclog01""))) as locked_login","avg(bytes_out) as avg_bytes_out sum(eval(if(action=""allowed"",bytes_out,0))) as bytes_allowed sum(eval(if(action=""blocked"",bytes_out,0))) as bytes_blocked sum(eval(if(action=""allowed"",packets_out,0))) as packets_allowed sum(eval(if(action=""blocked"",packets_out,0))) as packets_blocked earliest(_time) as _time by src_ip dest_ip app protocol",0
"GMC-239","Report on Forwarder Connection Types",fwd,"Universal_Forwarders","","index=_internal source=*metrics.log group=tcpin_connections 
| eval sourceHost=if(isnull(hostname), sourceHost,hostname) 
| eval connectType=case(fwdType==""uf"",""univ fwder"", fwdType==""lwf"", ""lightwt fwder"",fwdType==""full"", ""heavy fwder"", connectType==""cooked"" or connectType==""cookedSSL"",""Splunk fwder"", connectType==""raw"" or connectType==""rawSSL"",""legacy fwder"") 
| eval version=if(isnull(version),""pre 4.2"",version) 
| rename version as Ver 
| fields connectType connectionType sourceIp sourceHost destPort kb tcp_eps tcp_Kprocessed tcp_KBps splunk_server Ver 
| eval Indexer= splunk_server 
| eval Hour=relative_time(_time,""@h"") 
| stats avg(tcp_KBps) sum(tcp_eps) sum(tcp_Kprocessed) sum(kb) by Hour connectType connectionType sourceIp sourceHost destPort Indexer Ver 
| fieldformat Hour=strftime(Hour,""%x %H"")",0
"GMC-240","Users with Index access",users,Identities,"","| rest /services/authentication/users 
| table title roles 
| rename title as user 
| mvexpand roles 
| join type=left roles 
    [ rest /services/authorization/roles 
    | table title srchIndexesAllowed srchIndexesDefault 
    | rename title as roles] 
| makemv srchIndexesAllowed tokenizer=(\S+) 
| makemv srchIndexesDefault tokenizer=(\S+) 
| fillnull value="" "" 
| mvexpand srchIndexesAllowed 
| mvexpand srchIndexesDefault 
| join type=left max=999 srchIndexesAllowed 
    [ rest /services/data/indexes 
    | table title 
    | eval srchIndexesAllowed = if(match(title, ""^_""), ""_*"", ""*"") 
    | rename title as IndexesAllowed] 
| join type=left max=999 srchIndexesDefault 
    [ rest /services/data/indexes 
    | table title 
    | eval srchIndexesDefault = if(match(title, ""^_""), ""_*"", ""*"") 
    | rename title as IndexesDefault] 
| stats values(*) as * by user 
| foreach srch* 
    [ eval <<FIELD>> = mvappend(<<FIELD>>, <<MATCHSTR>>) 
    | eval <<FIELD>> = mvfilter(match(<<FIELD>>, ""^[^*]+$""))] 
| fields - Indexes*",0
"GMC-241","Data Sources Mapped to Indexes","","Data_Onboarding","","| rest /services/data/indexes count=0 
| dedup title 
| fields title 
| map 
    [| metadata type=sourcetypes index=""$title$"" 
    | eval type=""$title$""] maxsearches=1000 
| stats values(totalCount) AS EventCount values(sourcetype) AS Sourcetype by type 
| rename type as index 
| fields index Sourcetype EventCount",0
"GMC-242","Incomplete Data Model Accelerations","","Data_Models","","| rest /services/admin/summarization by_tstats=t splunk_server=local count=0 
| stats count as total, count(eval('summary.complete'<1 AND 'summary.is_inprogress'=0)) as enabled 
| eval op = enabled . ""/"" . total 
| fields op, enabled",0
"GMC-095","Bundle Replication attempts",bundle,"Search_Head_Cluster","","index=_internal source=*metrics.log group=bundle* total_bytes!=0 
| stats sum(success_count), sum(total_count) by host, group 
| rename sum(total_count) AS ""Total Attempts"", sum(success_count) AS ""Successful Attempts"", host as ""Instance"", group as ""Transfer Type""",0
"GMC-096","Number of Scheduled Searches set to run each minute","","Scheduled_Jobs","","| rest /servicesNS/-/-/saved/searches splunk_server=local
| regex cron_schedule=""\*\s\*\s\*\s\*$""
| search is_scheduled=1 disabled=0
| eval cron_schedule=if(title=""test1 Clone"",""1,12,24,36,48 * * * *"",cron_schedule)
| fields title cron_schedule
| rex field=cron_schedule ""^(?<minute_position>(?<first_minute>[^-\/\s,]+)(?:[^\/]*\/(?<divisor>\S+))?\S*)""
| eval first_minute=case(match(minute_position,""^\*""),""0"",match(cron_schedule,"",""),split(minute_position,"",""),1=1,first_minute)
| mvexpand first_minute
| eval divisor=if(match(minute_position,""\*$""),""1"",divisor)
| eval multiplier=round(60/divisor,2)
| eval trimmer=floor(60-multiplier)
| eval rex=""^.{"".trimmer.""}(.*)""
| eval job_run_count=split(replace(""||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"",rex,""\1""),"""")
| fillnull value="" | "" job_run_count
| eval count=1
| mvexpand job_run_count
| streamstats sum(count) AS count by title
| eval scheduled_min=if(isnull(multiplier),first_minute,floor((60/multiplier)*(count-1)+first_minute)),scheduled_min=if(len(scheduled_min)=1,""0"".scheduled_min,scheduled_min)
| where scheduled_min < 60
| stats count(cron_schedule) by scheduled_min",0
"GMC-243","Saved Searches with Suggested Cron Schedule","","Scheduled_Jobs","","| rest splunk_server=local /servicesNS/-/-/saved/searches 
| search is_scheduled=1 disabled=0 
| fields search title author cron_schedule eai:acl.app eai:acl.sharing dispatch.earliest_time dispatch.latest_time action.summary_index search 
| search NOT (dispatch.earliest_time=rt* OR dispatch.latest_time=rt* OR action.summary_index=1 OR search=*timechart* OR search=bin OR search=span OR search=*bucket*) 
| eval fudge_author = case(
    'eai:acl.sharing' == ""user"", 'author',
    'eai:acl.sharing' == ""global"", ""nobody"",
    'eai:acl.sharing' == ""app"", ""nobody"") 
| eval cron_type=case
    (match(cron_schedule,""\*/5 \* \* \* \*""),5,
    match(cron_schedule,""\*/10 \* \* \* \*""),10,
    match(cron_schedule,""\*/15 \* \* \* \*""),15,
    match(cron_schedule,""\*/30 \* \* \* \*""),30,
    match(cron_schedule,""^0 \* \* \* \*""),0,0=0,-1) 
| where cron_type >= 0 
| sort cron_type 
| eval reset_count=case(cron_type=5,4,
    cron_type=10,9,
    cron_type=15,14,
    cron_type=30,29,
    cron_type=0,59
    ) 
| streamstats count reset_after=""count=reset_count"" by cron_type 
| eval lower_bound=count 
| eval upper_bound=59 
| eval new_cron=case
    (
    cron_type=0,count . "" * * * *"",
    cron_type!=0,lower_bound . ""-"" . upper_bound . ""/"" . cron_type . "" * * * *""
    ) 
| eval encoded_title=title 
| rex field=encoded_title mode=sed
    ""s:%:%25:g s:\+:%2B:g s:\"":%22:g s:\(:%28:g s:\):%29:g s: :%20:g  s:<:%3C:g  s:>:%3E:g  s:#:%23:g  s:{:%7B:g  s:}:%7D:g  s:\|:%7C:g s:\\\:%5C:g  s:\^:%5E:g  s:~:%7E:g
     s:\[:%5B:g  s:\]:%5D:g  s:\`:%60:g  s:;:%3B:g  s:/:%2F:g  s:\?:%3F:g  s/:/%3A/g  s:@:%40:g  s:=:%3D:g  s:&:%26:g  s:\$:%24:g  s:\!:%21:g  s:\*:%2A:g"" 
| fields title author dispatch.earliest_time dispatch.latest_time cron_schedule new_cron eai:acl.app eai:acl.sharing",0
"GMC-108","Log Lags",lag,"Data_Onboarding","","index=* sourcetype=* 
| eval time=_time 
| eval itime=_indextime 
| eval lag=(itime-time)/60 
| stats AVG(lag) MIN(lag) MAX(lag) MEDIAN(lag) by host,sourcetype",0
"GMC-097","Users Browser & Location Info based on their Client IP","",Usage,"","index=_internal sourcetype=splunkd_ui_access useragent=* status=200 user!=""-"" 
| eval user=lower(trim(user)) 
| rex field=useragent ""(?<Browser_and_Version>(MSIE|Trident|(?!Gecko.+)Firefox|(?!AppleWebKit.+Chrome.+)Safari(?!.+Edge)|(?!AppleWebKit.+)Chrome(?!.+Edge)|(?!AppleWebKit.+Chrome.+Safari.+)Edge|AppleWebKit(?!.+Chrome|.+Safari)|Gecko(?!.+Firefox))(?: |\/)([\d\.apre]+)|Splunk.*?\s)"" 
| rex field=useragent ""(Version/|iPhone\sOS\s)(?<Safari_Version>.*?)\s"" 
| rex field=Browser_and_Version ""(?<Browser_Name>.*?)/"" 
| rex field=Browser_and_Version ""/(?<Browser_Version>.*?)$"" 
| rex field=useragent ""(?<Architecture>(Win64|x86_64))"" 
| rex field=useragent ""(?<OS_Name>(Linux|Macintosh|Windows\s*\w+|iPhone|iPad))"" 
| rex field=useragent ""(Linux\s|Macintosh;\s|i\w+;\s|Windows\sNT\s)(?<OS_Version>.*?)[;)]"" 
| eval 
    Browser_Version = if(isnotnull(Safari_Version), Safari_Version, Browser_Version), 
    Browser_Name=if(Browser_Name=""Trident"", ""Internet Explorer"", Browser_Name), 
    OS_Name = case ( 
    match(useragent,""Windows NT 6.1""), ""Windows 7"", 
    match(useragent,""Windows NT 6.2""), ""Windows 8"", 
    match(useragent,""Windows NT 6.3""), ""Windows 8.1"", 
    match(useragent,""Windows NT 10.0""), ""Windows 10"", 
    match(useragent,""Pixel""), ""Google Pixel"",
    match(useragent,""Android""), ""Android"", 
    true(), OS_Name ) 
| fields _time clientip OS_Name OS_Version Browser_Name Browser_Version user 
| `get_iplocation_info(clientip)` 
| stats 
    values(clientip) as Login_Source
    values(OS_Name) as OS_Name
    values(OS_Version) as OS_Version
    values(Browser_Name) as Browser_Name
    values(Browser_Version) as Browser_Version
    dc(clientip) as Num_Login_Sources
    Values(work_city) As work_city
    Values(work_region) As work_region
    Values(work_country) As work_country
    latest(_time) as _time
    by user 
| rename user AS User 
| `strftime_format(_time)` 
| table _time User OS_Name OS_Version Login_Source Num_Login_Sources Browser_Name Browser_Version work_city work_region work_country 
| `rename_common_fields` 
| `rename_identity_fields`",0
"GMC-244","List of all use cases collected from the UC SH","",SPL,"","index=gmc_summary source=security_use_cases_lookup_searches 
| table Use_Case_Number Use_Case_Name Application_Context Description Adoption_Phase ATT_CK_Tactic Data_Enrichment Data_Model Data_Source Dataset ESCU_Analytic_Story ESCU_Asset_At_Risk ESCU_Channel ESCU_CIS_Critical_Security_Controls ESCU_Confidence ESCU_Creation_Date ESCU_Details ESCU_How_To_Implement ESCU_Known_False_Positives ESCU_Providing_Technologies ESCU_Required_Fields ESCU_Search_Type Kill_Chain_Phases Notable_Severity SE_Category SE_Data_Sources SE_Journey SE_Security_Use_Case Security_Domain source_data Use_Case_Collection Search 
| search Use_Case_Name IN (*index*, *splunk*, *queu*, *health*, *perf*, *skip*, *lag*, *delay*, *forward*, *universal*, *search*, *scheduler*) 
| table Use_Case_Number Use_Case_Name Search",0
"GMC-245","Gather all dashboard searches","",SPL,"","| from lookup:splunk_rest_data_ui_views_sh_kv_store_lookup 
| search  app IN (*gmc_backup) 
| stats values(description) As description count values(panel_title) as panel_title values(search) as search values(*) as * by title 
| table title app count panel_title description search",0
"GMC-246","Skipped search detail by Reason",skips,Scheduler,"","index=_internal sourcetype=scheduler status=""skipped"" |bucket _time span=1d| eval Day = strftime(_time, ""%D"") | join max=0 savedsearch_name [| rest /services/saved/searches | where is_scheduled=1 |rename eai:acl.app AS app |rename title AS savedsearch_name| table app savedsearch_name cron_schedule] |eval app_search=app+"" | ""+savedsearch_name+"" | ""+cron_schedule |eval Reason=reason+"" | ""+app | convert ctime(*time) | chart limit=25 count over Reason by Day",0
"GMC-247","Significant Change in Volume of Data Indexed Alert",volume,Indexes,"","index=_internal host=lm*.*.splunkcloud.com source=*license_usage.log* type=""Usage"" | bin _time span=15m | eval date_wday=strftime(_time,""%w"") | eval date_hour=strftime(_time,""%H"") | eval weekday_weekend=if(((date_wday > 0) AND (date_wday < 6)), ""weekday"", ""weekend"") | eval biz_hours=if(((date_hour < 8) OR (date_hour > 18) OR (weekend_weekday==""weekend"")), ""no"", ""yes"") | stats sum(b) AS byte_sum by idx, weekday_weekend, biz_hours | join type=inner idx,weekday_weekend, biz_hours [| inputlookup avg_index_bytes_15m.csv] | eval devs=(byte_sum - average)/std | eval currentGB=round((byte_sum/1024/1024/1024), 3) | eval averageGB=round((average/1024/1024/1024), 3) | table idx, currentGB, averageGB, devs | where (devs < -2) AND averageGB > 0.75 | sort devs | rename idx AS Index, currentGB AS ""GB Indexed over Past 15 Minutes"", averageGB AS ""Average GB Indexed per 15 Minutes"", devs AS ""Z-Score""",0
"GMC-248","Data Model Acceleration Lag",lag,Search,"","| tstats allow_old_summaries=false summariesonly=t latest(_time) AS time from datamodel=Network_Traffic | convert ctime(time) AS last_time | eval current_time=now(), lag(mins)=round((time - current_time) / 60, 1) | convert ctime(current_time) | table last_time, current_time, lag(mins)
19	UCCUS1032	Splunk Servers storage issue - HPOV	 	",0
"GMC-249","Skynet Searches",skynet,SPL,"","| inputlookup Skynet_Btool_savedsearches_conf.csv append=t 
| inputlookup Skynet_Dashboards.csv append=t 
| inputlookup Skynet_Macros.csv append=t 
| inputlookup Skynet_SavedSearches.csv append=t",0
